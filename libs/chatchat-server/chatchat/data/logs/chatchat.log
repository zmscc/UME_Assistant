024-12-11 17:34:48.138 | WARNING  | chatchat.server.utils:detect_xf_models:107 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-11 17:34:48.139 | WARNING  | chatchat.server.utils:get_default_llm:205 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-11 17:34:48.159 | WARNING  | chatchat.server.utils:get_default_embedding:214 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-11 17:34:48.162 | WARNING  | chatchat.server.utils:get_default_embedding:214 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-11 17:34:48.164 | WARNING  | chatchat.server.utils:get_default_embedding:214 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-11 17:34:48.166 | WARNING  | chatchat.server.utils:get_default_embedding:214 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 09:14:45.918 | WARNING  | chatchat.server.utils:detect_xf_models:107 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 09:14:45.919 | WARNING  | chatchat.server.utils:get_default_llm:205 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 09:14:45.941 | WARNING  | chatchat.server.utils:get_default_embedding:214 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 09:14:45.944 | WARNING  | chatchat.server.utils:get_default_embedding:214 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 09:14:45.946 | WARNING  | chatchat.server.utils:get_default_embedding:214 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 10:59:38.025 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 10:59:38.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 10:59:38.053 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 10:59:38.056 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 10:59:38.058 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 13:56:36.504 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 13:56:36.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 13:56:36.526 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 13:56:36.529 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 13:56:36.531 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 14:02:37.303 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 14:29:29.136 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:34:12.238 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:34:55.090 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:35:01.508 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:35:01.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:35:08.227 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:36:31.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:38:26.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:12.016 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 15:42:12.051 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 15:42:18.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:23.704 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:23.715 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:25.623 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:25.988 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 15:42:30.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:30.020 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 15:42:46.444 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:46.447 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:42:46.449 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:42:46.449 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:42:46.450 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:42:46.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:46.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:46.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:46.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:52.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:52.435 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:42:52.437 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:42:52.437 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:42:52.438 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:42:52.453 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:52.454 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:52.455 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:42:52.455 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:08.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:08.355 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:08.358 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:08.358 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:08.359 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:43:08.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:08.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:08.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:08.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:57.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:57.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:57.547 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:57.548 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:57.548 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:57.548 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:43:59.988 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:43:59.991 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:59.993 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:59.993 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:43:59.994 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:44:00.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.015 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.017 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.314 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:44:00.318 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:44:00.319 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:44:00.320 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:44:00.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:44:00.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:03.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:03.795 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:03.804 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:03.806 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:03.806 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:03.806 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:48:09.085 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.088 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:09.089 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:09.090 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:09.091 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:48:09.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.112 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.113 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.388 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:09.394 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:09.395 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:48:09.396 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:48:09.412 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.413 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:48:09.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:49:36.650 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 15:49:36.701 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 15:49:36.702 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 15:49:36.702 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 15:50:57.629 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:03.146 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:03.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:05.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:05.567 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 15:51:10.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:10.048 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 15:51:15.516 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:15.518 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:15.519 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:15.520 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:15.521 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:51:15.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:15.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:15.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:15.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:54.404 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:54.408 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:54.409 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:54.409 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:54.410 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:51:54.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:54.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:54.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:54.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:58.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:58.853 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:58.855 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:58.856 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 15:51:58.856 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 15:51:58.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:58.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:58.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 15:51:58.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:00.343 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 16:06:00.394 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 16:06:00.396 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 16:06:00.397 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 16:06:07.696 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:13.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:13.998 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:16.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:16.870 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 16:06:21.108 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:21.116 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 16:06:25.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.612 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.614 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.614 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.614 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:06:25.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.630 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.635 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.629 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.637 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.635 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.639 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.641 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.641 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:06:25.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.642 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:25.647 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:06:25.651 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.747 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.747 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.748 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.752 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:25.840 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:28.751 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:28.753 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:28.754 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:28.755 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:28.755 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:06:28.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:28.765 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:28.766 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:28.766 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:48.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:48.292 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:48.294 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:48.295 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:06:48.296 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:06:48.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:48.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:48.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:06:48.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:09:38.502 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 16:09:38.528 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 16:09:38.528 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 16:09:38.529 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 16:09:45.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:09:50.998 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:09:51.010 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:09:52.967 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:09:53.322 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 16:09:57.231 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:09:57.238 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 16:10:02.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.537 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:02.540 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:02.542 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:02.543 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:10:02.552 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.558 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:02.562 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:02.566 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:02.566 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:10:02.578 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:02.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:13.511 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:13.518 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:13.520 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:13.521 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:13.521 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:10:13.532 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:13.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:13.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:13.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:22.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:22.586 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:22.588 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:22.589 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:22.589 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:10:22.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:22.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:22.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:22.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:29.600 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:29.608 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:29.611 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:29.612 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:29.612 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:10:29.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:10:29.881 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:29.882 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:29.882 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:10:29.883 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:18:49.641 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:18:49.647 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.165 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:18:50.167 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.169 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.170 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.171 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.172 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:18:50.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.200 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.569 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.570 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.570 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.571 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:18:50.970 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:50.979 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.980 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.981 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:50.981 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:18:52.116 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.118 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:52.120 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:52.121 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:52.121 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:18:52.140 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.359 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:52.361 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:52.362 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:52.364 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:18:52.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:52.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:55.415 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:55.422 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:55.424 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:55.424 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:55.424 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:18:55.621 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:18:55.630 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:55.632 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:55.634 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:18:55.636 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:19:29.822 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:29.825 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:29.827 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:29.827 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:29.828 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:19:29.849 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:29.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:29.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:29.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:30.039 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:30.042 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:30.043 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:30.044 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:30.044 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:19:30.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:30.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:30.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:30.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:37.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:37.648 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:37.650 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:37.651 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:37.652 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:19:37.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:37.890 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:37.892 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:37.893 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:37.894 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:19:52.289 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:19:52.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:52.469 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:19:52.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:52.471 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:52.473 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:52.474 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:52.476 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:19:52.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:52.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:52.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:52.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:58.364 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:58.374 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:58.376 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:58.376 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:58.378 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:19:58.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:19:58.611 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:58.612 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:58.614 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:19:58.614 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:31:30.312 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:31:30.318 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:31:30.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:31:30.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:31:30.853 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:31:30.854 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:31:30.854 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:31:30.855 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:31:30.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:31:30.878 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:31:30.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:31:30.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:44:24.299 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:44:24.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:44:24.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:44:24.324 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:44:24.326 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:44:24.328 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:44:30.197 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 16:44:30.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 16:44:30.216 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:44:30.219 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:44:30.221 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:44:30.223 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-12 16:54:41.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:54:45.486 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:55:39.871 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 16:55:39.917 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 16:55:39.919 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 16:55:39.919 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 16:55:45.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:55:51.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:55:51.236 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:55:53.541 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:55:53.904 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 16:55:57.966 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:55:57.974 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 16:56:04.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.144 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.152 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.159 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.159 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.160 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.160 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.161 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.161 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.161 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.162 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:04.162 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:56:04.162 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:56:04.163 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:56:04.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:04.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:15.463 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:15.472 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:15.475 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:15.476 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:15.476 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:56:15.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:15.773 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:15.775 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:15.776 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:15.776 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:56:50.052 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:56:50.062 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:50.064 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:50.064 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /knowledge_base/list_knowledge_bases: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:56:50.065 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:57:05.989 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:57:06.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:57:06.065 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:57:06.067 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:57:06.067 | ERROR    | chatchat.webui_pages.utils:get:64 - TypeError: error when get /tools: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:57:06.067 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 16:57:06.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:57:06.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:57:06.078 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:57:06.078 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 16:57:27.815 | ERROR    | chatchat.webui_pages.utils:post:87 - TypeError: error when post /knowledge_base/create_knowledge_base: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:57:27.817 | ERROR    | chatchat.webui_pages.utils:post:87 - TypeError: error when post /knowledge_base/create_knowledge_base: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:57:27.817 | ERROR    | chatchat.webui_pages.utils:post:87 - TypeError: error when post /knowledge_base/create_knowledge_base: Client.__init__() got an unexpected keyword argument 'proxies'
2024-12-12 16:57:27.818 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-12 17:06:46.223 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 17:06:46.289 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-12 17:06:46.292 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 17:06:46.294 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-12 17:07:24.531 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:30.868 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:30.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:32.920 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:33.278 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 17:07:37.863 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:37.874 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-12 17:07:43.369 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:43.436 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:43.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:43.439 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:43.439 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:49.240 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:07:49.545 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:04.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:04.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:04.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:04.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:04.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.423 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.479 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.480 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.483 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.488 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:05.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:14.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:14.062 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:14.064 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:14.065 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:14.065 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:08:14.363 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-12 17:08:56.451 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest\content\航旅纵横实习生活指南1.docx
2024-12-12 17:09:06.638 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('MyTest', 'bge-m3') 保存到磁盘
2024-12-12 17:09:10.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:10.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:43.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:43.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:43.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:43.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:43.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:44.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:44.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:44.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:44.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:44.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:53.239 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:53.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:53.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:53.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:09:53.281 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:10:02.406 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:10:02.718 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:10:16.269 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:10:45.702 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:10:54.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:11:07.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:11:49.247 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest\content\航旅钱包客服问题.docx
2024-12-12 17:11:51.681 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('MyTest', 'bge-m3') 保存到磁盘
2024-12-12 17:11:55.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:11:55.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:11:59.901 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:12:00.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:12:14.039 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:12:26.058 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:12:45.910 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:13:18.514 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:13:42.945 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:13:57.597 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:14:08.048 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:14:27.786 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:14:45.243 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:14:57.393 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:15:20.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:16:18.519 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:16:59.678 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:16:59.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:01.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:01.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:01.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:01.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:03.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:03.701 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:27.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:17:48.389 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:18:01.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:18:32.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:19:00.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:20:26.685 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:21:20.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:00.775 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:00.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:02.447 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:02.448 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:02.448 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:02.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:03.093 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:03.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:22:39.241 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:23:20.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:23:49.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:24:35.847 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:24:58.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:25:39.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-12 17:31:02.371 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 17:31:02.373 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:31:02.729 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 17:31:02.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:31:04.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:31:04.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:31:04.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:31:04.591 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:31:14.292 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:31:14.643 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:10.492 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 17:39:10.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:10.642 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 17:39:10.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:13.679 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:13.897 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:18.774 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:19.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:21.083 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:21.083 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:21.083 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:21.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:48.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:50.878 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:50.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:50.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:50.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:39:53.136 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 17:40:17.293 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-12 17:40:17.293 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:40:19.095 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:40:19.095 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:40:19.096 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-12 17:40:19.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 08:55:20.104 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 08:55:20.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 08:55:21.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 08:55:21.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 08:55:21.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 08:55:21.500 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:46.894 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 09:03:46.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:47.089 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 09:03:47.089 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:52.713 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:52.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.104 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.105 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.381 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.424 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.426 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:03:55.426 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:08.727 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:08.791 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:08.792 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:08.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:08.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:15.475 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:15.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:15.514 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:15.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:04:15.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:12:40.052 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 09:12:40.123 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 09:12:40.125 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 09:12:40.127 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 09:12:51.333 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:12:57.726 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:12:57.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:12:59.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:00.186 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 09:13:05.164 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:05.172 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 09:13:10.753 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:10.815 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:10.816 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:10.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:10.818 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:20.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:21.162 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.431 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.473 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.474 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.475 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.777 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:25.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:28.786 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:28.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:28.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:28.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:28.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:29.620 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:29.657 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:29.657 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:29.657 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:29.658 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:34.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:34.296 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:34.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:34.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:34.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.698 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.699 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.699 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.700 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.842 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.883 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:13:54.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:03.203 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-13 09:14:03.210 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-13 09:14:21.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:21.433 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:21.502 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:21.502 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:21.503 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:21.503 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:26.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:26.909 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:26.909 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:26.910 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:26.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:50.308 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:50.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:50.356 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:50.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:50.358 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:57.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:57.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:57.281 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:57.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:14:57.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:08.991 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:09.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:09.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:09.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:09.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:25.074 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:25.133 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:25.134 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:25.135 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:25.136 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:35.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:35.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:35.419 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:35.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:35.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:36.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:36.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:36.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:36.369 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:36.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:15:44.856 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-13 09:15:44.871 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-13 09:17:57.753 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:57.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:57.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:57.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:57.822 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:59.835 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:59.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:59.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:59.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:17:59.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:02.217 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:02.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:02.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:02.292 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:02.293 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:56.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:56.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:56.085 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:56.086 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:56.086 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:57.302 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:57.329 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:57.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:57.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:18:57.419 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:19:05.693 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-13 09:19:05.697 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-13 09:20:53.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:54.083 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:54.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:54.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:54.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:54.144 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:57.130 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:57.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:57.205 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:57.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:20:57.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:19.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:20.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:20.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:20.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:20.032 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:21.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:21.169 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:21.195 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:21.221 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:21.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:21:29.270 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-13 09:21:29.275 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-13 09:23:36.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:23:36.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:24:16.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:24:17.365 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:24:26.509 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:24:31.214 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-13 09:24:54.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:26:36.807 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:27:03.712 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:28:03.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:28:29.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:28:44.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:29:01.846 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:30:01.423 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:30:29.329 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:31:24.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:31:37.883 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:32:23.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:33:29.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:34:20.700 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:34:32.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:34:33.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:34:33.746 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:34:33.968 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:34:39.478 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:35:26.416 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:36:40.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:38:05.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:39:38.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:39:40.161 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:39:48.698 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 09:43:11.946 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 09:43:11.948 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:12.312 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 09:43:12.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:12.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:12.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:12.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:12.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:15.091 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:15.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:15.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:15.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:15.230 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:16.704 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:16.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:16.816 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:16.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:16.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:24.630 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:24.738 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:24.738 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:24.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:24.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 09:43:26.874 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 13:00:51.430 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:00:51.466 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:00:51.468 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:00:51.468 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:01:02.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:09.990 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:10.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:12.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:12.929 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:01:17.689 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:17.709 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:01:25.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:25.875 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:25.876 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:25.876 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:25.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:36.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:37.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:50.688 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:01:54.110 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-13 13:02:51.035 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:02:51.085 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:02:51.087 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:02:51.088 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:03:03.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:30.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:37.175 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:37.186 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:39.254 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:39.648 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:24:44.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:44.156 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:24:49.480 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:49.537 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:49.538 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:49.538 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:24:49.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:11.466 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:11.518 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:11.519 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:11.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:11.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:12.272 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:12.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:12.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:12.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:25:12.312 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:26:01.953 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:26:02.002 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:26:02.003 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:26:02.004 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:26:15.698 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:17.846 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:17.874 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:22.445 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:23.407 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:27:34.555 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:34.575 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:27:42.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:42.256 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:42.256 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:42.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:27:42.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:21.815 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:21.875 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:21.878 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:21.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:21.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:23.211 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:23.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:23.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:23.266 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:28:23.267 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:33:24.076 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:33:24.107 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:33:24.108 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:33:24.109 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:33:50.989 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:33:56.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:33:56.810 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:33:58.846 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:33:59.213 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:34:03.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:03.548 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:34:09.179 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:09.244 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:09.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:09.246 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:09.246 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:13.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:13.802 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:13.803 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:13.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:34:13.805 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:35:03.765 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:35:03.787 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:35:03.787 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:35:03.788 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:35:15.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:36:21.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:09.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:28.010 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:28.038 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:32.294 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:33.186 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:37:41.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:41.590 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:37:48.050 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:48.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:48.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:48.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:37:48.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:18.978 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest\content\接口文档.docx
2024-12-13 13:38:27.644 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:27.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:27.733 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:27.735 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:27.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:29.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:29.998 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:30.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:30.002 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:38:30.004 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:39:02.070 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-13 13:39:04.004 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('MyTest', 'bge-m3') 保存到磁盘
2024-12-13 13:39:53.750 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:39:53.782 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:39:53.783 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:39:53.784 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:41:53.379 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:00.497 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:00.509 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:02.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:03.030 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:42:07.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:07.223 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:42:11.813 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:11.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:11.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:11.871 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:11.872 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:42:34.582 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest\content\用户权限分配.docx
2024-12-13 13:42:52.271 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-13 13:42:53.162 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('MyTest', 'bge-m3') 保存到磁盘
2024-12-13 13:43:35.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:43:35.964 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:43:39.447 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:43:39.679 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:43:54.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:45:14.148 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-13 13:45:51.619 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest\content\航旅纵横实习生活指南1.docx
2024-12-13 13:45:51.620 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest\content\航旅钱包客服问题.docx
2024-12-13 13:45:55.749 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('MyTest', 'bge-m3') 保存到磁盘
2024-12-13 13:46:10.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:46:11.116 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:46:47.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:46:47.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:46:58.679 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:47:15.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:47:41.617 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:48:01.921 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:48:23.064 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:49:11.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:18.906 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:58:18.963 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 13:58:18.964 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:58:18.964 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 13:58:26.052 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:32.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:32.308 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:34.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:34.603 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:58:38.898 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:38.910 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 13:58:44.304 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:44.361 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:44.362 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:44.363 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:44.363 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:46.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:46.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:46.978 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:46.979 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:46.979 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:47.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:47.660 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:47.661 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:47.661 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:47.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:47.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:48.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:55.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:58:55.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:59:10.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 13:59:12.722 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-13 13:59:48.656 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 14:09:43.816 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:09:43.818 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:44.167 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:09:44.167 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:44.754 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:44.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:44.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:44.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:47.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:48.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:48.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:48.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:48.100 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:52.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:52.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:52.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:52.583 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:09:52.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:02.704 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:02.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:02.871 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:02.872 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:02.872 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:04.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:04.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:04.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:04.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:04.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:13.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:13.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:13.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:13.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:10:13.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:50:22.001 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:50:22.007 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:50:22.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:50:22.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:50:22.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:50:22.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:50:31.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:50:31.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:15.676 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:15.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.613 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.654 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.655 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.655 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.656 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:18.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:46.268 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:51:46.268 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:51:46.468 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:51:46.468 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:53:18.434 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:53:18.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:53:18.540 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:53:18.703 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:53:18.706 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 14:53:19.895 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 14:53:19.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 15:31:57.667 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 15:31:57.722 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-13 15:31:57.723 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 15:31:57.724 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-13 15:32:08.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:17.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:17.304 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:21.663 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:22.142 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 15:32:27.710 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:27.724 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-13 15:32:36.768 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:36.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:36.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:36.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:36.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:42.767 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:43.058 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:46.898 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:46.950 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:46.951 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:46.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:46.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:47.221 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:47.290 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:47.290 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:47.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:47.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:48.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:32:48.333 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:33:10.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:33:14.866 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-13 15:33:58.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:33:58.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:33:59.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:33:59.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:33:59.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:33:59.236 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:34:00.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 15:34:00.381 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:25:49.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:25:49.649 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:25:54.194 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.368 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.650 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.801 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:00.802 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:03.927 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:04.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:04.103 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:04.107 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:04.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:05.175 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:05.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:05.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:05.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:05.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:14.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:14.264 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:14.266 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:14.267 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:14.268 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:27.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.912 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:28.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:29.653 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:29.772 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:29.773 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:29.773 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:26:29.774 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:28:52.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:28:53.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:28:53.130 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:28:53.136 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:28:53.139 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:16.754 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:17.403 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:17.702 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:20.953 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.110 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.113 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.392 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:21.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:39.674 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:40.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.112 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.243 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.244 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.786 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.788 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.789 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:41.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:43.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:43.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:43.740 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:43.741 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:43.742 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:48.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:48.223 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:48.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:48.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:48.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:59.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:34:59.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:35:53.072 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:36:08.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:36:08.558 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:36:09.792 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:36:10.548 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:46:44.587 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:46:55.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:12.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:12.680 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:12.681 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:12.682 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:12.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:12.966 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:13.345 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:13.346 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:13.347 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:13.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:15.692 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:47:16.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:50:22.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:50:22.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:50:23.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:50:23.315 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:50:23.316 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:50:23.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:57:57.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:57:57.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:57:57.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:57:57.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 16:57:57.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:35.463 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:35.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:38.724 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.091 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.093 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.753 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:39.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:40.133 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:00:40.818 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:13.281 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:31.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:31.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:31.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:31.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:31.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:32.043 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:32.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:32.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:32.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:04:32.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-13 17:09:24.558 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:09:24.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:09:25.315 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:09:25.316 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:09:51.463 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:09:51.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:09:52.242 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:09:52.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:09:52.246 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:09:52.246 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:26.468 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:13:26.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:26.724 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:13:26.725 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:28.885 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:29.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:29.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:29.540 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:29.540 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:29.541 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:48.789 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:13:49.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:31:46.108 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:31:46.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:31:46.772 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:31:46.773 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:31:47.273 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:31:47.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:31:47.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:31:47.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:38:27.151 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:38:27.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:38:27.481 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:38:27.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:38:27.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:38:27.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:12.839 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:42:12.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:13.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:13.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:13.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:13.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:17.281 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:17.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:17.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:17.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:17.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:19.897 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:20.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:20.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:20.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:20.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:28.106 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:28.407 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:28.407 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:28.407 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:42:28.407 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:43:44.476 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:43:44.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:43:44.662 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-13 17:43:44.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:43:44.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:43:44.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:43:44.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-13 17:43:44.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-14 12:49:57.825 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-14 12:49:57.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-14 12:49:57.876 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-14 12:49:57.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-14 12:49:57.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-14 12:49:57.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:13:49.428 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-15 12:13:49.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:13:49.752 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:13:49.754 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:13:49.754 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:13:49.754 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:22:52.268 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-15 12:22:52.272 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:22:52.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:22:52.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:22:52.478 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:22:52.478 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:02.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:03.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:03.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:03.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:03.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:34.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:34.455 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.596 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.596 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.596 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.752 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.803 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.806 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.806 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-15 12:23:37.806 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:20.070 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 09:19:20.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:20.091 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:20.093 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:20.266 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:27.366 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 09:19:27.367 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:27.377 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:29.172 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.175 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.196 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.201 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.479 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.653 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:29.744 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.748 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.750 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:29.753 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:30.034 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-16 09:19:35.437 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 09:19:35.439 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:35.446 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-16 09:19:39.438 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:39.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:39.443 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:42.743 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:42.749 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 09:19:43.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:43.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:43.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:43.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:19:43.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 09:23:21.747 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-16 09:23:21.748 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-16 09:23:21.748 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-16 09:23:21.749 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-16 10:52:07.756 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 10:52:07.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:52:07.778 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:52:07.781 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:52:08.210 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:00.504 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 10:54:00.504 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:00.522 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:00.524 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:00.664 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:08.023 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 10:54:08.024 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:08.075 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:10.008 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.010 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.039 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.039 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.040 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.040 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.044 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.308 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.444 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:10.526 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.530 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.533 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.535 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:10.816 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-16 10:54:16.418 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 10:54:16.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:16.433 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-16 10:54:20.161 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:20.165 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:20.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:23.621 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:23.625 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 10:54:23.923 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:24.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:24.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:24.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 10:54:24.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:00:32.939 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-16 11:00:32.977 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-16 11:00:32.977 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-16 11:00:32.977 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-16 11:00:41.117 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 11:00:41.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:00:41.145 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:41.146 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:41.323 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:48.621 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 11:00:48.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:00:48.633 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:00:50.164 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.167 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.189 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.189 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.189 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.190 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.194 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.435 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:00:50.634 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.638 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.640 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.642 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:50.950 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-16 11:00:56.463 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 11:00:56.465 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:00:56.473 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-16 11:00:59.487 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:59.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:00:59.495 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:01:00.774 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:01:00.778 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-16 11:01:00.986 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.053 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.061 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.062 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 11:01:01.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:06.843 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 13:29:06.849 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:07.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:07.374 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:07.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:07.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:22.909 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:23.443 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:24.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.004 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.004 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.246 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:25.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:27.545 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:27.580 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:27.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:27.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:27.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:36.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:36.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:36.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:36.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:36.356 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:39.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:39.062 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:39.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:39.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:39.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:41.332 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 13:29:41.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:41.541 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:41.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:41.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:41.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:29:49.646 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen:7b', 'ollama')
2024-12-16 13:29:49.651 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-16 13:30:14.339 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 13:30:14.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.387 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.388 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.473 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.504 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:14.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 13:30:22.134 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen:7b', 'ollama')
2024-12-16 13:30:22.138 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-16 13:52:36.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.022 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.024 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.605 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.634 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.663 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:52:37.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:05.316 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:05.368 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:05.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:05.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:05.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.714 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.715 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.715 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.717 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.849 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.888 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:26.994 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:34.548 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-16 13:53:34.552 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-16 13:53:51.366 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:51.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:51.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:51.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:51.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:56.236 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:56.288 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:56.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:56.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:53:56.293 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:03.514 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:03.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:03.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:03.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:03.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.161 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.230 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.231 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.232 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.654 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.682 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.709 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:18.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:26.262 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-16 13:54:26.266 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-16 13:54:29.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.424 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.424 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.648 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:29.681 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.901 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.937 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:43.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:44.009 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:54:44.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.805 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.912 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.946 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:14.985 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:15.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:15.060 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:55:21.481 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:61 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-16 13:55:21.485 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-16 13:59:50.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:59:50.442 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:59:50.443 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:59:50.444 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 13:59:50.445 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-16 14:54:06.086 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 14:54:06.091 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:06.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:06.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:06.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:06.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:14.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:14.333 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:14.334 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:14.335 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 14:54:14.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 15:00:21.966 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-16 15:00:21.967 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 15:00:22.009 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 15:00:22.010 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 15:00:22.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-16 15:00:22.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:14:20.545 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 09:14:20.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:14:20.578 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:14:20.580 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:14:21.009 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:33.025 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 09:18:33.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:33.079 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:33.082 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:33.313 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:40.852 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 09:18:40.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:40.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:43.589 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:43.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:43.615 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:43.615 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:43.615 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:43.616 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:43.619 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:43.958 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:44.243 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:44.405 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:44.410 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:44.412 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:44.415 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:44.721 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-17 09:18:50.685 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 09:18:50.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:50.715 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-17 09:18:54.166 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:54.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:54.171 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:57.234 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:57.238 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-17 09:18:57.524 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:57.599 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:57.600 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:57.600 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:18:57.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:09.843 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 09:22:09.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:09.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:09.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:09.937 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:09.937 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:14.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:14.160 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:14.161 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:14.161 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:14.161 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:17.716 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:17.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:17.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:17.765 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:17.765 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:26.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:26.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:26.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:26.415 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:26.415 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:28.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:28.110 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:28.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:28.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:28.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:31.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:31.712 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:31.713 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:31.714 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:31.714 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:45.223 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:45.273 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:45.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:45.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:22:45.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:24:27.662 | ERROR    | chatchat.server.api_server.openai_routes:task:134 - failed request to platform: oneapi
2024-12-17 09:24:27.667 | ERROR    | chatchat.server.api_server.openai_routes:task:134 - failed request to platform: xinference
2024-12-17 09:24:27.739 | ERROR    | chatchat.server.api_server.openai_routes:task:134 - failed request to platform: ollama
2024-12-17 09:24:36.538 | ERROR    | chatchat.server.api_server.openai_routes:task:134 - failed request to platform: openai
2024-12-17 09:35:19.004 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 09:35:19.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:35:19.079 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:35:19.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:35:19.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 09:35:19.081 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-17 13:14:28.870 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 13:14:31.951 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'missing', 'loc': ['body', 'messages'], 'msg': 'Field required', 'input': {'model': 'string', 'prompt': '恼羞成怒', 'echo': False, 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 1024, 'n': 1, 'presence_penalty': 0, 'stream': True, 'temperature': 0.01, 'top_p': 1}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}]}
2024-12-17 13:17:12.901 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-17 13:17:13.056 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'missing', 'loc': ['body', 'messages'], 'msg': 'Field required', 'input': {'model': 'string', 'prompt': '恼羞成怒', 'echo': False, 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 1024, 'n': 1, 'presence_penalty': 0, 'stream': True, 'temperature': 0.01, 'top_p': 1}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}]}
2024-12-17 13:17:16.358 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'missing', 'loc': ['body', 'messages'], 'msg': 'Field required', 'input': {'model': 'string', 'prompt': '恼羞成怒', 'echo': False, 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 1024, 'n': 1, 'presence_penalty': 0, 'stream': True, 'temperature': 0.01, 'top_p': 1}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}]}
2024-12-17 16:12:59.743 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-18 18:38:32.087 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-18 18:38:32.104 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-18 18:38:33.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-18 18:38:33.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-18 18:38:33.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-18 18:38:33.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 08:54:16.337 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 08:54:16.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 08:54:16.792 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 08:54:16.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 08:54:16.794 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 08:54:16.795 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:31.823 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 14:23:31.832 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:32.277 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:32.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:32.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:32.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:38.652 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:38.709 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:38.711 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:38.711 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:38.712 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:49.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:49.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:49.935 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:49.935 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:49.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:52.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:52.942 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:52.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:52.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:23:52.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:24:19.771 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 14:24:20.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:43:45.156 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 15:43:45.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:43:45.702 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:43:45.704 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:43:45.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:43:45.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:43:47.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:43:47.483 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:27.756 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 15:59:27.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:28.071 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 15:59:28.072 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:28.169 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:28.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:28.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:28.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:29.740 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:29.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:29.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:29.786 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 15:59:29.786 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:01:16.028 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 16:01:16.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:01:16.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:01:16.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:01:16.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:01:16.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:50.739 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 16:02:50.740 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:50.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:50.838 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:50.838 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:50.838 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:55.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:55.968 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:55.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:55.970 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:02:55.970 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:22.875 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:23.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:34.530 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-19 16:03:34.531 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-19 16:03:34.531 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-19 16:03:34.531 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-19 16:03:41.337 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:41.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.483 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.542 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.542 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.543 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.543 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.770 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.807 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.808 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.808 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:43.809 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:54.514 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 16:03:54.514 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:54.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:54.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:54.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:03:54.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:03.242 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 16:15:03.253 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-19 16:15:03.255 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-19 16:15:25.409 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:25.627 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:37.812 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:37.861 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:37.861 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:37.862 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:37.862 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:38.147 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:38.181 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:38.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:38.183 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:38.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:41.847 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:41.893 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:41.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:41.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:15:41.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:19.182 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-19 16:16:19.183 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:19.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:19.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:19.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:19.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:21.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:21.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:21.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:21.830 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 16:16:21.831 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-19 17:21:21.390 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 09:45:24.983 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 09:45:25.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 09:45:25.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 09:45:25.893 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 09:45:25.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 09:45:25.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:07.609 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-20 10:20:07.699 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-20 10:20:07.701 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-20 10:20:07.701 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-20 10:20:21.396 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 10:20:21.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:21.421 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:21.423 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:21.967 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:29.657 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 10:20:29.658 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:29.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:32.000 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.003 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.028 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.028 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.028 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.029 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.033 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.324 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.502 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:32.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.596 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.598 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.600 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:32.939 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-20 10:20:38.640 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 10:20:38.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:38.675 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-20 10:20:44.397 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:44.399 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:44.401 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:47.464 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:47.467 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-20 10:20:47.740 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:47.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:47.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:47.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:47.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:54.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:54.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:54.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:54.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:20:54.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 10:24:07.053 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:24:07.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:24:10.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:24:11.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:24:24.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:24:27.656 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-20 10:24:37.268 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-20 10:24:51.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:25:00.334 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-20 10:26:11.967 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:12.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:12.024 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:12.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:12.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:19.982 | ERROR    | chatchat.server.api_server.openai_routes:generator:106 - openai request error: Connection error.
2024-12-20 10:26:37.773 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:37.813 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:37.815 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:37.816 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:37.818 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:40.344 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:40.652 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 10:26:48.690 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-20 14:10:38.761 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 14:10:38.781 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 14:10:38.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:38.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:40.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:40.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:40.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:40.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:41.632 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:41.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:41.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:41.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:10:41.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:11:06.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:11:07.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:11:07.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:11:07.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:11:07.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:11:09.336 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 14:12:26.792 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 14:12:26.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:26.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:26.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:26.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:26.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:29.003 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-20 14:12:45.429 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:45.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:45.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:45.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-20 14:12:45.540 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 09:13:56.450 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 09:13:56.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 09:13:57.188 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 09:13:57.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 09:13:57.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 09:13:57.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 09:28:32.921 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 09:28:33.428 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'missing', 'loc': ['body', 'messages'], 'msg': 'Field required', 'input': {'model': 'string', 'prompt': '恼羞成怒', 'echo': False, 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 1024, 'n': 1, 'presence_penalty': 0, 'stream': True, 'temperature': 0.01, 'top_p': 1}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}]}
2024-12-23 10:56:34.570 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:00:53.405 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 11:00:53.468 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 11:00:53.468 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 11:00:53.468 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 11:01:04.056 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:01:04.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:04.097 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:04.112 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:04.704 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:12.177 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:01:12.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:12.215 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:14.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.235 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.235 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.235 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.235 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.235 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.548 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.704 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:14.800 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.800 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.800 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:14.800 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:15.082 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 11:01:20.513 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:01:20.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:20.528 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 11:01:24.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:24.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:24.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:27.938 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:27.940 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:01:28.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:28.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:28.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:28.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:01:28.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:03:07.588 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:10:59.903 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:12:18.982 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 11:12:19.014 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 11:12:19.014 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 11:12:19.014 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 11:12:26.663 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:12:26.663 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:26.730 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:26.730 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:26.902 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:34.143 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:12:34.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:34.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:35.665 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:35.665 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:35.680 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:35.680 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:35.680 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:35.680 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:35.696 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:35.899 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:36.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:36.087 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:36.103 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:36.103 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:36.103 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:36.415 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 11:12:41.814 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:12:41.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:41.846 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 11:12:44.676 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:44.676 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:44.693 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:45.977 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:45.993 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:12:46.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:46.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:46.250 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:46.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:12:46.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:14:01.319 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:24:23.359 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:25:38.089 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 11:25:38.105 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 11:25:38.105 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 11:25:38.105 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 11:25:45.320 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:25:45.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:25:45.378 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:45.378 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:45.549 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:52.698 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:25:52.698 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:25:52.713 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:25:54.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.229 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.229 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.229 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.229 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.229 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.447 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:25:54.635 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.635 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.635 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.635 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:25:54.960 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 11:26:00.301 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 11:26:00.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:00.335 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 11:26:03.550 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:26:03.553 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:26:03.555 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:26:04.929 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:26:04.929 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 11:26:05.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:05.205 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:05.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:05.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:05.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:06.345 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:06.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:06.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:06.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:06.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:14.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:14.923 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:14.940 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:14.973 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:16.040 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 11:26:29.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:29.599 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:29.648 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:29.682 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 11:26:29.782 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 12:12:06.441 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 12:12:06.472 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 12:12:06.472 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 12:12:06.472 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 12:12:24.863 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:12:24.863 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:12:24.910 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:24.926 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:25.411 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:40.169 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:12:40.169 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:12:40.215 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:12:44.145 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:44.145 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:44.176 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:44.176 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:44.176 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:44.176 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:44.192 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:44.721 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:45.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:12:45.206 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:45.221 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:45.237 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:45.237 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:45.942 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 12:12:55.888 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:12:55.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:12:55.923 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 12:12:59.886 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:59.886 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:12:59.903 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:13:03.918 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:13:03.935 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:13:04.387 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.486 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.490 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.500 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:13:04.583 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:14:51.125 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:14:51.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:14:51.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:14:51.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:14:51.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:14:52.306 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 12:15:45.445 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:45.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:45.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:45.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:45.738 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 12:15:57.171 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:15:57.254 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:57.288 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:57.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:57.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:15:57.570 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 12:16:07.748 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 12:16:07.779 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 12:16:07.779 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 12:16:07.779 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 12:16:16.045 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:16:16.045 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:16.086 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:16.086 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:16.262 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:23.340 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:16:23.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:23.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:24.837 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:24.837 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:24.868 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:24.868 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:24.868 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:24.868 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:24.868 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:25.088 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:25.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:25.291 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:25.306 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:25.306 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:25.322 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:25.595 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 12:16:30.980 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 12:16:30.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:30.980 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 12:16:33.950 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:33.950 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:33.950 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:35.699 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:35.699 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 12:16:35.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:35.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:35.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:35.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:35.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:36.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:36.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:36.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:36.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:36.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:49.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:49.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:49.957 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:49.974 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 12:16:50.690 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 13:11:45.561 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:11:45.624 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:11:45.624 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:11:45.624 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:12:01.851 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:12:01.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:01.944 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:01.960 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:02.132 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:16.212 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:12:16.212 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:16.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:19.765 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:19.765 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:19.796 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:19.796 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:19.796 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:19.796 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:19.812 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:20.281 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:20.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:20.734 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:20.734 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:20.750 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:20.750 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:21.419 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:12:31.592 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:12:31.592 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:31.623 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:12:34.935 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:34.935 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:34.952 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:37.167 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:37.183 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:12:37.747 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:37.836 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:37.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:37.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:12:37.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:13:49.103 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:13:49.960 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:13:49.993 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:13:50.043 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:13:50.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:13:51.110 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 13:14:00.564 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:14:00.564 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:14:00.564 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:14:00.564 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:14:20.362 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:14:20.362 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:20.393 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:20.409 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:20.565 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:34.937 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:14:34.937 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:34.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:38.395 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:38.410 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:38.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:38.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:38.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:38.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:38.457 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:38.926 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:39.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:39.329 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:39.345 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:39.345 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:39.361 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:40.033 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:14:50.418 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:14:50.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:50.434 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:14:54.544 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:54.558 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:54.561 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:56.556 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:56.638 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:14:57.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:57.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:57.183 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:57.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:14:57.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:15:45.679 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:15:46.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:15:46.545 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:15:46.578 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:15:46.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:15:47.661 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 13:16:56.423 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:18:36.467 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:18:36.468 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:18:36.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:18:36.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:18:36.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:18:36.801 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'system'", 'input': 'tool', 'ctx': {'expected': "'system'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'user'", 'input': 'tool', 'ctx': {'expected': "'user'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'assistant'", 'input': 'tool', 'ctx': {'expected': "'assistant'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'tool_call_id'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'missing', 'loc': ['body', 'messages', 3, 'typed-dict', 'name'], 'msg': 'Field required', 'input': {'role': 'tool', 'content': 'string'}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}, {'type': 'literal_error', 'loc': ['body', 'messages', 3, 'typed-dict', 'role'], 'msg': "Input should be 'function'", 'input': 'tool', 'ctx': {'expected': "'function'"}, 'url': 'https://errors.pydantic.dev/2.7/v/literal_error'}]}
2024-12-23 13:23:07.510 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:23:07.723 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:08.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:08.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:08.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:08.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:21.775 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:21.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:21.868 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:21.868 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:21.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:23.365 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:23.436 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:23.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:23.440 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:23.440 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:25.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.007 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.008 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.916 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:26.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:35.378 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:35.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:35.433 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:35.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:35.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:37.653 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:23:44.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:44.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:44.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:44.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:23:44.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:24:03.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:24:03.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:24:03.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:24:03.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:24:03.646 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:25:54.892 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:25:54.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:25:55.021 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:25:55.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:25:55.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:25:55.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:25:57.180 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:26:49.122 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:26:49.205 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:26:49.206 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:26:49.207 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:27:08.807 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:27:08.808 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:08.861 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:08.871 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:09.093 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:25.041 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:27:25.044 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:25.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:28.856 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:28.863 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:28.920 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:28.920 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:28.922 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:28.922 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:28.932 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:29.446 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:29.735 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:29.919 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:29.928 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:29.936 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:29.944 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:30.625 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:27:40.643 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:27:40.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:40.678 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:27:44.576 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:44.593 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:44.593 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:46.325 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:46.341 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-23 13:27:46.898 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:46.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:46.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:46.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:46.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:51.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:51.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:51.506 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:51.507 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:51.508 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:57.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:57.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:57.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:57.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:57.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:58.439 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:58.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:58.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:58.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:27:58.516 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:28:02.822 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:28:02.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:28:02.893 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:28:02.893 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:28:02.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:30:36.493 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:30:36.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:30:36.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:30:36.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:30:36.605 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:30:36.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:30:44.492 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen:7b', 'ollama')
2024-12-23 13:30:44.521 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:31:16.241 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:31:16.244 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.490 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.529 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:16.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:24.319 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen:7b', 'ollama')
2024-12-23 13:31:24.333 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:31:54.734 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:54.810 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:54.810 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:54.812 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:54.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:56.932 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 13:31:57.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:57.064 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:57.106 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:57.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:31:57.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 13:32:04.758 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen:7b', 'ollama')
2024-12-23 13:32:04.789 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:39:57.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:57.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:57.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:57.219 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:57.219 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:58.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:58.085 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:58.128 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:58.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:39:58.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:49.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:49.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:49.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:49.612 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:49.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:50.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:50.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:50.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:50.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:50.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:41:57.752 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:41:57.773 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:42:21.241 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.583 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.626 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.700 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:21.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.543 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.544 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.792 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.830 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.868 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.905 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:30.942 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:36.920 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:37.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:37.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:37.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:37.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.401 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.776 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:42:43.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:08.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:08.929 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:08.930 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:08.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:08.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:17.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:17.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:17.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:17.938 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:17.939 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:18.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:18.650 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:18.685 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:18.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:18.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:24.543 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:43:24.581 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:43:31.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.065 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.360 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:32.474 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:44.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:44.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:44.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:44.938 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:44.940 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:45.221 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:45.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:45.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:45.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:45.393 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:43:52.226 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:43:52.239 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:44:04.062 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.195 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.528 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:04.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:30.631 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:30.781 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:30.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:30.783 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:30.783 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:31.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:31.531 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:31.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:31.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:31.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:38.533 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:44:38.572 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:44:43.040 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.227 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.228 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.537 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.620 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:43.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:50.096 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:44:50.135 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:44:58.682 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:58.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:58.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:58.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:58.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:59.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:59.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:59.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:59.266 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:44:59.312 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.412 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.413 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.413 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.679 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.727 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.802 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:45:16.846 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:04.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:04.256 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:04.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:04.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:04.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:10.219 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:10.433 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:10.433 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:10.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:10.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:11.112 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:11.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:11.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:11.220 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:11.256 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:18.351 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:46:18.399 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:46:29.368 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:29.605 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:29.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:29.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:29.612 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:29.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:29.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:29.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:30.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:30.083 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:37.626 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:46:37.667 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:46:44.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:44.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:44.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:44.620 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:44.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:44.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:44.922 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:44.959 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:45.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:45.045 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:52.442 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:46:52.473 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:46:53.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:46:53.710 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:47:08.377 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:47:13.537 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-23 13:47:23.410 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-23 13:47:32.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:47:41.475 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-23 13:48:05.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:05.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:05.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:05.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:05.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:05.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:06.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:06.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:06.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:06.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:07.350 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:07.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:12.550 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:22.112 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-23 13:48:36.311 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:48:36.348 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 13:48:36.350 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:48:36.350 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 13:48:47.635 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:53.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:53.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:55.426 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:55.778 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:48:59.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:48:59.491 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 13:49:04.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:04.362 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:05.293 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:05.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:05.338 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:05.338 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:05.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:14.222 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:14.531 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:20.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:49:22.593 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-23 13:49:39.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:06.910 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:07.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:07.358 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:07.358 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:07.358 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:07.358 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:10.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:10.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:10.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:10.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:10.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:16.343 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:16.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:16.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:16.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:16.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:29.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.010 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.896 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.923 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.950 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:30.979 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:31.006 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:37.191 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:50:37.202 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:50:39.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:40.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:50:43.844 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:08.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:37.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:38.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:38.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:38.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:38.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:38.617 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:39.904 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:40.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:40.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:40.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:40.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:42.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:42.306 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:42.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:42.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:42.308 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:51.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:52.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:52.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:52.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:52.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:53.542 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:53.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:53.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:53.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:53.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:55.518 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:55.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:55.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:55.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:51:55.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:07.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.617 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.644 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.676 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:08.703 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:14.994 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:52:15.016 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 13:52:23.708 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.022 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.024 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.145 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:24.263 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 13:52:30.524 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-23 13:52:30.544 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-23 14:02:32.995 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 14:02:33.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 14:02:33.395 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 14:02:33.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 14:02:33.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 14:02:33.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-23 14:02:35.571 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-23 14:08:02.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:02.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:02.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:02.956 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:02.956 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:03.725 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:03.775 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:03.812 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:03.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:03.876 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:19.123 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 14:08:19.195 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 14:08:19.196 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 14:08:19.196 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 14:08:36.548 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:50.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:50.626 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:56.315 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:08:57.218 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 14:09:06.632 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:06.672 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 14:09:12.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:13.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:13.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:13.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:13.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:24.587 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:24.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:24.660 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:24.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:24.663 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:27.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:27.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:27.871 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:27.872 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:27.872 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:30.543 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:30.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:30.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:30.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:30.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:33.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:33.974 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:33.974 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:33.987 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:33.988 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:48.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:48.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:48.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:48.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:09:48.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:16:28.091 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 14:16:28.137 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-23 14:16:28.138 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 14:16:28.139 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-23 14:17:18.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:32.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:32.065 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:36.369 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:37.248 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 14:17:45.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:45.882 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-23 14:17:52.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:52.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:52.350 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:52.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:52.352 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:57.440 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:57.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:57.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:57.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:57.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:59.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:59.555 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:59.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:59.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:17:59.558 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:02.228 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:02.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:02.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:02.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:02.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:04.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:04.228 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:04.228 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:04.228 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:04.230 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:05.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:05.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:05.904 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:05.905 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:05.905 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:15.294 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:15.358 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:15.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:15.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:18:15.360 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:25:49.635 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:25:49.711 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:25:49.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:25:50.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-23 14:25:50.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 10:12:46.012 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 10:12:46.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 10:17:46.202 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2024-12-24 10:22:46.215 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2024-12-24 10:27:46.230 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2024-12-24 10:27:46.242 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 10:27:48.371 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 10:27:48.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 10:27:48.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 10:27:48.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 10:27:48.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 12:16:59.256 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 12:16:59.256 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 12:21:59.360 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2024-12-24 12:26:59.389 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2024-12-24 12:31:59.395 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2024-12-24 12:31:59.395 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 12:32:01.472 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 12:32:01.472 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 12:32:01.472 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 12:32:01.472 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 12:32:01.472 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:33:45.543 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 13:33:45.605 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 13:33:45.621 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 13:33:45.621 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 13:40:48.882 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:40:48.882 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:40:48.939 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:40:48.954 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:40:49.555 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:03.873 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:41:03.873 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:03.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:07.165 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.165 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.196 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.196 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.196 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.196 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.212 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.665 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:07.993 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:08.181 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:08.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:08.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:08.212 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:08.895 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 13:41:19.121 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:41:19.121 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:19.155 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 13:41:24.523 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:24.532 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:24.536 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:29.144 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:29.159 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:41:29.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:29.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:29.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:29.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:41:29.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:42:07.180 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:42:08.011 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'missing', 'loc': ['body', 'messages'], 'msg': 'Field required', 'input': {'model': 'string', 'prompt': '恼羞成怒', 'echo': False, 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 1024, 'n': 1, 'presence_penalty': 0, 'stream': True, 'temperature': 0.01, 'top_p': 1}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}]}
2024-12-24 13:45:52.265 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:45:52.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:45:52.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:45:52.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:45:52.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:45:52.486 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:45:55.200 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:47:17.542 | ERROR    | chatchat.server.api_server.openai_routes:generator:106 - openai request error: Connection error.
2024-12-24 13:47:34.208 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:47:34.211 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:47:34.331 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:47:34.331 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:47:34.332 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:47:34.335 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:47:36.460 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:47:43.921 | ERROR    | chatchat.server.api_server.openai_routes:generator:106 - openai request error: Connection error.
2024-12-24 13:47:44.466 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:47:44.876 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:47:51.728 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:47:51.729 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:47:51.730 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:47:51.730 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-24 13:48:22.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.304 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.745 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.748 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:48:22.750 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:50:10.490 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:50:10.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:50:10.734 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:50:10.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:50:39.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:50:39.673 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:50:48.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:50:49.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:51:45.367 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:51:45.369 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:51:48.309 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 13:51:48.312 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-24 13:51:48.326 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-24 13:51:48.327 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-24 13:52:03.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 13:52:03.881 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-24 13:52:03.884 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-24 13:52:03.884 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-24 13:57:12.672 | ERROR    | chatchat.server.utils:wrap_done:46 - UnprocessableEntityError: Caught exception: Error code: 422 - {'detail': [{'type': 'missing', 'loc': ['body', 'messages'], 'msg': 'Field required', 'input': {'model': 'string', 'prompt': '恼羞成怒', 'echo': False, 'frequency_penalty': 0, 'logit_bias': {}, 'max_tokens': 1024, 'n': 1, 'presence_penalty': 0, 'stream': True, 'temperature': 0.01, 'top_p': 1}, 'url': 'https://errors.pydantic.dev/2.7/v/missing'}]}
2024-12-24 13:57:25.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:57:28.092 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-24 13:58:57.488 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-24 13:59:23.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:41.250 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-24 13:59:47.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:47.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:47.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:47.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:47.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:53.046 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:53.332 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 13:59:57.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:00:17.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:01:16.105 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:02:47.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:02:48.110 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:02:48.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:02:48.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:02:48.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:02:48.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:03.103 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:03.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:03.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:03.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:03.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:16.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:17.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:17.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:17.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:17.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:25.052 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:25.316 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:25.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:25.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:25.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:29.203 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:29.500 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:29.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:29.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:29.502 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:39.231 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:39.489 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:39.490 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:39.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:39.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:03:42.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:05:43.775 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:06:27.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:06:44.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:07:00.302 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:32.242 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:34.366 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:14:36.395 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:14:38.438 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:14:38.439 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:14:38.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:38.688 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:38.689 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:38.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:51.089 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:53.161 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:14:55.193 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:14:57.238 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:14:57.238 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:14:57.281 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:57.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:57.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:14:57.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:04.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:05.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:06.966 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:07.240 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:09.021 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:09.281 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:11.051 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:11.052 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:15:11.312 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:11.314 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:15:11.468 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:13.533 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:15.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:15.582 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:17.375 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:17.630 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:17.630 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:15:19.423 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:21.472 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:21.472 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:15:21.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:21.554 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:21.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:21.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:28.580 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:28.838 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:30.641 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:30.899 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:32.681 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:32.943 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:34.702 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:34.704 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:15:34.976 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:34.978 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:15:43.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:43.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:15:45.549 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:47.597 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:49.650 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:15:49.650 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:16:30.786 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 14:16:30.833 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 14:16:30.833 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 14:16:30.833 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 14:16:49.713 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:04.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:04.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:08.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:09.815 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 14:17:20.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:20.320 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 14:17:27.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:27.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:27.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:27.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:17:27.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:18:32.946 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:18:33.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:18:44.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:18:44.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:18:52.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:18:55.824 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-24 14:19:02.670 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:02.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:02.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:02.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:02.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:03.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:03.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:03.144 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:03.144 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:03.145 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:05.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:06.095 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:06.096 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:06.097 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:06.097 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:06.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:07.106 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:07.107 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:07.108 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:07.108 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:18.768 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:18.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:18.904 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:18.905 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:18.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:32.489 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:32.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:32.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:32.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:32.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:39.230 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:39.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:39.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:39.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:39.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:40.343 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:52.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:52.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:52.970 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:19:53.007 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:26.365 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:28.422 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:20:30.469 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:20:32.505 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:20:32.507 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:20:32.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:32.666 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:32.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:32.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:45.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:45.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:20:47.700 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:20:49.740 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:20:51.763 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2024-12-24 14:20:51.765 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2024-12-24 14:21:14.563 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 14:21:14.579 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 14:21:14.579 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 14:21:14.579 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 14:21:29.580 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:21:44.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:21:44.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:21:48.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:21:49.597 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 14:21:59.795 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:21:59.843 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 14:22:06.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:22:06.428 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:22:06.430 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:22:06.431 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:22:06.431 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:22:25.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 14:22:25.530 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:46:57.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:46:58.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:46:58.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:46:58.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:46:58.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:47:01.288 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:47:02.127 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:47:06.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:47:13.355 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-24 15:47:33.247 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:02.875 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:02.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:03.203 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:03.203 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:03.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:03.205 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:04.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:05.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:05.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:05.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:05.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:05.803 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:06.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:06.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:06.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:06.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:11.260 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:11.464 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:11.466 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:11.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:11.468 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:16.600 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:16.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:16.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:16.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 15:48:16.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:02:41.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:02:41.447 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:02:41.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:02:41.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:02:41.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:02:42.710 | WARNING  | chatchat.server.chat.chat:chat_iterator:254 - streaming progress has been interrupted by user.
2024-12-24 16:03:08.260 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:08.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:08.335 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:08.374 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:08.411 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:35.741 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 16:03:35.747 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 16:03:35.762 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 16:03:35.763 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 16:03:50.779 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:51.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:51.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:51.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:03:51.061 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:04:33.326 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 16:04:33.399 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 16:04:33.401 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 16:04:33.402 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 16:05:04.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:18.074 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:18.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:23.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:24.042 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 16:05:32.832 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:32.878 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 16:05:42.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:42.273 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:42.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:42.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:42.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:47.241 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:47.295 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:47.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:47.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:47.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:48.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:48.223 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:48.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:48.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:48.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:52.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:52.863 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:52.864 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:52.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:05:52.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:06:00.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:06:00.655 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:06:00.656 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:06:00.656 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:06:00.657 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:08:26.736 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 16:08:26.738 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 16:08:26.738 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 16:08:26.739 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 16:08:44.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:08:58.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:08:58.657 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:03.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:03.939 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 16:09:12.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:12.907 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 16:09:18.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:18.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:18.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:18.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:18.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:23.629 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:23.708 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:23.708 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:23.709 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:23.710 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:24.528 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:24.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:24.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:24.577 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:24.578 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:28.795 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:28.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:28.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:28.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:28.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:42.201 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:42.250 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:42.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:42.253 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:09:42.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:23:15.967 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 16:23:16.014 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 16:23:16.014 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 16:23:16.014 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 16:23:31.479 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:23:44.296 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:23:44.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:23:48.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:23:49.424 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 16:23:58.073 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:23:58.120 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 16:24:05.073 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:05.155 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:05.155 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:05.155 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:05.155 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:06.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:06.404 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:06.404 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:06.404 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:06.404 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:28.264 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:28.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:28.329 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:28.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:28.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:29.144 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:29.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:29.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:29.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:29.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:32.766 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:32.813 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:32.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:32.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:32.816 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:46.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:46.626 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:46.627 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:46.628 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:24:46.628 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:28:08.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:28:08.775 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:28:08.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:28:08.846 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:28:08.882 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:28:16.692 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 16:28:16.706 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 16:29:22.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:22.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:22.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:22.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:22.215 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:54.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:54.053 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:54.089 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:54.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:29:54.163 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:30:01.013 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 16:30:01.033 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 16:30:27.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:30:27.978 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:30:27.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:30:27.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:30:27.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:32:08.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:35:20.909 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:36:51.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:37:18.871 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:37:36.272 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:40:01.498 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 16:40:17.348 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 16:40:39.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:40:59.690 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:40:59.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:40:59.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:40:59.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:00.290 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:00.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:00.363 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:00.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:00.458 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:11.763 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 16:41:11.799 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 16:41:22.718 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:22.815 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:22.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:22.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:22.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:23.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:23.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:23.083 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:23.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:41:23.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:44:34.646 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 16:46:49.133 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 16:46:54.876 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:54.988 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:54.989 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:54.990 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:54.990 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:55.624 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:55.661 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:55.701 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:55.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:46:55.776 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 16:48:06.811 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:39:11.109 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 17:39:11.200 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 17:39:11.201 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 17:39:11.202 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 17:39:25.064 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:39:38.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:39:38.383 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:39:44.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:39:44.948 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 17:39:53.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:39:53.820 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 17:40:01.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:02.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:02.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:02.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:02.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:06.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:06.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:06.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:06.237 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:06.238 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:10.106 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:10.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:10.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:10.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:10.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:13.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:13.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:13.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:13.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:13.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:28.783 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:28.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:28.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:28.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:28.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:29.918 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:29.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:29.990 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:30.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:40:30.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:14.251 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:41:22.852 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:41:29.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.681 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.682 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.939 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:29.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:35.767 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:41:35.779 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:41:50.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.112 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.113 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.113 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.335 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.408 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:41:50.444 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:33.462 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:33.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:33.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:33.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:33.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:34.323 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:34.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:34.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:34.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:42:34.472 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:11.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:11.380 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:19.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:22.220 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-24 17:43:31.181 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:31.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:31.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:31.356 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:31.356 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:31.652 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2024-12-24 17:43:32.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:32.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:32.528 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:32.528 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:32.529 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:39.394 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:39.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:39.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:39.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:39.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:41.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:42.053 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:42.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:42.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:42.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:52.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.082 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.085 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.086 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.087 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.722 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:53.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:43:54.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:01.382 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:44:01.407 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:44:17.104 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.287 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.290 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.290 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.292 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:17.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:33.106 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:33.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:33.277 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:33.278 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:33.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:36.051 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:36.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:36.238 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:36.238 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:36.239 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.788 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.823 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.875 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.922 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:46.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:44:54.969 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:44:54.988 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:45:26.753 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:26.947 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:26.948 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:26.950 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:26.951 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:27.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:27.527 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:27.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:27.605 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:45:27.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:46:58.911 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:47:07.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:21.703 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:47:21.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:21.963 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:21.965 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:21.968 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:41.966 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.364 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.412 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.454 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:47:42.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:06.237 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:48:10.755 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:48:21.470 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:21.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:21.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:21.596 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:21.598 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:23.045 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:23.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:23.120 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:23.121 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:23.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:25.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:25.923 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:25.923 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:25.924 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:25.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:29.325 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:29.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:29.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:29.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:29.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:32.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:32.815 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:32.815 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:32.816 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:32.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:32.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:32.993 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:33.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:33.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:48:33.116 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:02.767 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:06.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:06.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:06.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:06.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:07.329 | WARNING  | chatchat.server.chat.chat:chat_iterator:254 - streaming progress has been interrupted by user.
2024-12-24 17:49:07.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:07.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:07.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:07.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:07.701 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:19.188 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:49:19.218 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:49:21.425 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:49:30.527 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:49:39.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:39.742 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:39.742 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:39.742 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:39.742 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.253 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.510 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.578 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:52.580 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:55.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:55.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:55.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:55.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:55.302 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:58.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:58.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:58.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:58.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:49:58.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:03.586 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:03.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:03.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:03.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:03.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.135 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.792 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:50:15.907 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:07.104 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 17:51:07.154 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 17:51:07.156 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 17:51:07.157 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 17:51:22.078 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:35.583 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:35.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:40.163 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:41.093 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 17:51:50.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:50.752 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 17:51:56.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:56.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:56.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:56.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:56.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:58.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:58.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:58.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:58.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:51:58.643 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:01.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:01.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:01.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:01.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:01.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:03.304 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:03.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:03.374 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:03.374 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:03.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:13.514 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:13.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:13.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:13.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:13.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:14.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:14.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:14.629 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:14.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:14.700 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:24.048 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:31.241 | WARNING  | chatchat.server.chat.chat:chat_iterator:254 - streaming progress has been interrupted by user.
2024-12-24 17:52:31.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:31.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:31.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:31.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:52:36.791 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:52:36.805 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:53:03.771 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:03.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:03.893 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:03.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:03.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:04.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:04.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:04.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:04.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:04.702 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:13.654 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:16.845 | WARNING  | chatchat.server.chat.chat:chat_iterator:254 - streaming progress has been interrupted by user.
2024-12-24 17:53:16.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:16.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:16.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:16.882 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:23.989 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 17:53:24.008 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 17:53:24.347 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.423 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.583 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.663 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.700 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 17:53:24.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:02:24.384 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 18:02:24.462 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 18:02:24.462 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 18:02:24.462 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 18:02:41.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:02:55.100 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:02:55.165 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:00.337 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:01.275 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 18:03:10.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:10.615 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 18:03:17.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:17.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:17.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:17.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:17.302 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:23.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:23.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:23.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:23.423 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:23.424 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:30.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:30.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:30.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:30.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:30.120 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:32.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:32.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:32.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:32.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:32.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.635 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:03:42.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:05:23.871 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 18:07:32.352 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 18:10:49.835 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:50.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:50.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:50.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:50.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:50.876 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:50.938 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:51.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:51.792 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:10:51.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:32.432 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 18:11:39.779 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 18:11:49.205 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.281 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.446 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:11:49.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:12:31.804 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 18:12:40.717 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 18:16:35.612 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:35.719 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:35.720 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:35.722 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:35.724 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:36.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:36.407 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:36.433 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:36.479 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:16:36.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:36.581 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 18:19:44.510 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 18:19:50.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.277 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.277 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.459 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:19:50.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:22:59.538 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 18:24:09.669 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 18:24:09.751 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 18:24:09.753 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 18:24:09.754 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 18:24:46.368 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:00.377 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:00.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:04.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:05.826 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 18:25:14.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:14.296 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 18:25:21.807 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:21.888 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:21.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:21.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:21.893 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:27.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:27.908 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:27.909 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:27.910 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:27.910 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:32.500 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:32.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:32.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:32.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:32.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:33.475 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:33.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:33.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:33.552 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:33.553 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.090 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.140 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.325 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.408 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.446 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:25:49.481 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:29:10.729 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 18:31:14.384 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 18:31:14.416 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-24 18:31:14.416 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 18:31:14.416 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-24 18:31:31.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:31:45.032 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:31:45.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:31:49.223 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:31:50.096 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 18:31:58.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:31:58.791 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-24 18:32:04.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.445 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.553 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.559 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:04.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:52.696 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:52.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:52.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:52.761 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:52.765 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.287 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.288 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.921 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.983 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:32:54.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:01.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:01.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:01.104 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:01.104 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:01.105 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:11.771 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:11.844 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:11.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:11.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:11.846 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:13.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:13.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:13.364 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:13.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:13.436 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-24 18:33:40.570 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-24 18:33:40.587 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-24 18:57:29.724 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 18:57:29.734 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 18:57:29.734 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-24 18:57:29.734 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:29.734 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:29.734 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:30.812 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:30.812 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:30.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:30.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:30.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:30.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:31.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:31.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:31.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:31.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:31.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-24 18:57:31.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 08:55:14.093 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 08:55:14.125 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 08:55:14.125 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 08:55:14.125 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 08:55:26.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:55:41.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:55:42.052 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:55:47.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:55:48.116 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 08:55:57.809 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:55:57.825 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 08:56:04.455 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:04.548 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:04.549 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:04.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:04.551 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:32.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:32.115 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:32.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:32.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:32.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:55.910 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:55.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:55.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:55.978 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:56:55.979 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:16.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:16.671 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:16.672 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:16.674 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:16.674 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:22.404 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:22.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:22.497 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:22.500 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:22.500 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.277 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.367 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.368 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.369 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.619 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.655 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 08:57:30.692 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:00:29.050 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-25 09:00:55.862 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-25 09:05:10.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:10.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:10.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:10.419 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:10.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:10.962 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:10.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:11.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:11.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:05:11.106 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:08:19.192 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 09:08:19.244 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 09:08:19.246 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 09:08:19.247 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 09:08:33.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:08:46.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:08:46.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:08:51.337 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:08:52.220 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 09:09:00.538 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:00.568 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 09:09:06.560 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:06.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:06.660 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:06.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:06.663 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:07.992 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:08.043 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:08.044 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:08.046 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:08.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:12.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:12.560 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:12.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:12.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:12.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:16.522 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:16.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:16.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:16.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:16.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.560 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.862 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:09:31.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:10:18.014 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-25 09:11:33.013 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-25 09:13:32.417 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 09:13:32.433 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 09:13:32.433 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 09:13:32.433 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 09:13:43.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:13:56.145 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:13:56.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:00.334 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:01.202 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 09:14:09.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:09.544 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 09:14:15.267 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:15.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:15.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:15.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:15.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:19.532 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:19.586 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:19.586 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:19.587 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:19.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:32.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:32.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:32.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:32.161 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:32.162 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:36.673 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:36.727 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:36.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:36.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:36.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.253 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.253 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.254 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.468 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.509 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.586 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:14:40.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:47.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:47.767 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:47.768 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:47.768 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:47.769 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:47.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:48.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:48.083 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:48.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:15:48.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:42.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:42.743 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:42.743 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:42.743 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:42.743 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:43.243 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:43.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:43.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:43.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:20:43.391 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:23:33.708 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-25 09:24:49.344 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 09:24:49.411 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-25 09:24:49.413 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 09:24:49.414 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-25 09:25:37.781 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:25:52.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:25:53.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:25:58.650 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:25:59.821 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 09:26:10.346 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:10.410 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-25 09:26:16.666 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:16.748 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:16.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:16.750 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:16.751 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:23.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:23.559 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.115 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.488 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.555 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:28.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:36.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:36.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:36.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:36.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:26:36.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:06.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:06.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:06.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:06.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:06.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.514 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.613 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:08.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:25.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:26.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:26.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:26.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:26.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:29.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:29.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:29.214 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:29.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:29.217 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.165 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.230 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.232 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.412 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.451 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.488 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:27:39.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:28:12.085 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-25 09:30:00.605 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-25 09:30:09.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:09.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:09.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:09.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:09.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:09.914 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:09.950 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:09.987 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:10.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:30:10.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:36:18.962 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-25 09:36:35.584 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-25 09:38:24.965 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.801 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.847 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.888 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.924 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:38:25.964 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:16.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:16.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:16.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:16.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:16.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:16.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:17.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:17.070 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:17.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:17.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:45.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:45.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:45.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:45.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:45.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:46.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:46.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:46.244 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:46.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:46.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:39:58.625 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-25 09:39:58.634 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-25 09:41:29.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.308 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.862 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.898 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:29.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:30.008 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.471 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.772 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:41:45.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:01.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:01.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:01.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:01.200 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:01.201 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:07.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:07.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:07.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:07.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:07.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.647 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.689 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:42:15.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:03.363 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:03.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:03.507 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:03.510 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:03.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:04.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:04.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:04.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:04.244 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:04.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:16.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:17.040 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:17.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:17.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:17.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.894 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.940 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:25.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-25 09:43:32.705 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-25 09:43:32.715 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-25 10:02:17.721 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-25 10:02:17.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 10:02:17.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 10:02:17.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 10:02:17.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 10:02:17.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 11:12:09.907 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-25 11:12:09.918 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 11:12:10.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 11:12:10.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 11:12:10.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-25 11:12:10.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-26 09:20:36.144 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-26 09:20:36.144 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-26 09:20:36.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-26 09:20:36.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-26 09:20:36.442 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-26 09:20:36.445 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 10:21:26.946 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 10:21:27.009 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 10:21:27.009 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 10:21:27.009 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 10:21:39.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:21:46.858 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:21:46.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:21:49.497 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:21:49.904 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 10:21:54.429 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:21:54.444 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 10:22:02.312 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:02.403 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:02.405 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:02.405 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:02.406 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:07.262 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:07.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:11.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:11.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:40.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:22:43.499 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest/vector_store/bge-m3' from disk.
2024-12-27 10:23:40.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:41.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:41.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:41.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:41.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:41.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:42.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:43.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:43.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:43.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:43.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:56.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:56.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:56.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:56.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:56.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:57.219 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:57.325 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:57.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:57.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:57.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:58.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:58.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:58.211 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:58.212 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:23:58.212 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:00.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:00.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:00.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:00.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:00.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:45.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:45.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:45.633 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:45.633 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:45.633 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:24:45.633 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:19.167 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:19.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:19.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:19.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:19.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:33.672 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:33.779 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:33.781 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:33.781 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:25:33.781 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:08.261 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:08.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.138 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.271 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.272 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.272 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.273 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.470 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.577 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:11.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:12.253 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:17.678 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:28.097 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:41.651 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:27:58.674 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:28:07.007 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:28:07.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:28:07.823 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:28:07.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:28:07.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:28:07.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:29:17.454 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:29:17.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:29:17.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:29:17.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:29:17.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:14.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:14.818 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:16.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:17.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:17.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:17.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:17.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:32:17.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:21.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:21.701 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:24.723 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:25.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:25.483 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:25.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:25.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:33:25.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:27.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:28.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:28.227 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:28.227 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:28.228 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:39.502 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:39.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:39.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:39.879 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:39.881 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:49.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:50.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:50.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:50.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:50.015 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:51.697 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:51.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:51.844 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:51.871 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:51.899 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:34:58.343 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-27 10:34:58.478 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-27 10:35:31.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.337 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.377 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.457 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:35:32.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:08.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:08.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:08.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:08.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:08.537 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:09.411 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:09.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:09.474 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:09.506 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:09.541 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:16.928 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-27 10:36:16.946 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-27 10:36:45.387 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.908 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.938 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.968 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:45.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:46.027 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:36:53.682 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2024-12-27 10:36:53.697 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2024-12-27 10:37:08.334 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:08.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:25.592 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:26.089 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:26.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:26.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:26.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:26.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:31.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:31.605 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:44.627 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:44.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:46.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:46.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:46.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:37:46.502 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:38:49.538 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:38:49.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:38:57.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:38:57.676 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:41:03.908 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:41:03.908 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:41:04.471 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:41:04.472 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:41:04.473 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:41:04.474 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 10:56:59.744 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 10:56:59.748 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 10:57:00.589 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 10:57:00.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 10:57:04.136 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 10:57:04.329 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:49:52.719 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 12:49:52.799 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 12:49:52.799 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 12:49:52.799 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 12:50:02.261 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:50:02.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:50:02.311 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:02.315 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:41.510 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:50:41.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:50:41.550 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:41.550 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:41.554 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:49.225 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:50:49.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:50:49.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:50:51.018 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.022 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.044 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.044 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.044 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.044 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.048 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.316 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:50:51.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.596 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.600 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.604 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:50:51.916 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 12:50:57.770 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:50:57.774 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:50:57.795 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 12:51:02.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:51:02.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:51:02.498 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:51:04.099 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:51:04.102 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:51:04.316 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:04.458 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:04.462 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:04.462 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:04.462 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:07.269 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:07.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:07.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:07.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:07.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:07.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:08.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:08.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:08.043 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:51:08.043 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 12:58:54.166 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:58:54.166 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:58:54.170 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:03.040 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:03.041 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.342 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.342 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.552 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.553 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.738 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.738 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.914 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:04.915 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:07.637 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:59:07.638 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:59:07.647 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:59:07.652 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:59:07.653 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 12:59:08.066 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 12:59:08.155 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 12:59:08.155 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 12:59:08.238 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 12:59:08.409 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 12:59:08.530 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:08.530 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:12.533 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:12.534 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:12.615 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'test/vector_store/quentinz/bge-large-zh-v1.5' from disk.
2024-12-27 12:59:14.654 | ERROR    | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:140 - Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000170515DB7F0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-27 12:59:14.654 | ERROR    | chatchat.server.knowledge_base.kb_api:create_kb:44 - RuntimeError: 创建知识库出错： 向量库 test 加载失败。
2024-12-27 12:59:15.437 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:15.437 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:15.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 12:59:15.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:53:35.466 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 13:53:35.557 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 13:53:35.561 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 13:53:35.561 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 13:53:50.053 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 13:53:50.053 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:53:50.076 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:53:50.081 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:53:50.081 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:53:59.843 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 13:53:59.843 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:53:59.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:01.995 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:01.995 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.029 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.029 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.029 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.029 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.033 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.311 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:02.599 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.603 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.607 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.610 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:02.957 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 13:54:09.772 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 13:54:09.772 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:09.787 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 13:54:15.070 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:15.071 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:15.075 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:17.166 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:17.170 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:17.471 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:17.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:17.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:17.732 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:17.733 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.120 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.882 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.956 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.956 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:25.957 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 13:54:31.261 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:31.262 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:43.457 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:43.459 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:46.035 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 13:54:46.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:54:46.170 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:55:12.814 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 13:55:12.816 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:55:12.816 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:55:15.039 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 13:55:15.127 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 13:55:15.426 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:55:15.427 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:56:39.988 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 13:56:39.988 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:56:39.988 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:57:01.869 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:57:01.870 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 13:57:03.943 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:03:01.011 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 14:03:01.044 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 14:03:01.044 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 14:03:01.044 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 14:05:44.549 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:05:44.549 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:05:44.612 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:05:44.620 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:05:44.628 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:01.948 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:06:01.948 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:01.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:06.575 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:06.583 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:06.622 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:06.622 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:06.622 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:06.622 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:06.632 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:07.250 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:07.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:08.155 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:08.167 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:08.186 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:08.202 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:09.681 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 14:06:22.760 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:06:22.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:22.781 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 14:06:28.293 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:28.313 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:28.324 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:31.414 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:31.426 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:31.998 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:31.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.264 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.271 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.273 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.273 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:32.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:34.727 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:34.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:35.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:35.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:35.020 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:06:42.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:42.338 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:50.675 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:50.676 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:59.226 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:06:59.226 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:09:35.732 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 14:09:35.773 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 14:09:35.774 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 14:09:35.774 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 14:10:17.285 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:10:17.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:10:17.346 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:17.358 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:17.371 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:35.268 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:10:35.269 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:10:35.303 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:10:40.127 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:40.139 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:40.188 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:40.188 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:40.188 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:40.188 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:40.201 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:40.841 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:41.202 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:10:41.506 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:41.514 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:41.522 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:41.537 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:10:42.455 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 14:10:56.958 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:10:56.959 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:10:56.981 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 14:11:06.005 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:06.017 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:06.030 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:08.957 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:08.973 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:09.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.652 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.883 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.896 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.929 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:09.938 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:35.888 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:35.889 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:48.819 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:48.821 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:11:59.369 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:11:59.373 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:11:59.924 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:11:59.924 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:12:00.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:12:00.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:12:00.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:12:00.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:16:34.190 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:16:34.202 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:16:48.907 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/create_knowledge_base: timed out
2024-12-27 14:16:49.782 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:16:49.783 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:17:09.731 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:17:09.732 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:17:31.018 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:17:31.027 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:17:31.010 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:17:35.579 | ERROR    | chatchat.server.knowledge_base.kb_api:create_kb:44 - FileExistsError: 创建知识库出错： [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\deepLearningCode\\Langchain-Chatchat\\libs\\chatchat-server\\chatchat\\data\\knowledge_base\\test02\\content'
2024-12-27 14:17:35.580 | ERROR    | chatchat.server.knowledge_base.kb_api:create_kb:44 - FileExistsError: 创建知识库出错： [WinError 183] 当文件已存在时，无法创建该文件。: 'C:\\deepLearningCode\\Langchain-Chatchat\\libs\\chatchat-server\\chatchat\\data\\knowledge_base\\test02\\content'
2024-12-27 14:21:19.595 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 14:21:19.634 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 14:21:19.634 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 14:21:19.638 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 14:21:45.171 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:21:45.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:21:45.201 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:45.201 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:45.210 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:55.242 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:21:55.242 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:21:55.250 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:21:57.642 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:57.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:57.686 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:57.686 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:57.686 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:57.688 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:57.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:58.062 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:58.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:21:58.421 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:58.430 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:58.434 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:58.439 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:21:58.865 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 14:22:05.830 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 14:22:05.830 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:22:05.838 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 14:22:09.966 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:22:09.969 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:22:09.972 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:22:12.139 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:22:12.144 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 14:22:12.486 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:22:12.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:22:12.583 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:22:12.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:22:12.585 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 14:31:19.368 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\test02\content\day10-领取优惠券.docx
2024-12-27 14:34:48.717 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001D2A8602DD0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-27 14:39:05.680 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\test03\content\航旅纵横实习生活指南1.docx
2024-12-27 14:41:04.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 14:41:04.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 14:41:04.639 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 14:41:04.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 14:41:04.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-27 15:13:21.636 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:13:21.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:13:22.480 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:13:22.481 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:13:22.481 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:13:22.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:13:31.630 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:13:31.631 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:13:43.473 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:13:43.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:13:50.412 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:13:52.658 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:13:54.951 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:13:54.973 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:13:55.252 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2024-12-27 15:14:02.803 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:02.897 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:03.203 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:06.426 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:06.523 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:06.711 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:13.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:14:13.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:14:13.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:14:13.666 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:14:13.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:14:37.029 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:14:37.030 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:42.606 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:42.732 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:46.420 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:46.504 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:14:46.761 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:15:26.490 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:15:32.098 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:15:32.107 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:15:32.506 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:15:32.522 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\test03\content\航旅纵横实习生活指南1.docx
2024-12-27 15:15:34.463 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:15:34.467 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-27 15:15:34.502 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:34:40.291 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:34:40.304 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:34:41.390 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:34:41.392 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:34:41.392 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:34:41.393 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:35:39.140 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:35:49.415 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:35:49.415 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:35:51.923 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:35:51.932 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:35:52.024 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:35:52.045 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\test03\content\航旅钱包客服问题.docx
2024-12-27 15:35:53.055 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:35:53.056 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-27 15:35:53.066 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-27 15:43:44.998 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 15:43:45.066 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-27 15:43:45.066 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 15:43:45.070 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-27 15:44:00.234 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:44:00.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:00.267 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:00.271 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:00.271 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:10.906 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:44:10.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:10.920 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:13.738 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:13.742 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:13.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:13.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:13.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:13.794 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:13.802 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:14.155 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:14.379 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:14.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:14.496 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:14.500 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:14.501 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:14.835 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 15:44:21.930 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:44:21.930 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:21.938 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-27 15:44:28.423 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:28.427 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:28.430 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:30.699 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:30.703 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-27 15:44:31.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:31.511 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:31.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:31.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:31.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:41.087 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:44:41.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:49:33.502 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 15:49:33.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:49:34.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:49:34.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:49:34.060 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 15:49:34.060 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 17:01:46.780 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-27 17:01:46.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 17:01:47.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 17:01:47.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 17:01:47.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-27 17:01:47.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:14:35.798 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:14:35.861 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:14:35.861 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:14:35.861 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:17:32.008 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:17:32.009 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:17:32.068 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:17:32.076 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:18:09.308 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:18:09.308 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:18:09.355 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:18:09.355 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:08.381 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:19:08.382 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:19:08.420 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:08.420 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:08.436 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:23.805 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:19:23.805 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:19:23.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:19:27.772 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:27.788 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:27.835 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:27.835 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:27.835 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:27.835 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:27.835 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:28.873 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:58.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:19:58.405 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:58.420 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:58.420 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:58.436 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:19:59.196 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:20:10.475 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:20:10.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:20:10.496 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:20:16.917 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:20:16.934 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:20:16.951 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:20:19.499 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:20:19.516 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:20:20.167 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:22:52.255 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:22:52.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:22:52.371 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:22:52.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:46:38.541 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:46:38.572 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:46:38.572 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:46:38.572 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:47:42.803 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:47:42.803 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:47:42.928 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:47:42.928 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:47:42.944 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:47:58.173 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:47:58.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:47:58.188 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:01.884 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:01.899 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:01.931 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:01.931 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:01.931 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:01.931 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:01.946 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:02.431 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:02.694 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:02.884 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:02.884 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:02.899 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:02.899 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:03.683 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:48:13.940 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:48:13.940 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:13.956 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:48:18.511 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:18.529 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:18.544 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:20.774 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:20.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:48:21.254 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:21.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:21.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:21.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:21.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:48:58.109 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:48:58.109 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:48:58.109 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:48:58.109 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:49:10.796 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:49:10.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:10.828 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:10.843 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:10.843 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:27.248 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:49:27.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:27.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:31.532 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:31.548 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:31.579 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:31.579 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:31.579 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:31.579 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:31.594 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:32.079 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:32.360 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:32.547 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:32.563 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:32.563 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:32.579 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:33.360 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:49:44.129 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:49:44.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:44.145 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:49:49.891 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:49.909 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:49.910 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:52.541 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:52.557 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:49:53.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:53.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:53.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:53.312 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:49:53.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:06.984 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:50:07.000 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:50:07.000 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:50:07.000 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:50:20.517 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:50:20.517 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:20.563 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:20.565 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:20.565 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:36.496 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:50:36.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:36.512 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:40.312 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:40.328 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:40.368 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:40.368 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:40.368 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:40.368 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:40.383 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:40.882 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:41.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:41.376 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:41.392 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:41.408 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:41.408 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:42.210 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:50:53.056 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:50:53.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:53.071 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:50:56.988 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:57.004 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:57.021 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:59.237 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:59.237 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:50:59.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:59.822 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:59.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:59.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:50:59.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:51:43.561 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:51:43.561 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 10:51:43.561 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:51:43.561 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 10:51:56.303 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:51:56.303 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:51:56.365 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:51:56.365 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:51:56.381 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:11.184 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:52:11.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:11.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:14.983 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:14.983 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.014 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.014 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.014 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.014 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.030 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.493 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.773 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:15.945 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.945 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.961 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:15.961 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:16.762 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:52:27.527 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 10:52:27.527 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:27.542 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 10:52:31.422 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:31.422 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:31.436 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:33.801 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:33.817 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 10:52:34.391 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:34.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:34.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:34.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 10:52:34.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:03:41.275 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:03:41.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:03:41.842 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:03:41.842 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:03:51.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:03:51.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:18:39.946 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 11:18:39.993 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 11:18:39.993 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 11:18:39.993 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 11:19:03.907 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:19:03.907 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:19:03.961 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:03.961 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:03.976 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:28.461 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:19:28.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:19:28.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:19:32.305 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:32.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:32.359 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:32.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:32.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:32.361 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:32.361 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:32.896 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:33.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:19:33.366 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:33.366 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:33.381 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:33.381 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:19:34.134 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 11:20:24.326 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:20:24.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:20:24.344 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 11:20:28.663 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:20:28.663 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:20:28.679 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:20:30.911 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:20:30.928 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:20:31.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:20:31.510 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:20:31.510 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:20:31.510 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:20:31.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:22:01.070 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:22:01.070 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:22:01.070 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:22:29.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:22:29.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:22:31.188 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:22:31.190 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:22:35.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:22:35.486 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:23:30.976 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:24:25.889 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:24:58.903 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:24:58.978 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest01\content\接口文档.docx
2024-12-30 11:25:42.603 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001923EC82A40>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 11:25:44.681 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:27:25.282 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:27:33.184 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:27:41.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:41.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:41.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:41.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:41.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:55.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:55.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:55.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:55.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:27:55.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:30:40.695 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:30:40.695 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:30:40.695 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:30:51.542 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:30:51.545 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:30:59.343 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:30:59.345 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:31:01.722 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:31:13.014 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest02/vector_store/quentinz/bge-large-zh-v1.5' from disk.
2024-12-30 11:31:15.083 | ERROR    | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:140 - Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001924129DDE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 11:31:15.083 | ERROR    | chatchat.server.knowledge_base.kb_api:create_kb:44 - RuntimeError: 创建知识库出错： 向量库 MyTest02 加载失败。
2024-12-30 11:31:35.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:31:35.983 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:31:35.986 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:31:35.989 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:31:35.990 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:31:39.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:31:39.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 11:31:47.050 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 11:31:47.050 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:31:47.050 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:31:47.050 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:31:47.050 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:32:18.034 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:32:18.036 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:32:34.429 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:32:34.432 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 11:32:36.653 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 13:13:34.183 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 13:13:34.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 13:13:34.902 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 13:13:34.902 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 13:13:53.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 13:13:53.697 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 13:17:10.105 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 13:17:10.436 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 13:20:16.762 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest03\content\接口文档.docx
2024-12-30 13:20:52.961 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000019241258F10>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 13:21:38.063 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest03\content\接口文档.docx
2024-12-30 13:22:14.763 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001923E6122F0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 13:22:46.405 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest03\content\接口文档.docx
2024-12-30 13:23:22.748 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000019242328EE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 13:23:50.777 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 13:23:50.825 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 13:23:50.828 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 13:23:50.829 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 13:24:01.133 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:06.968 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:06.978 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:09.459 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:09.873 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 13:24:14.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:14.110 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 13:24:22.113 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.194 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.194 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.194 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.194 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.323 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.324 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.324 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:22.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:23.304 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:23.364 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:23.364 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:23.364 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:24:23.365 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:25:31.329 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\接口文档.docx
2024-12-30 13:27:50.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:27:50.711 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:27:55.242 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:27:55.430 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:28:06.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:28:23.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:29:06.267 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:29:17.787 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:31:03.143 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest05/vector_store/bge-m3' from disk.
2024-12-30 13:31:15.125 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest05\content\接口文档.docx
2024-12-30 13:31:49.904 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('MyTest05', 'bge-m3') 保存到磁盘
2024-12-30 13:32:16.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:32:17.140 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:32:20.633 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:32:20.861 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:32:40.346 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:32:42.746 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002037B8F9510>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 13:32:42.747 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002037B8F9510>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 13:33:08.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:33:20.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 13:33:31.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:04:23.265 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 14:04:23.288 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:04:28.661 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:04:32.292 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:04:46.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:04:49.537 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 14:04:51.582 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002037B8D9660>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 14:04:51.586 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002037B8D9660>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 14:04:59.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:05:07.647 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:05:08.294 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 14:05:08.294 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-30 14:05:08.294 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-30 14:28:44.608 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 14:28:44.655 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 14:28:44.655 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 14:28:44.655 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 14:29:15.279 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 14:29:15.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:15.326 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:15.326 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:15.342 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:31.950 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 14:29:31.950 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:31.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:36.203 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:36.203 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:36.250 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:36.250 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:36.250 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:36.250 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:36.265 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:36.733 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:37.044 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:37.232 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:37.247 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:37.247 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:37.263 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:38.012 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 14:29:49.093 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 14:29:49.093 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:49.108 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 14:29:53.408 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:53.424 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:53.434 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:55.723 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:55.740 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 14:29:56.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:56.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:56.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:56.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:29:56.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:03.074 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:03.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:03.132 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:03.133 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:03.133 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:04.087 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:04.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:04.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:04.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:04.160 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:43.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:43.139 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:43.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:43.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:43.144 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:47.137 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:30:47.383 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 14:31:45.253 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:32:07.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:33:21.910 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest05/vector_store/bge-m3' from disk.
2024-12-30 14:33:31.168 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 14:35:04.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:35:17.621 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 14:35:23.893 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:35:31.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:35:32.798 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 14:35:47.383 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:35:47.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:35:52.923 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:35:56.612 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:07.971 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 14:36:12.644 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:12.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:13.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:13.247 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:13.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:13.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:17.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:17.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:17.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:17.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:17.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:36:25.642 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2024-12-30 14:36:55.106 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 14:36:55.161 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 14:36:55.162 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 14:36:55.163 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 14:37:07.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:21.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:21.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:26.532 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:27.718 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 14:37:36.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:36.632 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 14:37:43.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:43.656 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:43.658 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:43.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:43.660 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:45.583 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:45.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:52.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:37:56.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:38:06.290 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest05/vector_store/bge-m3' from disk.
2024-12-30 14:38:39.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:42:00.516 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:47:36.459 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 14:47:40.649 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:03.611 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:00:03.679 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:00:03.680 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:00:03.682 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:00:18.121 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:31.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:31.597 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:36.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:37.484 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:00:46.058 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:46.074 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:00:53.776 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:53.818 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:00:53.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:53.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:53.857 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:53.858 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:58.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:58.749 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:00:59.048 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:00:59.093 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:02.290 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:02.335 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:02.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:02.539 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:05.002 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:05.045 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:05.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:05.251 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:08.347 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:08.394 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:08.542 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:08.601 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:11.950 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:11.993 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:12.162 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:12.206 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:27.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:27.131 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:01:27.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:01:27.367 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:35:21.928 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:35:21.973 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:35:21.975 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:35:21.975 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:35:38.508 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 15:35:38.508 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:35:38.558 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:38.566 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:38.575 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:55.800 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 15:35:55.801 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:35:55.816 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:35:59.644 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:59.644 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:59.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:59.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:59.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:59.691 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:35:59.707 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:00.191 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:00.490 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:36:00.688 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:00.688 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:00.704 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:00.720 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:01.479 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:36:12.992 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 15:36:12.992 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:36:13.007 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:36:16.945 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:16.961 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:16.961 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:18.893 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:18.910 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:36:19.377 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:36:19.409 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:36:19.456 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:36:19.458 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:36:19.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:36:19.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:38:25.270 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:38:25.524 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:38:42.076 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:38:59.947 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:38:59.995 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:00.175 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:00.214 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:03.072 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.090 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:03.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.127 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.130 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.321 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.372 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:03.402 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.404 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.405 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:03.407 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:06.179 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:06.238 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:06.468 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:06.516 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:07.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.471 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:07.497 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.502 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.503 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.691 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.723 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:07.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:07.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:15.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:15.388 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:39:15.419 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:15.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:15.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:15.421 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:39:30.202 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:39:30.213 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:39:30.213 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:39:30.214 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:39:50.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:04.079 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:04.110 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:08.726 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:09.695 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:40:20.586 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:20.618 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:40:28.046 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:28.080 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:40:28.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:28.127 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:28.128 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:40:28.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:44:31.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:44:32.088 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:44:36.666 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Extra data: line 1 column 5 (char 4)
2024-12-30 15:49:22.438 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:49:22.453 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 15:49:22.453 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:49:22.453 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 15:49:38.143 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 15:49:38.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:49:38.205 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:38.221 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:38.221 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:53.375 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 15:49:53.375 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:49:53.390 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:49:57.139 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.155 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.186 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.186 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.186 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.186 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.208 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.693 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:57.986 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:49:58.161 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:58.177 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:58.177 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:58.192 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:49:58.938 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:50:09.637 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 15:50:09.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:50:09.653 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 15:50:13.291 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:50:13.307 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:50:13.307 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:50:15.498 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:50:15.498 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 15:50:15.993 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:50:16.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:50:16.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:50:16.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:50:16.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:50:30.692 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:50:30.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 15:54:03.162 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:55:19.085 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 15:57:12.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:23:18.936 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:23:18.967 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:23:18.967 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:23:18.983 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:23:35.951 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 16:23:35.951 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:23:35.982 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:35.998 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:35.998 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:51.598 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 16:23:51.598 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:23:51.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:23:55.744 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:55.744 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:55.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:55.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:55.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:55.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:55.807 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:56.299 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:56.632 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:23:56.830 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:56.846 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:56.846 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:56.861 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:23:57.568 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:24:08.416 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 16:24:08.416 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:08.436 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:24:12.218 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:24:12.218 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:24:12.235 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:24:14.268 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:24:14.285 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2024-12-30 16:24:14.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.964 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:14.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:16.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:17.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:17.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:17.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:24:17.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2024-12-30 16:26:24.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:26:24.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:27:17.082 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:27:17.331 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:27:26.270 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:27:34.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:29:36.363 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'MyTest05/vector_store/bge-m3' from disk.
2024-12-30 16:29:45.644 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 16:32:25.772 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:32:34.695 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:33:08.069 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 16:34:52.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:35:46.666 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 16:36:56.882 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:37:06.465 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:37:26.906 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2024-12-30 16:40:11.217 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:40:11.269 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:40:11.270 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:40:11.271 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:40:26.321 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:40:40.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:40:40.914 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:40:46.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:40:47.108 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:40:56.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:40:56.332 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:41:03.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:05.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:05.823 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:15.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:23.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:41:48.692 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:44:25.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:44:46.389 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:44:46.421 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:44:46.422 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:44:46.422 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:44:57.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:11.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:12.008 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:16.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:17.738 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:45:26.726 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:26.750 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:45:33.470 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.591 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:33.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:35.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:35.600 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:35.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:35.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:35.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:44.176 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:44.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:45:56.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:16.845 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230F2B163E0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 16:46:16.847 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x00000230F2B163E0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 16:46:41.929 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:41.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:42.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:42.002 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:42.002 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:46.395 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:46.456 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:46.457 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:46.458 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:46.458 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:55.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:46:55.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:47:01.470 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:47:05.697 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:47:07.027 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:47:10.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:33.878 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:48:33.925 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:48:33.925 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:48:33.925 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:48:40.377 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:46.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:46.970 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:49.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:49.392 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:48:53.390 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:53.406 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:48:58.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:58.241 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:58.241 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:58.241 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:48:58.241 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:00.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:00.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:00.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:00.070 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:00.070 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.075 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.716 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:01.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:03.497 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:03.840 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:06.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:11.295 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:14.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:14.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:14.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:14.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:14.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:16.463 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:16.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:20.707 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:27.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:37.830 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:38.125 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2024-12-30 16:49:38.993 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:39.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:40.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:49:47.657 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:50:12.237 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:08.820 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:51:08.834 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2024-12-30 16:51:08.835 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:51:08.836 | INFO     | __main__:start_main_server:326 - Process status: %s
2024-12-30 16:51:14.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:20.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:20.442 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:22.446 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:22.846 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:51:26.776 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:26.776 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2024-12-30 16:51:31.423 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.423 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.675 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:31.688 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:34.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:34.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:44.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:47.361 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E682AADA0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 16:51:47.363 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E682AADA0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 16:51:53.093 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:51:57.670 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:07.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:08.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:12.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:21.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:21.989 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:26.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:43.916 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:52:49.292 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:56:02.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:56:07.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 16:57:52.721 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 16:57:52.721 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 16:58:01.849 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 16:58:07.420 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 16:58:15.295 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 16:58:17.655 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 16:58:17.660 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 16:58:17.696 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 16:58:17.764 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\打补丁步骤.docx
2024-12-30 16:59:23.766 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 16:59:23.766 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 16:59:23.766 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-30 16:59:25.870 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2024-12-30 16:59:25.874 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:15.392 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:16.754 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:24.147 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:24.274 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:24.322 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:24.323 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-30 17:24:24.878 | ERROR    | chatchat.webui_pages.utils:ret_sync:208 - RemoteProtocolError: API通信遇到错误：peer closed connection without sending complete message body (incomplete chunked read)
2024-12-30 17:24:25.021 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:25.135 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:42.509 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:42.630 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:42.654 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:42.655 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-30 17:24:42.660 | ERROR    | chatchat.webui_pages.utils:ret_sync:208 - RemoteProtocolError: API通信遇到错误：peer closed connection without sending complete message body (incomplete chunked read)
2024-12-30 17:24:42.824 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:42.950 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:52.069 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:52.238 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:24:52.468 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\打补丁步骤.docx
2024-12-30 17:25:32.129 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:25:32.145 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-30 17:25:32.541 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:25:32.657 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:25:48.471 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:25:48.643 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:25:48.690 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\打补丁步骤.docx
2024-12-30 17:26:26.638 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:26:26.638 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2024-12-30 17:26:26.787 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:26:26.918 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2024-12-30 17:26:59.413 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\打补丁步骤.docx
2024-12-30 17:28:43.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:28:43.451 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:28:46.420 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:28:46.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:28:50.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:28:56.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:28:56.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:29:03.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:29:23.013 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2024-12-30 17:29:23.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:29:24.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:29:24.928 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:29:25.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:29:31.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:30:26.060 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:12.613 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:28.458 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:36.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:36.820 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2024-12-30 17:31:37.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:38.263 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:38.695 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:39.219 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:39.538 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:40.361 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:40.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:31:45.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:32:00.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:32:26.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:33:51.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:33:51.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:34:34.455 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:34:34.696 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:35:57.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:35:57.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:00.801 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:01.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:16.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:20.380 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E07C4E710>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 17:36:20.384 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E07C4E710>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 17:36:25.145 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:28.325 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E09C522C0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 17:36:28.327 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E09C522C0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 17:36:44.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:44.888 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:44.889 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:44.889 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:44.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:50.379 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:50.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:55.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:36:58.091 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E09C6A470>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 17:36:58.093 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000025E09C6A470>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2024-12-30 17:37:03.723 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:37:09.237 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:37:27.542 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:37:36.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:37:36.820 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2024-12-30 17:37:36.887 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:37:47.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:38:01.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:38:49.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:39:24.104 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2024-12-30 17:39:42.379 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-01 10:28:04.426 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:28:04.440 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:28:04.469 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:28:04.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:28:07.269 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-01 10:28:07.269 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:28:07.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:28:07.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:28:07.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:32:45.073 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 10:32:45.133 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 10:32:45.133 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 10:32:45.133 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 10:33:06.781 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:33:06.781 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:06.822 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:06.828 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:06.828 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:22.618 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:33:22.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:22.634 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:27.463 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:27.479 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:27.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:27.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:27.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:27.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:27.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:28.148 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:28.489 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:28.692 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:28.692 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:28.708 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:28.708 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:29.435 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 10:33:41.131 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:33:41.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:41.146 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 10:33:47.128 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:47.146 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:47.148 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:52.491 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:52.506 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 10:33:53.238 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:53.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:53.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:53.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:33:53.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:09.995 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:10.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:27.167 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:27.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:29.462 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:33.034 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:34:35.066 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020A186FEBF0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-01-01 10:34:35.069 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000020A186FEBF0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-01-01 10:34:43.532 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:34:43.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:45.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:34:45.933 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 10:34:47.255 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 10:34:47.285 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 10:34:47.286 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-01 10:34:47.287 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-01 10:35:16.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:16.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.743 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:18.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:21.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:35:21.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:37:35.283 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:37:35.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:37:35.383 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:37:35.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:37:41.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:37:41.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:38:34.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:38:34.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:38:34.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:38:34.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:38:34.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:38:34.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:38:41.181 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:38:41.182 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:38:41.182 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 10:38:41.185 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 10:43:03.066 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:43:03.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 10:43:03.358 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 10:43:03.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:42:07.063 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 15:42:07.117 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 15:42:07.117 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 15:42:07.117 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 15:42:26.685 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:42:26.685 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:42:26.731 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:26.731 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:26.747 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:41.566 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:42:41.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:42:41.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:42:45.610 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:45.626 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:45.657 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:45.673 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:45.673 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:45.673 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:46.218 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:46.687 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:46.967 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:42:47.170 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:47.170 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:47.185 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:47.201 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:42:47.928 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 15:42:58.370 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:42:58.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:42:58.386 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 15:43:04.032 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:43:04.033 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:43:04.050 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:43:08.146 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:43:08.163 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 15:43:08.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:43:08.898 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:43:08.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:43:08.901 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:43:08.902 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 15:43:14.870 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:43:14.873 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:43:27.965 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:43:32.633 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:43:33.779 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:43:33.823 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx
2025-01-01 15:43:35.315 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/大唐数据初始化的一些相关操作.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx'
2025-01-01 15:43:35.354 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:44:03.676 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:44:03.677 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:44:03.711 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:44:03.745 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx
2025-01-01 15:44:04.126 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/大唐数据初始化的一些相关操作.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx'
2025-01-01 15:44:06.195 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:44:06.196 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:44:50.775 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:44:50.810 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:44:50.844 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx
2025-01-01 15:44:51.236 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/大唐数据初始化的一些相关操作.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx'
2025-01-01 15:44:51.269 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:46:08.725 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:46:08.725 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:46:08.776 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:46:08.813 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx
2025-01-01 15:46:09.193 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/大唐数据初始化的一些相关操作.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx'
2025-01-01 15:46:11.248 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:46:11.249 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:47:25.685 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:47:25.686 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:49:55.207 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:49:55.207 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:52:23.586 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 15:52:25.740 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:52:25.740 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:54:42.392 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:54:42.393 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:57:23.632 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 15:57:25.753 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:57:25.753 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:58:21.379 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 15:59:40.301 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 15:59:42.429 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 15:59:42.429 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:00:21.772 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:00:21.810 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:00:21.848 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx
2025-01-01 16:00:22.336 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/大唐数据初始化的一些相关操作.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\大唐数据初始化的一些相关操作.docx'
2025-01-01 16:00:24.526 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:00:24.543 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:02:23.654 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 16:02:23.657 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-01 16:03:21.293 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 16:03:23.407 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:03:23.407 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:04:40.328 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 16:04:42.443 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:04:42.443 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:08:21.297 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 16:08:23.451 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:08:23.451 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:09:40.352 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 16:09:40.352 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-01 16:13:21.331 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 16:13:21.331 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-01 16:18:45.096 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:18:45.158 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:18:45.158 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:18:45.158 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:21:51.707 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:21:51.707 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:21:51.738 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:21:51.754 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:21:51.754 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:06.874 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:22:06.874 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:06.889 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:10.786 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:10.786 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:10.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:10.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:10.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:10.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:11.386 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:11.807 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:12.079 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:12.282 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:12.282 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:12.298 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:12.314 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:13.081 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:22:23.602 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:22:23.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:23.618 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:22:27.909 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:27.926 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:27.926 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:30.472 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:30.490 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:22:31.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:31.175 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:31.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:31.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:31.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:22:46.848 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:22:46.852 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:23:03.173 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:23:09.118 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:23:09.119 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:23:10.807 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-01 16:23:10.824 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:23:13.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:13.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:13.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:13.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:13.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:15.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:15.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:15.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:15.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:15.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:23:34.162 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:23:34.222 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-01 16:23:36.270 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:23:36.271 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:29:05.538 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:29:05.585 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:29:05.585 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:29:05.585 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:29:22.871 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:29:22.871 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:29:22.918 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:22.934 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:22.934 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:38.624 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:29:38.624 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:29:38.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:29:42.373 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:42.373 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:42.404 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:42.404 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:42.404 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:42.404 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:42.967 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:43.389 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:43.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:29:43.815 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:43.830 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:43.830 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:43.846 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:29:44.620 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:29:56.485 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:29:56.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:29:56.501 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:30:00.422 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:30:00.438 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:30:00.438 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:30:02.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:30:02.588 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:30:03.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:30:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:30:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:30:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:30:03.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:30:11.581 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:30:11.585 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:30:25.481 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:30:29.472 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:30:30.628 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:30:30.655 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 16:30:31.787 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 16:30:31.836 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:31:11.781 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:31:11.782 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:32:34.665 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:32:34.665 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:32:34.737 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 16:32:35.227 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 16:32:37.371 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:32:37.385 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:34:11.466 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:34:11.497 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:34:11.497 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:34:11.497 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:34:25.676 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:34:25.676 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:34:25.792 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:25.792 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:25.808 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:41.382 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:34:41.382 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:34:41.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:34:45.811 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:45.811 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:45.842 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:45.842 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:45.842 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:45.842 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:46.367 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:46.836 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:47.116 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:34:47.320 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:47.336 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:47.336 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:47.352 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:34:48.109 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:34:58.612 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:34:58.612 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:34:58.619 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:35:02.488 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:35:02.504 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:35:02.504 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:35:04.777 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:35:04.786 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:35:05.295 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:35:05.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:35:05.398 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:35:05.398 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:35:05.398 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:35:08.743 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:35:08.740 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:35:15.050 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:35:18.960 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:38:35.964 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:38:36.011 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:38:36.011 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:38:36.011 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:38:50.071 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:38:50.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:38:50.118 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:38:50.134 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:38:50.134 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:06.188 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:39:06.188 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:06.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:09.880 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:09.880 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:09.912 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:09.912 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:09.927 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:09.927 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:10.427 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:10.833 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:11.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:11.290 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:11.305 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:11.305 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:11.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:12.071 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:39:23.650 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:39:23.650 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:23.665 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:39:30.187 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:30.187 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:30.203 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:32.652 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:32.652 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:39:33.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:33.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:33.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:33.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:33.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:41.998 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:39:41.997 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:39:54.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:54.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:54.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:54.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:54.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:56.039 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:56.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:56.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:56.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:56.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:39:56.386 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:39:59.322 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:44:46.615 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:44:46.647 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:44:46.662 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:44:46.662 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:45:05.465 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:45:05.465 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:05.505 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:05.521 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:05.521 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:22.234 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:45:22.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:22.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:27.848 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:27.848 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:27.895 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:27.895 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:27.895 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:27.895 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:28.550 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:28.957 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:29.264 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:29.459 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:29.459 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:29.474 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:29.474 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:30.216 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:45:40.839 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:45:40.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:40.854 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:45:45.160 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:45.177 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:45.177 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:47.575 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:47.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:45:48.093 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:48.195 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:48.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:48.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:48.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:45:53.748 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:45:53.748 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:46:03.020 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:46:06.698 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:47:11.173 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:47:11.188 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:47:11.188 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:47:11.188 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:47:27.846 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:47:27.846 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:47:27.877 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:27.893 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:27.893 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:42.630 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:47:42.630 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:47:42.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:47:46.344 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:46.359 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:46.391 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:46.391 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:46.391 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:46.391 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:46.875 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:47.279 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:47.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:47:47.788 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:47.803 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:47.819 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:47.819 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:47:48.606 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:47:58.928 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:47:58.928 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:47:58.944 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 16:48:02.632 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:48:02.632 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:48:02.650 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:48:05.497 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:48:05.514 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:48:05.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:48:06.106 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:48:06.108 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:48:06.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:48:06.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:48:11.842 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:48:11.843 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:48:19.353 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:48:22.771 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:49:45.632 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:49:45.632 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:49:45.670 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 16:49:46.754 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 16:49:48.986 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:49:48.986 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 16:59:10.337 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:59:10.369 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 16:59:10.369 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:59:10.369 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 16:59:28.890 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:59:28.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:59:28.937 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:28.937 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:28.952 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:43.939 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 16:59:43.939 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:59:43.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:59:47.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:47.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:47.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:47.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:47.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:47.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:48.355 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:48.762 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:49.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 16:59:49.219 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:49.234 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:49.250 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:49.265 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 16:59:50.044 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:00:00.423 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:00:00.423 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:00:00.438 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:00:04.631 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:00:04.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:00:04.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:00:06.992 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:00:06.992 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:00:07.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:00:07.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:00:07.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:00:07.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:00:07.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:02:06.092 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:02:06.093 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:02:06.094 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:02:06.095 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:02:14.971 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:02:19.830 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:02:19.830 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:03:29.587 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:03:29.634 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:03:29.634 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:03:29.634 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:03:47.237 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:03:47.238 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:03:47.278 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:03:47.298 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:03:47.310 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:03.141 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:04:03.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:03.162 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:07.283 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:07.299 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:07.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:07.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:07.337 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:07.337 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:07.897 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:08.328 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:08.599 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:08.783 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:08.793 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:08.793 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:08.814 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:09.508 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:04:20.864 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:04:20.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:20.882 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:04:24.951 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:24.967 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:24.968 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:27.617 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:27.640 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:04:28.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:28.381 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:28.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:28.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:28.387 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:35.247 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:04:35.254 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:04:43.311 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:04:44.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:45.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:45.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:45.007 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:45.007 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:46.121 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:46.222 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:46.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:46.227 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:46.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:04:47.775 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:05:24.288 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:05:24.288 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:05:24.297 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:05:24.298 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:05:42.055 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:05:42.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:05:42.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:05:42.110 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:05:42.111 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:05:57.930 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:05:57.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:05:57.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:01.696 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:01.696 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:01.740 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:01.740 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:01.740 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:01.740 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:02.264 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:02.645 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:02.887 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:03.079 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:03.086 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:03.086 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:03.112 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:03.814 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:06:14.410 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:06:14.411 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:14.430 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:06:18.372 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:18.388 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:18.406 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:20.554 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:20.568 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:06:21.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.073 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.195 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.195 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:21.203 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:23.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:23.424 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:23.426 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:23.428 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:23.429 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:06:27.917 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:06:27.933 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:06:35.411 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:06:38.644 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:08:01.949 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:08:01.981 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:08:01.983 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:08:01.983 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:11:26.918 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:11:26.918 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:26.939 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:26.939 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:26.943 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:35.508 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:11:35.511 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:35.543 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:38.094 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:38.094 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:38.131 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:38.132 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:38.132 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:38.132 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:38.512 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:39.533 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:39.743 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:39.822 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:39.824 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:39.824 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:39.824 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:40.109 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:11:46.579 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:11:46.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:46.604 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:11:50.172 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:50.172 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:50.172 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:51.918 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:51.919 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:11:52.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:52.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:52.200 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:52.201 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:11:52.202 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:12:02.546 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:12:02.546 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:12:11.504 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:12:14.427 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:12:15.673 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:12:15.702 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 17:12:16.870 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 17:12:16.902 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:16:07.058 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:16:07.078 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:16:07.079 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:16:07.080 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:16:25.466 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:16:25.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:16:25.514 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:25.514 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:25.532 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:41.539 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:16:41.540 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:16:41.556 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:16:45.231 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:45.248 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:45.284 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:45.285 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:45.285 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:45.286 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:45.828 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:46.194 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:46.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:16:46.686 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:46.689 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:46.689 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:46.712 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:16:47.424 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:16:58.094 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:16:58.095 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:16:58.112 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:17:01.845 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:17:01.846 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:17:01.860 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:17:04.026 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:17:04.044 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:17:04.620 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:04.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:04.733 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:04.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:04.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:33.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:34.017 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:34.024 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:34.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:34.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:34.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:35.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:35.045 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:35.046 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:35.046 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:17:52.882 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:17:52.882 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:18:02.125 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:18:02.126 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:18:05.338 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:18:05.339 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:18:07.378 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:18:19.826 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 17:18:21.003 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 17:19:00.027 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:19:13.700 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:19:13.701 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:19:39.150 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:19:42.835 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:19:42.836 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:19:42.906 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:19:42.941 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 17:19:43.312 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 17:19:43.341 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:20:08.452 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:20:08.529 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:20:08.572 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 17:20:08.978 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 17:20:09.023 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:20:17.391 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:20:17.393 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:20:20.802 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:20:20.903 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:20:21.000 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx
2025-01-01 17:20:21.000 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用户权限分配.docx
2025-01-01 17:20:21.850 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用友项目描述.docx'
2025-01-01 17:20:21.850 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用户权限分配.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用户权限分配.docx'
2025-01-01 17:20:21.899 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:22:08.737 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:22:08.738 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:22:10.486 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:22:10.489 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:22:58.144 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:23:34.295 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用户权限分配.docx
2025-01-01 17:23:34.757 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 MyTest04/用户权限分配.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\用户权限分配.docx'
2025-01-01 17:24:01.709 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:24:01.710 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:48:31.914 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:48:31.959 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:48:31.961 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:48:31.962 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:48:50.215 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:48:50.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:48:50.291 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:48:50.291 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:48:50.309 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:06.668 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:49:06.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:06.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:10.601 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:10.617 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:10.663 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:10.664 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:10.664 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:10.665 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:11.285 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:11.715 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:12.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:12.272 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:12.280 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:12.299 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:12.299 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:13.564 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:49:26.248 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:49:26.250 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:26.268 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:49:31.230 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:31.242 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:31.252 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:33.842 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:33.854 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:49:34.443 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:34.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:34.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:34.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:34.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:53.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:53.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:53.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:53.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:53.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:54.132 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:54.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:54.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:54.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:49:54.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:51:09.847 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:51:09.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:51:10.830 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:51:10.830 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:51:10.876 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:51:10.880 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:51:10.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:51:10.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:51:10.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:51:10.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:51:29.481 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:51:29.482 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 17:51:29.483 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:51:29.483 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 17:52:21.409 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:52:21.410 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:52:21.456 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:21.457 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:21.457 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:37.666 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:52:37.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:52:37.695 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:52:41.357 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:41.373 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:41.406 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:41.406 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:41.407 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:41.408 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:41.937 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:42.328 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:42.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:52:42.783 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:42.795 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:42.795 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:42.819 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:43.568 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:52:54.214 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:52:54.215 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:52:54.234 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 17:52:58.303 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:58.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:52:58.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:53:00.637 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:53:00.651 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:53:01.133 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:53:01.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:53:01.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:53:01.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:53:01.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 17:53:48.794 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:53:48.794 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:53:59.450 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:53:59.451 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:54:02.416 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 17:54:02.417 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:54:03.588 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-01 17:54:03.599 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 17:54:47.819 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 17:54:47.820 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 19:46:34.524 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 19:46:34.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 19:46:35.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 19:46:35.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 19:46:35.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 19:46:35.801 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 19:56:13.037 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用户权限分配.docx
2025-01-01 19:56:15.599 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用户权限分配.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用户权限分配.docx'
2025-01-01 19:58:35.913 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 19:58:35.952 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:00:51.477 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:00:51.490 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:01:08.349 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:01:49.582 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用户权限分配.docx
2025-01-01 20:01:50.035 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用户权限分配.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用户权限分配.docx'
2025-01-01 20:04:12.742 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadError: error when post /knowledge_base/upload_docs: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-01-01 20:04:14.790 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:04:16.841 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:04:16.842 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-01 20:04:18.901 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:04:18.902 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:04:29.564 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:04:31.601 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:04:33.681 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:04:33.682 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-01 20:04:33.741 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:05:05.198 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:05:10.005 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:05:16.010 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:05:18.075 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:05:20.122 | ERROR    | chatchat.webui_pages.utils:post:87 - ConnectError: error when post /knowledge_base/upload_docs: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-01-01 20:05:20.123 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-01 20:05:22.206 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:05:22.207 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:05:39.812 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 20:05:39.860 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-01 20:05:39.862 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 20:05:39.863 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-01 20:06:04.018 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:06:04.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:04.066 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:04.066 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:04.085 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:19.558 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:06:19.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:19.613 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:23.913 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:23.922 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:23.970 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:23.971 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:23.972 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:23.972 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:24.684 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:25.104 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:25.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:25.753 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:25.763 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:25.777 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:25.791 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:26.581 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 20:06:37.398 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:06:37.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:37.417 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-01 20:06:42.269 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:42.279 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:42.287 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:44.687 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:44.709 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-01 20:06:45.220 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:45.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:45.324 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:45.325 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:45.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:06:48.544 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:06:48.544 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:07:03.554 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:07:08.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:07:08.877 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:07:08.881 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:07:08.883 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:07:08.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-01 20:07:12.869 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:07:13.230 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:07:17.974 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:07:23.716 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:07:23.717 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:07:25.170 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:08:46.492 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:08:47.564 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:10:30.560 | ERROR    | chatchat.server.utils:run_in_thread_pool:755 - error in sub thread: 'NoneType' object has no attribute 'result'
2025-01-01 20:10:32.631 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:10:32.632 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:10:37.584 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:10:37.586 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:10:37.661 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:11:02.198 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:11:02.579 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:15:35.332 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 20:15:37.463 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:15:37.463 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:15:37.518 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:17:16.942 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:17:17.402 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:17:19.510 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:17:19.511 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:17:31.790 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:17:31.792 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:18:06.735 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:18:06.773 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:18:07.163 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:21:38.326 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:21:38.329 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:21:50.178 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:21:50.184 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:21:50.304 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:21:50.334 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:21:50.910 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:21:51.022 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:22:03.768 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:22:03.816 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:22:03.846 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:22:04.228 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:22:04.252 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:22:13.511 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:22:13.562 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:22:13.588 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:22:13.974 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:22:14.015 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:27:12.874 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:27:12.890 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:28:43.196 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:28:43.206 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:29:28.272 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:29:28.726 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:31:19.352 | ERROR    | chatchat.server.utils:run_in_thread_pool:755 - error in sub thread: 'NoneType' object has no attribute 'result'
2025-01-01 20:31:21.563 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:31:21.586 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:31:35.092 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:31:35.095 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:31:45.986 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:31:57.631 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:32:16.982 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:32:17.037 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:32:21.597 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:32:21.651 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:32:32.462 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:32:33.375 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:32:45.151 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:32:45.152 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:32:52.065 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:32:52.067 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:32:52.176 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:33:15.952 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:37:20.914 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:37:20.915 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:37:20.986 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:37:34.310 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:37:34.307 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:42:18.844 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 20:42:20.938 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:42:20.938 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:42:20.969 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:42:20.985 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-01 20:45:21.338 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:45:21.690 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/用友项目描述.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx'
2025-01-01 20:45:23.807 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:45:23.807 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:45:34.105 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:45:34.105 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:45:34.172 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:50:32.059 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 20:50:34.238 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:50:34.238 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:50:34.285 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:55:32.126 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 20:55:34.202 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 20:55:34.202 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 20:55:34.258 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-01 21:00:32.143 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-01 21:00:32.152 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-01 21:00:34.552 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-01 21:00:34.567 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 08:52:04.527 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 08:52:04.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 08:52:05.673 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-02 08:52:05.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 08:52:05.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 08:52:05.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 08:52:05.721 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:01:43.058 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 09:01:43.120 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 09:01:43.136 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 09:01:43.136 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 09:01:59.172 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:01:59.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:01:59.212 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:01:59.212 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:01:59.228 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:14.403 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:02:14.403 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:14.419 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:18.276 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:18.292 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:18.339 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:18.339 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:18.339 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:18.339 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:18.936 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:19.392 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:19.689 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:19.875 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:19.875 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:19.890 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:19.890 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:20.642 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 09:02:31.226 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:02:31.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:31.241 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 09:02:34.982 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:34.998 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:34.998 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:37.148 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:37.164 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:02:37.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:37.831 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:37.831 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:37.831 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:02:37.831 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:03:07.039 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:03:07.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:03:07.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:03:07.186 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:03:07.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 09:22:57.890 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:22:57.892 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:23:18.104 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:23:24.897 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:23:24.899 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:23:27.185 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-02 09:23:27.207 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:23:37.976 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:23:38.008 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-02 09:23:38.043 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:24:47.251 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:24:47.252 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:24:50.022 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:24:54.065 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:24:54.066 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:24:54.089 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-02 09:24:54.106 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:27:55.731 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:27:55.732 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:27:55.861 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:32:53.621 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 09:32:55.944 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:32:55.946 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:32:56.015 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:36:55.645 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\打补丁步骤.docx
2025-01-02 09:36:55.661 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\打补丁步骤.docx
2025-01-02 09:36:58.884 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/打补丁步骤.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\打补丁步骤.docx'
2025-01-02 09:36:58.894 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:419 - PackageNotFoundError: 从文件 mytest/打补丁步骤.docx 加载文档时出错：Package not found at 'C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\打补丁步骤.docx'
2025-01-02 09:37:01.264 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:37:01.288 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:37:15.625 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:37:15.626 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:37:15.626 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:37:15.626 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 09:37:38.720 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:37:44.984 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:37:49.596 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:37:53.594 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 09:37:53.595 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 09:37:53.660 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-02 09:37:53.682 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:18:06.338 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:18:06.407 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:18:06.407 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:18:06.407 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:18:21.765 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:18:21.766 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:18:21.807 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:21.807 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:21.822 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:36.820 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:18:36.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:18:36.836 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:18:40.956 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:40.956 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:41.003 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:41.003 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:41.003 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:41.003 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:41.559 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:41.991 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:42.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:18:42.604 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:42.620 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:42.620 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:42.635 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:18:43.404 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:18:56.659 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:18:56.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:18:56.676 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:19:01.667 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:19:01.667 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:19:01.683 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:19:03.965 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:19:03.965 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:19:04.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:19:04.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:19:04.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:19:04.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:19:04.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:19:17.285 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:19:17.285 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:19:36.303 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:19:43.491 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:19:43.492 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:19:44.735 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:19:50.975 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:23:39.647 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:23:39.678 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:23:39.678 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:23:39.678 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:23:54.605 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:23:54.605 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:23:54.651 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:23:54.659 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:23:54.669 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:09.854 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:24:09.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:09.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:14.071 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:14.071 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:14.119 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:14.119 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:14.120 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:14.121 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:14.674 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:15.103 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:15.402 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:15.598 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:15.613 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:15.629 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:15.629 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:16.421 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:24:28.860 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:24:28.860 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:28.875 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:24:33.328 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:33.329 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:33.346 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:35.461 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:35.479 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:24:35.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:36.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:36.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:36.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:24:36.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:25:07.135 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:25:07.139 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:25:21.544 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:25:26.299 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:25:26.299 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:25:27.430 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:25:27.489 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:26:34.223 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:26:34.269 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:26:34.269 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:26:34.269 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:26:51.584 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:26:51.585 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:26:51.628 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:26:51.639 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:26:51.648 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:07.059 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:27:07.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:07.075 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:10.920 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:10.920 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:10.967 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:10.967 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:10.967 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:10.967 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:11.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:11.889 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:12.169 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:12.344 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:12.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:12.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:12.375 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:13.100 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:27:23.615 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:27:23.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:23.632 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:27:28.082 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:28.082 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:28.098 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:31.861 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:31.878 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:27:32.378 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:32.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:32.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:32.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:32.478 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:27:36.037 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:27:36.040 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:27:44.072 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:27:48.571 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:27:49.720 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:27:49.777 | INFO     | chatchat.server.knowledge_base.utils:file2docs:443 - RapidOCRDocLoader used for 用友项目描述.docx
2025-01-02 10:27:49.777 | ERROR    | chatchat.server.knowledge_base.utils:files2docs_in_thread_file2docs:529 - TypeError: 从文件 mytest/用友项目描述.docx 加载文档时出错：get_loader() got an unexpected keyword argument 'file_path_or_url'
2025-01-02 10:27:49.797 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:53:22.339 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:53:22.388 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 10:53:22.388 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:53:22.388 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 10:53:37.286 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:53:37.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:53:37.328 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:37.328 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:37.343 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:54.099 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:53:54.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:53:54.115 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:53:58.030 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:58.030 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:58.062 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:58.077 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:58.077 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:58.077 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:58.616 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:59.110 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:59.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:53:59.667 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:59.667 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:59.683 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:53:59.690 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:54:00.505 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:54:11.499 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 10:54:11.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:54:11.516 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 10:54:17.447 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:54:17.497 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:54:17.514 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:54:19.927 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:54:19.930 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 10:54:20.497 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:54:20.602 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:54:20.605 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:54:20.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:54:20.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 10:54:25.960 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:54:25.966 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:54:34.792 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:54:38.035 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:54:39.171 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 10:54:39.229 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:02:52.630 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:02:52.642 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:02:52.768 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:04:42.397 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:04:42.407 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:05:01.201 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:05:01.203 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:05:01.254 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:05:36.641 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:05:39.203 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:05:39.256 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:06:10.720 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 11:06:10.749 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 11:06:10.749 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 11:06:10.749 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 11:06:31.819 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:06:31.819 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:06:31.852 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:31.869 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:31.872 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:46.780 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:06:46.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:06:46.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:06:50.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:50.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:50.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:50.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:50.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:50.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:51.034 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:51.427 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:51.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:06:51.888 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:51.905 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:51.921 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:51.921 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:06:52.701 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 11:07:03.150 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:07:03.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:07:03.166 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 11:07:07.267 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:07:07.267 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:07:07.283 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:07:09.432 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:07:09.432 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:07:09.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:07:10.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:07:10.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:07:10.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:07:10.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:07:19.769 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:07:19.772 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:07:30.811 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:07:37.049 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:07:38.465 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:11.363 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:09:11.364 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:11.396 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:13.509 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:09:13.510 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:23.871 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:23.931 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:23.995 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:32.237 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:09:32.324 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:09:32.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:09:32.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:09:32.329 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:09:34.148 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:34.438 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:38.957 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:40.407 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:40.443 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:40.488 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:53.336 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:53.376 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:09:53.438 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:10:00.035 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:10:03.763 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:10:05.220 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:10:05.272 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:10:05.344 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:10:43.738 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 11:10:43.754 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 11:10:43.754 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 11:10:43.754 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 11:10:58.121 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:10:58.121 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:10:58.168 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:10:58.184 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:10:58.184 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:13.558 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:11:13.559 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:13.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:18.546 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:18.554 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:18.576 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:18.576 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:18.576 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:18.576 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:19.157 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:19.593 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:19.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:20.162 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:20.172 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:20.186 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:20.198 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:20.992 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 11:11:32.135 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:11:32.136 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:32.151 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 11:11:36.033 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:36.049 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:36.049 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:37.998 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:37.998 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:11:38.470 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:38.548 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:38.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:38.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:11:38.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:17:48.057 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:17:48.058 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:17:48.058 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:17:48.062 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:17:59.079 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:18:05.047 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:18:05.048 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:18:08.263 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:18:08.370 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:18:34.105 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:18:34.157 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:18:34.206 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:19:07.479 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:19:07.480 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:19:07.530 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:21:37.413 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:21:37.413 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:37:42.451 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 11:37:42.515 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 11:37:42.515 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 11:37:42.515 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 11:38:01.387 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:38:01.387 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:01.418 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:01.434 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:01.449 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:17.383 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:38:17.383 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:17.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:21.392 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:21.392 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:21.439 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:21.439 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:21.439 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:21.439 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:22.475 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:22.939 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:23.260 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:23.611 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:23.627 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:23.627 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:23.643 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:24.507 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 11:38:35.544 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:38:35.544 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:35.560 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 11:38:41.557 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:41.724 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:41.740 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:46.120 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:46.136 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 11:38:46.723 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:46.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:46.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:46.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:46.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 11:38:51.863 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:38:51.864 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:39:02.017 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:39:05.177 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:39:06.405 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:40:39.672 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:40:39.672 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:40:43.002 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:40:43.003 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:40:43.042 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:45:40.936 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 11:45:43.102 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:45:43.102 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:45:43.132 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:50:40.986 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 11:50:43.048 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:50:43.049 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:50:43.113 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 11:55:41.006 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 11:55:41.022 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-02 11:55:43.404 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 11:55:43.416 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:15:16.311 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:15:16.374 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:15:16.374 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:15:16.374 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:15:31.327 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:15:31.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:15:31.358 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:31.382 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:31.393 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:47.331 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:15:47.331 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:15:47.365 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:15:51.265 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:51.281 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:51.315 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:51.315 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:51.315 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:51.331 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:51.888 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:52.299 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:52.672 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:15:52.854 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:52.870 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:52.870 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:52.885 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:15:53.627 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:16:04.143 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:16:04.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:16:04.165 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:16:08.873 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:16:08.882 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:16:08.893 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:16:11.771 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:16:11.771 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:16:12.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:16:12.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:16:12.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:16:12.388 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:16:12.389 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:16:28.194 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:16:28.193 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:16:54.020 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:16:59.060 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:16:59.060 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:17:00.192 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:17:00.250 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:20:09.533 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:20:09.564 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:20:09.564 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:20:09.564 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:20:24.511 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:20:24.511 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:20:24.542 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:24.558 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:24.558 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:39.373 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:20:39.373 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:20:39.388 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:20:43.063 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:43.079 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:43.110 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:43.110 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:43.110 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:43.110 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:43.632 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:44.042 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:44.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:20:44.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:44.540 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:44.540 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:44.556 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:20:45.343 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:20:56.180 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:20:56.180 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:20:56.211 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:21:01.914 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:21:01.931 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:21:01.931 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:21:05.972 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:21:05.972 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:21:06.475 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:21:06.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:21:06.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:21:06.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:21:06.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:21:14.754 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:21:14.754 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:21:22.386 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:21:29.909 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:21:31.072 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:21:33.353 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:24:02.955 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:24:02.957 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:24:03.246 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:29:00.873 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 13:29:02.936 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:29:02.938 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:29:02.973 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:31:17.697 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:31:17.718 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:31:23.159 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:31:23.159 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:31:23.193 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:33:16.206 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:33:16.298 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:33:16.299 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:33:16.300 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:33:32.486 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:33:32.486 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:33:48.859 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:33:48.874 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:33:48.874 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:03.712 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:34:03.712 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:34:03.727 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:34:49.682 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:49.697 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:49.744 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:49.744 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:49.744 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:49.744 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:50.300 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:50.685 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:50.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:34:51.150 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:51.166 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:51.174 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:51.174 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:34:51.928 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:35:03.709 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:35:03.709 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:35:03.724 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:35:11.132 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:35:11.132 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:35:11.148 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:35:13.082 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:35:13.097 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:35:13.581 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:35:13.681 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:35:13.681 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:35:13.697 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:35:13.698 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:35:19.638 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:35:19.635 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:35:28.815 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:35:35.868 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:35:35.869 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:35:37.799 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:40:32.489 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 13:40:34.584 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:40:34.585 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:40:34.657 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:42:33.195 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:42:33.216 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:42:33.217 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:42:33.218 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:42:46.849 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:42:46.849 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:42:50.214 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:42:50.230 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:42:50.230 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:05.111 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:43:05.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:05.128 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:12.946 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:12.962 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:12.995 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:12.995 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:12.995 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:12.995 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:13.508 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:13.869 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:14.130 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:14.304 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:14.319 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:14.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:14.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:15.129 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:43:25.532 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:43:25.532 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:25.548 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:43:34.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:34.353 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:34.353 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:36.400 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:36.416 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:43:36.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:36.985 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:36.985 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:37.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:37.002 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:43:46.013 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:43:46.017 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:44:07.800 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:44:15.494 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:44:15.495 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:44:16.727 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:46:23.622 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:46:23.645 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 13:46:23.646 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:46:23.646 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 13:55:46.385 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:55:46.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:55:46.417 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:55:46.433 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:55:46.433 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:02.014 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:56:02.015 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:02.044 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:06.723 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:06.723 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:06.771 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:06.771 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:06.771 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:06.771 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:07.301 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:07.734 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:08.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:08.238 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:08.254 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:08.254 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:08.270 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:09.053 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:56:19.479 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:56:19.479 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:19.494 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 13:56:23.187 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:23.187 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:23.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:25.654 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:25.668 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 13:56:26.260 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:26.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:26.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:26.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:26.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 13:56:31.527 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:56:31.531 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:56:45.146 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:56:48.456 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:56:50.247 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:57:43.540 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:57:43.541 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 13:57:43.541 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-02 13:57:53.736 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 13:57:53.737 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 14:00:10.791 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 14:00:10.792 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 14:00:34.544 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 14:00:44.269 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-02 14:05:42.718 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:43.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:50.004 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:50.201 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:50.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:50.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:50.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:53.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:05:53.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:06:01.274 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:06:35.324 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:06:53.455 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:06:54.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:06:54.767 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:06:59.885 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 14:07:43.634 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-02 14:08:18.044 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-02 14:43:43.188 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 14:43:43.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 14:43:43.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 14:43:43.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 14:43:43.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 14:43:43.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:12:42.578 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 17:12:42.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:23:27.999 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 17:23:28.058 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 17:23:28.058 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 17:23:28.058 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 17:23:44.538 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 17:23:44.538 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:23:45.147 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:23:45.152 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:23:45.168 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:00.113 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 17:24:00.113 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:00.128 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:04.524 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:04.524 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:04.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:04.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:04.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:04.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:04.587 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:04.995 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:05.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:05.486 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:05.486 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:05.502 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:05.502 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:06.282 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 17:24:17.146 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-02 17:24:17.146 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:17.161 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 17:24:23.370 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:23.370 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:23.387 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:25.806 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:25.833 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-02 17:24:26.528 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:26.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:26.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:26.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:24:26.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-02 17:27:01.263 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:27:01.462 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:27:06.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:27:20.115 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-02 17:27:21.202 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:27:21.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:27:29.710 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:27:38.796 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-02 17:35:28.110 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 17:40:28.168 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-02 17:40:37.569 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 17:40:37.611 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-02 17:40:37.611 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 17:40:37.611 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-02 17:40:50.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:04.214 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:04.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:09.378 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:10.407 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 17:41:19.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:19.118 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-02 17:41:28.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.444 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.444 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.479 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.629 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.630 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-02 17:41:28.632 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:01:36.262 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:01:36.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:01:39.774 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 09:01:40.074 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:01:40.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:01:40.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:01:40.107 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:02:49.317 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:02:49.318 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:02:59.951 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:03:04.217 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:03:04.256 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:03:04.744 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 09:03:04.782 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:03:29.113 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:03:29.143 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 09:03:29.167 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:06:00.254 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:06:00.265 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:06:00.513 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:06:40.006 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:06:40.050 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:08:06.706 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:08:06.708 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:08:06.759 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:08:51.538 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:08:51.582 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:08:51.582 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:08:51.582 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:09:40.172 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:09:40.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:09:41.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:09:41.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:09:41.227 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:09:57.246 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:09:57.247 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:09:57.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:02.459 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:02.460 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:02.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:02.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:02.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:02.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:02.520 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:02.966 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:03.292 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:03.501 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:03.512 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:03.521 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:03.530 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:04.289 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:10:15.169 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:10:15.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:15.185 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:10:22.349 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:22.364 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:22.364 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:27.752 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:27.769 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:10:28.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:28.619 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:28.621 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:28.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:28.623 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:10:32.469 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:10:32.472 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:10:42.169 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:10:47.169 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:10:48.367 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:11:48.284 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:11:48.286 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:11:48.381 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:11:58.343 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:11:58.344 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:12:00.200 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:12:00.233 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:12:41.024 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:16:46.544 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:16:46.666 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:16:47.149 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:20:10.011 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:20:10.046 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:20:10.047 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:20:10.048 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:20:25.949 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:20:25.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:20:26.495 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:26.510 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:26.510 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:42.123 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:20:42.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:20:42.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:20:47.123 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.123 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.185 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.185 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.653 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:47.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:20:48.131 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:48.131 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:48.146 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:48.146 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:20:48.955 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:20:59.824 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:20:59.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:20:59.839 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:21:04.440 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:21:04.440 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:21:04.455 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:21:06.639 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:21:06.655 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 09:21:07.242 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.259 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.506 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:07.507 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:09.465 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:09.554 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:09.554 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:09.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:09.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 09:21:58.527 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:22:14.323 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:22:14.324 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:22:19.328 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:22:19.329 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:22:24.228 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:25:55.126 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:25:55.146 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:25:55.150 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-03 09:26:51.595 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:26:51.620 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:27:38.136 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:27:38.136 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:27:38.189 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:27:51.096 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:27:51.096 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-03 09:27:51.121 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:28:02.423 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:28:02.468 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:28:02.972 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:28:02.972 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-03 09:28:05.036 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 09:28:05.037 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 09:33:14.367 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 09:46:53.038 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 09:47:36.761 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:47:36.811 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:47:36.811 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:47:36.811 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:47:51.529 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:07.587 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:07.619 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:12.808 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:13.755 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:48:24.180 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:24.195 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:48:31.250 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:31.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:31.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:31.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:31.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:51.272 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:51.352 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:51.352 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:51.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:51.353 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:52.367 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:52.431 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:52.447 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:52.447 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:48:52.447 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:49:40.228 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 09:53:49.856 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 09:54:20.009 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:54:20.041 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 09:54:20.041 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:54:20.056 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 09:54:37.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:54:51.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:54:51.875 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:54:58.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:54:59.500 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:55:10.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:55:10.065 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 09:55:17.074 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:55:17.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:55:17.175 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:55:17.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:55:17.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 09:56:13.092 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 10:03:40.429 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-03 10:03:43.232 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 10:03:43.265 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 10:03:43.465 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 10:03:58.737 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 10:03:58.737 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-03 10:04:00.980 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 10:04:00.996 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 10:12:52.716 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 10:12:52.794 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 10:12:52.810 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 10:12:52.810 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 10:13:12.234 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 10:13:12.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:12.840 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:12.855 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:12.855 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:28.045 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 10:13:28.045 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:28.061 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:32.476 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:32.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:32.523 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:32.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:32.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:32.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:32.539 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:32.947 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:33.291 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:33.498 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:33.513 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:33.513 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:33.529 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:34.245 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 10:13:44.835 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 10:13:44.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:44.853 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 10:13:49.168 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:49.184 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:49.184 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:51.581 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:51.599 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 10:13:52.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:52.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:52.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:52.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:13:52.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 10:16:19.307 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 10:18:07.226 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 10:18:07.257 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 10:18:07.257 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 10:18:07.257 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 10:18:26.514 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:18:40.860 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:18:40.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:18:45.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:18:46.689 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 10:18:55.844 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:18:55.860 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 10:19:02.531 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:02.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:02.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:02.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:02.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:28.930 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:28.993 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:28.993 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:29.009 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:29.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:30.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:30.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:30.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:30.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:19:30.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:21:13.553 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 10:30:13.756 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-03 10:35:13.806 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-03 10:40:13.847 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-03 10:40:13.847 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-03 10:57:45.891 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:57:46.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:57:46.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:57:46.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 10:57:46.940 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 11:18:43.019 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 11:18:43.097 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 11:18:43.097 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 11:18:43.097 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 11:19:09.761 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:19:09.761 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:10.365 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:10.381 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:10.388 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:26.477 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:19:26.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:26.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:30.858 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:30.873 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:30.905 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:30.917 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:30.918 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:30.918 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:30.918 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:31.548 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:31.857 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:32.080 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:32.102 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:32.102 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:32.118 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:32.866 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 11:19:43.742 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:19:43.742 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:43.759 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 11:19:48.501 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:48.501 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:48.518 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:50.816 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:50.816 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 11:19:51.366 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:51.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:51.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:51.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:19:51.538 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:20:29.816 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:20:29.809 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:21:07.311 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:21:07.311 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:21:11.292 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:21:11.293 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:21:12.733 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 11:22:56.795 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:22:56.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:22:57.103 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:22:57.104 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:23:05.299 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:23:05.574 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:23:07.773 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:23:07.824 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:23:07.823 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:23:07.840 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:23:07.939 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 11:23:08.077 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 11:27:09.204 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:27:09.204 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:10.004 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:27:10.004 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:18.535 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:27:19.058 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:27:42.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:42.354 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:42.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:42.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:42.358 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:49.585 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:49.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:27:56.436 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:27:56.895 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:28:05.509 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:28:05.807 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:28:47.455 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:28:47.456 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:28:47.755 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:28:47.756 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:28:59.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:28:59.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:29:00.703 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:01.028 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:17.624 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:19.923 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:29:19.941 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:20.079 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 11:29:41.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:29:41.838 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:29:46.633 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:47.044 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:56.710 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:29:56.710 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:56.866 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:29:56.924 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 11:30:49.776 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:30:50.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:30:51.741 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:30:52.098 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:31:00.639 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:31:00.640 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:31:02.855 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:31:02.856 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:31:49.734 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 11:32:19.514 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:32:19.529 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:32:19.782 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:32:19.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 11:32:21.376 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:32:21.634 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:32:23.562 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:32:23.563 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:32:23.812 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:32:23.812 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:32:24.034 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:32:24.325 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:37:21.561 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:37:21.760 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:37:23.795 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:37:23.797 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:37:23.825 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:37:23.825 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:37:24.295 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:37:24.323 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:42:21.703 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:42:21.775 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:42:23.820 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:42:23.826 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:42:23.850 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:42:23.852 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:42:24.316 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:42:24.338 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:47:21.714 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:47:21.723 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-03 11:47:21.821 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:47:21.835 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-03 11:47:24.331 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/search_docs: timed out
2025-01-03 11:47:24.332 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-01-03 11:56:36.921 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 11:56:36.939 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 11:56:49.144 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 12:20:39.695 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 12:20:39.724 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 12:20:42.598 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 12:20:42.607 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 12:22:02.005 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 12:22:02.072 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 12:22:02.073 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 12:22:02.074 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:20:33.246 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:20:33.247 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:20:33.273 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:33.275 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:33.278 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:41.007 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:20:41.008 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:20:41.023 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:20:43.284 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:43.287 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:43.326 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:43.327 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:43.327 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:43.327 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:44.247 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:44.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:20:44.700 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:24:21.596 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:24:21.644 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:24:33.544 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:24:33.545 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:24:33.566 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:33.568 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:33.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:43.922 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:24:43.922 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:24:43.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:24:46.205 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.208 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.233 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.233 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.234 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.234 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.238 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.491 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:24:46.795 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.799 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.802 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:46.806 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:24:47.138 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:24:55.736 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:24:55.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:24:55.750 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:25:00.996 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:25:00.996 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:25:00.996 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:25:04.927 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:25:04.927 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:25:05.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:25:05.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:25:05.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:25:05.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:25:05.356 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:25:12.025 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:25:12.025 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:25:34.934 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:25:37.978 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:25:39.026 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:25:39.051 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用户权限分配.docx
2025-01-03 13:26:01.278 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:26:01.279 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:26:01.281 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-03 13:26:03.327 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:26:03.333 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:28:47.317 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:28:47.389 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:28:47.391 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:28:47.391 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:29:07.163 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:29:07.163 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:07.210 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:07.210 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:07.225 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:24.000 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:29:24.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:24.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:27.911 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:27.926 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:27.958 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:27.958 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:27.958 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:27.958 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:27.973 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:28.482 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:28.753 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:28.942 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:28.958 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:28.958 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:28.974 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:29.710 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:29:40.533 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-03 13:29:40.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:40.548 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:29:45.287 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:45.287 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:45.305 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:47.869 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:47.869 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-03 13:29:48.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:48.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:48.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:48.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:29:48.482 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:11.862 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:11.909 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:11.909 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:11.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:11.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:12.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:12.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:12.887 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:12.888 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:12.889 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-03 13:30:22.413 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:30:22.412 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-03 13:31:53.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:31:53.483 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:31:56.873 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:31:57.116 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:04.263 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:17.317 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-03 13:32:29.519 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:38.731 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-03 13:32:39.015 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.530 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.745 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.745 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:39.746 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:43.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:43.940 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:43.940 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:43.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:43.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:32:51.529 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-01-03 13:33:06.016 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:33:06.052 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:33:06.053 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:33:06.054 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:33:17.709 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:24.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:24.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:26.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:26.658 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:33:30.767 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:30.775 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:33:35.661 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.746 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.746 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.746 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.768 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:35.772 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:40.965 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:41.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:41.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:41.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:41.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:51.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:51.634 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:33:55.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:34:42.015 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:14.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:14.889 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:16.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:16.690 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:17.472 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:18.238 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:23.847 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:35:59.992 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:00.242 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:03.462 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:07.395 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:14.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:15.202 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2025-01-03 13:36:15.379 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:15.918 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:17.231 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:36:23.693 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:37:35.303 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:37:35.466 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:37:38.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:37:54.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:05.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:05.724 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2025-01-03 13:38:09.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:31.287 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:32.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:35.060 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:36.181 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:36.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:37.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:37.822 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:39.121 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:40.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:38:43.791 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:40:02.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:40:02.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:40:02.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:40:02.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:40:02.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:40:09.089 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 13:40:50.253 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 13:40:57.125 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 13:43:54.323 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:43:54.395 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:43:54.396 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:43:54.397 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:44:08.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:26.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:26.091 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:31.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:32.360 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:44:42.390 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:42.410 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:44:49.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:49.671 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:49.671 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:49.682 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:44:49.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:45:22.249 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 13:45:30.538 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-03 13:47:22.577 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 13:52:21.860 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-01-03 13:52:23.439 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 13:56:28.986 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:56:29.048 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 13:56:29.048 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:56:29.048 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 13:56:44.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:56:57.715 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:56:57.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:02.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:03.686 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:57:12.623 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:12.638 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 13:57:20.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:20.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:20.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:20.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:20.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 13:57:46.246 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 14:01:54.574 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 14:01:54.621 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 14:01:54.621 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 14:01:54.621 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 14:02:07.758 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:21.587 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:21.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:26.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:27.257 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 14:02:36.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:36.223 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 14:02:43.380 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:43.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:43.760 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:43.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:02:43.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:03:06.000 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 14:04:35.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:36.091 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:36.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:36.160 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:36.160 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:36.160 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:45.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:45.324 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:45.325 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:45.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:04:45.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:06:17.324 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 14:07:37.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:37.707 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.229 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.324 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.325 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.537 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:40.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:42.360 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:42.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:07:46.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:08:18.508 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:09:23.142 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 14:09:28.255 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:09:28.352 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:09:28.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:09:28.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:09:28.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:10:39.247 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用友项目描述.docx
2025-01-03 14:11:21.057 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用户权限分配.docx
2025-01-03 14:11:58.564 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\Langchain-Chatchat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\用户权限分配.docx
2025-01-03 14:20:44.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:44.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:44.958 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:44.966 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:44.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:44.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:48.095 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:48.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:48.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:48.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:48.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:53.849 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:54.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:54.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:54.058 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:20:54.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:10.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:11.203 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:11.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:11.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:11.211 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:11.918 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:11.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:12.027 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:12.072 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:12.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:21:18.934 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2025-01-03 14:21:18.948 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2025-01-03 14:22:43.309 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:43.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:43.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:43.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:43.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:44.140 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:44.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:44.225 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:44.266 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:44.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:22:52.363 | ERROR    | chatchat.server.api_server.openai_routes:get_model_client:62 - failed when request to ('qwen2-instruct', 'xinference')
2025-01-03 14:22:52.372 | ERROR    | chatchat.server.utils:wrap_done:46 - AttributeError: Caught exception: 'NoneType' object has no attribute 'dict'
2025-01-03 14:23:10.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:23:10.847 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:23:10.848 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:23:10.849 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 14:23:10.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:24.957 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:25.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:25.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:25.163 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:25.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:25.482 | WARNING  | chatchat.server.chat.chat:chat_iterator:255 - streaming progress has been interrupted by user.
2025-01-03 17:13:36.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:39.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:39.941 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:48.745 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:48.795 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:48.830 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:48.862 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:48.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:49.145 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:49.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:49.228 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:49.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:49.323 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:13:49.399 | WARNING  | chatchat.server.chat.chat:chat_iterator:255 - streaming progress has been interrupted by user.
2025-01-03 17:13:49.427 | WARNING  | chatchat.server.chat.chat:chat_iterator:255 - streaming progress has been interrupted by user.
2025-01-03 17:13:56.155 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:14:57.805 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:15:36.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:15:37.215 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:15:51.289 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2025-01-03 17:16:31.215 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2025-01-03 17:16:31.277 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 17:16:31.326 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-03 17:16:31.327 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 17:16:31.327 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-03 17:16:44.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:16:58.284 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:16:58.319 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:03.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:04.203 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 17:17:13.401 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:13.420 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-03 17:17:20.695 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:20.809 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:20.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:20.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:20.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:21.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:21.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:21.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:21.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:21.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:23.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:23.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:23.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:23.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:23.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:24.308 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:24.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:37.140 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:42.732 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:17:43.346 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2025-01-03 17:17:44.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:18:53.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:18:53.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:18:53.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:18:53.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:18:53.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:19:07.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:19:08.046 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:19:18.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:19:26.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-03 17:21:10.137 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-04 09:44:33.721 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-04 09:44:33.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-04 09:44:34.867 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-04 09:44:34.883 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-04 09:44:46.061 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-04 09:44:46.627 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-04 09:44:46.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-04 09:44:46.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-04 09:44:46.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-04 09:44:46.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-06 09:02:56.514 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-06 09:02:56.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-06 09:02:57.615 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-06 09:02:57.619 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-06 09:02:57.619 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-06 09:02:57.620 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:21:51.140 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:21:51.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:21:51.533 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-07 16:21:51.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:21:51.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:21:51.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:21:51.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:28:45.995 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-07 16:28:46.054 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-07 16:28:46.057 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-07 16:28:46.057 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-07 16:29:02.036 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:29:02.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:02.062 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:02.065 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:02.067 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:11.755 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:29:11.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:11.767 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:13.727 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:13.729 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:13.754 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:13.755 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:13.755 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:13.756 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:13.760 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:14.066 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:14.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:14.482 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:14.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:14.488 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:14.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:14.812 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-07 16:29:21.094 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:29:21.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:21.102 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-07 16:29:25.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:25.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:25.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:28.649 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:28.649 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:29:28.951 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:29.020 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:29.021 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:29.021 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:29:29.022 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:39.147 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:30:39.147 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:39.341 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:30:39.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.706 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.947 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.996 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:30:40.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:26.700 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-07 16:37:26.743 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-07 16:37:26.744 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-07 16:37:26.745 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-07 16:37:36.177 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:37:36.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:36.212 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:36.215 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:36.218 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:43.840 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:37:43.840 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:43.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:45.807 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:45.809 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:45.836 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:45.836 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:45.836 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:45.837 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:45.841 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:46.081 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:46.243 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:46.317 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:46.321 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:46.323 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:46.326 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:46.679 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-07 16:37:53.180 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:37:53.181 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:53.192 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-07 16:37:56.697 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:56.697 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:56.697 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:58.163 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:58.180 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-07 16:37:58.380 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:58.453 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:58.454 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:58.455 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:37:58.456 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:08.733 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:08.788 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:08.788 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:08.789 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:08.789 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:16.836 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:17.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:19.369 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-07 16:38:19.370 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-07 16:38:24.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.465 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.466 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.644 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.690 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.692 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.693 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:38:24.693 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:49.153 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:40:49.159 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-07 16:40:49.524 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:40:49.525 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-07 16:40:50.040 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:40:50.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:50.305 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:40:50.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:52.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:52.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:52.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:52.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:52.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:53.119 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:53.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:53.179 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:53.180 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:53.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:53.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:40:54.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 16:45:52.457 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 16:45:52.458 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-07 17:22:43.926 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 17:22:43.926 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:44.672 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 17:22:44.672 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.122 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.223 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.224 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.227 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.378 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.436 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:47.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:49.050 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:49.356 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:51.061 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-07 17:22:51.380 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-07 17:22:52.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 17:22:52.524 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 19:33:33.432 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 19:33:33.450 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-07 19:33:33.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 19:33:33.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 19:33:35.510 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-07 19:33:35.527 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 19:33:35.527 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 19:33:35.527 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-07 19:33:35.527 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:22:15.016 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 13:22:15.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:22:15.953 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:22:15.970 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:22:15.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:22:15.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:29:06.327 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 13:29:06.331 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:29:06.585 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 13:29:06.585 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:39:33.160 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-08 13:39:33.191 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-08 13:39:33.191 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-08 13:39:33.191 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-08 13:39:45.100 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 13:39:45.100 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:39:45.116 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:45.116 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:45.131 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:52.534 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 13:39:52.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:39:52.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:39:54.420 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.436 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.452 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.452 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.452 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.452 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.452 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.752 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:54.995 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:39:55.080 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:55.080 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:55.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:55.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:39:55.380 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-08 13:40:01.033 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 13:40:01.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:01.033 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-08 13:40:05.231 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:40:05.231 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:40:05.231 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:40:08.712 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:40:08.712 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 13:40:08.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:09.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:09.058 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:09.058 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:09.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:17.672 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:17.715 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:17.716 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:17.717 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:17.717 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:18.338 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:18.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:18.377 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:18.378 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:18.378 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:27.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:27.650 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:31.686 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 13:40:31.688 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 13:40:34.216 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:34.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:36.992 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.401 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.402 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.403 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:37.403 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:44.515 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:44.908 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:47.709 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 13:40:48.046 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 13:40:52.673 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 13:40:52.920 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:07:57.977 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 14:07:57.979 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 14:07:59.400 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 14:07:59.401 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:07:59.571 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 14:07:59.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:07:59.811 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:07:59.813 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:07:59.813 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:07:59.813 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:08:00.346 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:08:00.652 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:26:42.149 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 14:26:42.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:26:42.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:26:42.435 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:26:42.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 14:26:42.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 15:14:53.817 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 15:14:53.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 15:14:59.725 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 15:14:59.726 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 15:15:01.495 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 15:15:01.512 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 15:15:01.513 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-08 15:15:01.513 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-01-08 16:35:22.526 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 16:35:22.531 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 16:35:37.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:22:43.734 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-08 17:22:43.797 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-08 17:22:43.797 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-08 17:22:43.797 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-08 17:22:55.260 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 17:22:55.261 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:22:55.433 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:22:55.436 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:22:55.438 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:03.769 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 17:23:03.769 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:03.769 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:05.478 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.478 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.495 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.495 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.495 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.495 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.510 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.763 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:05.953 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:06.037 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:06.037 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:06.052 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:06.052 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:06.342 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-08 17:23:11.979 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 17:23:11.979 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:11.979 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-08 17:23:17.737 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:17.737 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:17.737 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:19.780 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:19.780 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-08 17:23:20.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.041 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.145 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.146 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:20.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:21.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:21.273 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:21.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:21.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:21.276 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:49.340 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 17:23:49.341 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 17:23:50.834 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:51.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:53.382 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 17:23:53.750 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-08 17:23:54.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:23:54.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.269 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.329 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.787 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.788 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:01.789 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:03.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 17:24:04.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 20:03:08.270 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 20:03:08.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 20:03:09.207 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-08 20:03:09.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 20:03:10.220 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-08 20:03:10.220 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 20:03:10.220 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 20:03:10.220 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-08 20:03:10.220 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:10:18.557 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-09 09:10:18.578 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:10:20.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:10:20.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:10:20.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:10:20.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:41:57.211 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-09 09:41:57.425 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:41:57.975 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:41:57.975 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:41:57.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 09:41:57.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 13:56:13.174 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-09 13:56:13.205 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 13:56:13.753 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 13:56:13.754 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 13:56:13.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-09 13:56:13.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:17.564 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 09:32:17.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:18.347 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 09:32:18.389 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:18.391 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:18.392 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:18.392 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:24.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:24.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:24.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:24.962 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 09:32:24.962 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:30:55.282 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 13:30:55.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:30:55.792 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:30:55.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:30:55.795 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:30:55.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:42:49.333 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 13:42:49.432 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 13:42:49.435 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 13:42:49.435 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 13:47:02.187 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 13:47:02.188 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:02.215 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:02.218 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:02.222 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:13.503 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 13:47:13.504 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:13.519 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:16.332 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:16.334 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:16.356 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:16.356 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:16.357 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:16.358 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:16.362 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:16.766 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:17.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:17.155 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:17.159 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:17.161 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:17.165 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:17.820 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 13:47:26.527 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 13:47:26.527 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:26.535 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 13:47:33.189 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:33.194 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:33.197 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:35.515 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:35.523 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 13:47:36.046 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:36.138 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:36.140 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:36.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 13:47:36.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 15:27:56.153 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 15:27:56.195 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 15:27:56.197 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 15:27:56.197 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 15:28:05.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:05.997 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:05.999 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:06.002 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:11.751 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:11.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:13.610 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:13.613 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:13.636 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:13.636 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:13.637 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:13.638 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:13.641 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:13.914 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:14.065 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:14.142 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:14.146 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:14.149 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:14.151 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:14.433 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 15:28:18.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:18.179 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 15:28:21.979 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:21.982 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:21.985 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:23.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:23.499 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 15:28:23.791 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:23.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:23.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:23.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:28:23.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:29:28.647 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:29:28.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:29:32.345 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:29:33.247 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:29:51.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:10.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:25.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:34.390 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:34.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:34.885 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:34.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:34.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:34.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:40.073 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:40.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:42.418 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:44.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:30:50.164 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:31:01.123 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:31:12.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:31:19.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:31:29.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:31:35.242 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:31:44.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:31:45.080 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:222 - error in knowledge chat: 1 validation error for BingSearchAPIWrapper
__root__
  Did not find bing_subscription_key, please add an environment variable `BING_SUBSCRIPTION_KEY` which contains it, or pass `bing_subscription_key` as a named parameter. (type=value_error)
2025-01-10 15:32:07.270 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:07.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:08.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:08.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:08.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:08.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:09.470 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:09.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:09.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:09.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:09.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:14.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:15.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:15.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:15.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:15.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.413 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.415 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.939 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.967 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:21.995 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:32:24.462 | ERROR    | chatchat.server.utils:wrap_done:46 - KeyError: Caught exception: 'districts'
2025-01-10 15:33:18.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.335 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.335 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.337 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.415 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.445 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.473 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.504 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:19.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:21.597 | ERROR    | chatchat.server.utils:wrap_done:46 - KeyError: Caught exception: 'districts'
2025-01-10 15:33:28.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:28.532 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:28.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:28.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:28.537 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:41.843 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.196 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.623 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.653 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:33:42.681 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:02.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:02.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:02.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:02.564 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:02.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:03.776 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:04.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:04.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:04.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:04.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:12.828 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:13.217 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:13.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:13.221 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:13.221 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:26.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:26.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:26.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:26.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:26.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:29.379 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:29.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:29.745 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:29.747 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:29.747 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:45.165 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:45.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:45.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:45.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:45.568 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:55.699 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.063 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.064 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.065 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.065 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.480 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.508 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.562 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:56.591 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:34:58.733 | ERROR    | chatchat.server.utils:wrap_done:46 - KeyError: Caught exception: 'districts'
2025-01-10 15:35:11.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:11.914 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:11.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:11.916 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:11.916 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:11.995 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:12.024 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:12.053 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:12.081 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:35:12.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:48.570 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:48.942 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:48.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:48.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:48.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:49.308 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:49.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:49.366 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:49.394 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:36:49.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:37:17.411 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:37:17.740 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:37:20.822 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:37:29.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 15:41:28.820 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 15:41:28.839 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 15:41:28.842 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 15:41:28.843 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 15:41:46.811 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 15:41:47.076 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 15:41:47.835 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 15:41:48.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 15:43:37.154 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 15:43:37.210 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 15:43:37.211 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 15:43:37.211 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:12:15.566 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:12:15.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:12:15.602 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:15.605 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:15.609 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:30.466 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:12:30.467 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:12:30.516 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:12:37.536 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:37.541 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:37.643 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:37.643 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:37.644 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:37.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:37.650 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:38.664 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:39.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:12:39.194 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:39.199 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:39.201 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:39.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:39.613 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:12:47.673 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:12:47.674 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:12:47.699 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:12:59.524 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:59.526 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:12:59.529 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:13:03.407 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:13:03.411 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:13:03.725 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:13:03.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:13:03.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:13:03.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:13:03.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:21:33.639 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:21:33.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:21:33.930 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:21:33.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:26:06.037 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:26:06.038 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:26:06.038 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:26:06.039 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:30:09.040 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:30:09.084 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:30:09.086 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:30:09.086 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:30:20.174 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:30:20.175 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:20.210 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:20.213 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:20.215 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:27.760 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:30:27.761 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:27.782 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:29.435 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.437 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.459 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.460 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.460 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.461 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.465 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.751 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:29.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:30.032 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:30.036 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:30.038 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:30.040 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:30.321 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:30:35.821 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:30:35.822 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:35.830 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:30:40.012 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:40.015 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:40.017 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:41.570 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:41.575 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:30:41.775 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:41.849 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:41.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:41.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:41.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:46.519 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:30:46.520 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:30:52.635 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:52.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:54.746 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:30:55.087 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:30:56.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:56.510 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:30:58.129 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:30:58.459 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:31:00.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:31:00.892 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:31:17.509 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:31:17.961 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:31:18.508 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:31:18.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:31:20.478 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:31:20.879 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:31:21.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:31:21.578 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:31:23.270 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:31:23.618 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:31:24.156 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:31:24.346 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:24.371 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:33:24.393 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:33:24.394 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:33:24.395 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:33:33.452 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:33:33.453 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:33.509 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:33.513 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:33.516 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:41.193 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:33:41.194 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:41.226 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:42.915 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:42.917 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:42.938 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:42.939 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:42.939 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:42.940 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:42.944 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:43.182 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:43.367 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:43.441 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:43.445 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:43.447 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:43.449 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:43.752 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:33:49.558 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:33:49.560 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:49.582 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:33:52.796 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:52.801 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:52.803 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:54.243 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:54.246 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:33:54.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.553 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.554 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.554 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.557 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:54.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:55.599 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:55.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:55.646 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:55.647 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:55.648 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:33:59.660 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:00.025 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:06.865 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:34:06.866 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:34:09.560 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:09.741 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:12.730 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:34:13.084 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:34:13.847 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:14.050 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:16.853 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:34:17.189 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:34:17.725 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:17.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:36.237 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:34:36.240 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:34:36.241 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:34:36.241 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:34:43.986 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:34:43.988 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:44.019 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:44.022 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:44.024 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:51.494 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:34:51.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:51.506 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:53.093 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.112 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.113 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.113 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.114 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.118 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.327 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:53.563 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.567 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.569 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:34:53.842 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:34:59.280 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 16:34:59.281 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:34:59.295 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:35:03.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:35:03.338 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:35:03.342 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:35:04.839 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:35:04.843 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 16:35:05.081 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.082 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.082 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.169 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.180 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.182 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.184 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.186 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.189 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.289 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.292 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.405 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:05.408 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:06.620 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:06.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:06.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:06.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:06.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:09.326 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:09.674 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:12.713 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:35:12.714 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:35:19.969 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 16:35:32.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:32.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:39.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:39.706 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:39.707 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:39.707 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:39.708 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:40.022 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:40.067 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:40.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:40.070 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:40.070 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:43.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:43.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:54.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:54.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:54.522 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:54.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:35:54.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 16:38:06.666 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:07.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:23.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:23.427 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:30.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:42.982 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-10 16:38:48.263 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.372 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.611 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.723 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.723 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.724 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:48.725 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:52.302 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:52.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:52.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:52.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:38:52.400 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:00.031 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-01-10 16:39:21.628 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:39:21.670 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 16:39:21.671 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:39:21.674 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 16:39:29.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:36.558 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:36.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:39.074 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:39.490 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:39:45.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:45.574 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 16:39:51.582 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:51.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:51.644 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:51.645 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:51.646 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:53.368 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:53.741 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:39:59.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:40:12.992 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:40:23.464 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:00.621 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:00.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:00.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:00.801 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:00.801 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:00.975 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:01.147 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:01.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:01.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:01.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:06.968 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:07.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:07.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:07.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:07.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:25.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:25.761 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:25.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:25.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:25.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:37.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:37.592 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:37.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:37.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:37.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:38.230 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:38.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:38.285 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:38.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:38.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:41:40.301 | ERROR    | chatchat.server.utils:wrap_done:46 - ImportError: Caught exception: Could not import duckduckgo-search python package. Please install it with `pip install -U duckduckgo-search`.
2025-01-10 16:43:40.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.021 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.022 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.723 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.824 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.857 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:43:41.888 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.176 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.655 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.685 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.724 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.772 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:44:16.804 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.384 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.929 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.957 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:18.985 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:19.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:19.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:44.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:44.949 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:44.952 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:44.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:44.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:45.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:45.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:45.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:45.159 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:45:45.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.563 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.765 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.766 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.768 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.881 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:03.987 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:04.018 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:07.160 | ERROR    | chatchat.server.utils:wrap_done:46 - ValidationError: Caught exception: 1 validation error for search_internetSchema
query
  str type expected (type=type_error.str)
2025-01-10 16:46:56.657 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:56.885 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:56.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:56.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:56.887 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:57.380 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:57.407 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:57.434 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:57.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:46:57.488 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:47:26.654 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:47:27.045 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:48:33.298 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\UME_Chat\libs\chatchat-server\chatchat\data\knowledge_base\ume\content\航旅纵横实习生活指南1.docx
2025-01-10 16:48:52.240 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:48:52.436 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:48:56.176 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:49:14.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:49:20.131 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2025-01-10 16:49:20.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:49:33.503 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 16:50:03.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-10 17:04:43.648 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 17:04:43.733 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 17:04:43.736 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 17:04:43.737 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 17:04:54.671 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:04:54.672 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:04:54.698 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:04:54.701 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:04:54.703 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:02.685 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:05:02.687 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:02.726 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:04.982 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:04.984 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.013 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.013 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.013 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.014 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.018 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.286 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:05.549 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.553 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.555 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.557 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:05.850 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 17:05:12.025 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:05:12.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:12.053 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 17:05:15.565 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:15.568 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:15.570 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:17.305 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:17.309 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:05:17.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.661 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.661 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.664 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.665 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.666 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.666 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:17.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:19.036 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:19.111 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:19.112 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:19.113 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:05:19.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:08:32.075 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:08:54.373 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:10:00.398 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:10:00.455 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:10:00.456 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:10:06.430 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:10:18.721 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:10:19.715 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:10:19.717 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:20:45.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:20:45.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:20:45.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:20:45.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:20:45.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:21:31.608 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:22:03.834 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:29:11.608 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 17:29:11.726 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 17:29:11.729 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 17:29:11.730 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 17:29:21.666 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:31.127 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:31.139 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:33.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:34.389 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 17:29:38.710 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:38.720 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 17:29:44.239 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:44.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:44.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:44.301 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:29:44.302 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:30:25.895 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:30:50.627 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:36:47.237 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 17:36:47.289 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-10 17:36:47.290 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 17:36:47.291 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-10 17:36:57.728 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:36:57.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:36:57.765 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:36:57.768 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:36:57.773 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:06.141 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:37:06.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:06.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:08.243 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.246 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.273 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.273 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.273 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.275 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.278 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.623 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.818 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:08.894 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.898 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.900 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:08.903 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:09.221 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 17:37:15.299 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 17:37:15.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:15.320 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-10 17:37:18.957 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:18.960 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:18.962 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:20.675 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:20.678 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:20.899 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:20.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:20.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:20.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:20.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:20.986 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:21.366 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:21.367 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:21.369 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:21.370 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:22.270 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:22.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:22.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:22.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:22.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 17:37:32.870 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 17:37:32.871 | ERROR    | chatchat.server.utils:get_Embeddings:360 - failed to create Embeddings for model: bge-m3.
2025-01-10 17:37:35.663 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:37:35.664 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-10 17:39:46.936 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:39:46.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:39:46.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:39:46.977 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:39:46.978 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-10 17:40:09.783 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 17:40:25.941 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-10 20:18:34.260 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-10 20:18:34.261 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-10 20:18:34.854 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-12 11:42:38.087 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-12 11:42:38.110 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-12 11:42:39.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-12 11:42:39.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-12 11:42:39.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-12 11:42:39.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-13 10:05:08.344 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-13 10:05:08.344 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-13 10:05:08.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-13 10:05:08.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-13 10:05:08.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-13 10:11:52.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:11:59.998 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:00.040 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:02.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:03.402 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-13 10:12:06.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:06.919 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-13 10:12:14.231 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:14.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:14.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:14.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:12:14.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:14:28.384 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\UME_Chat\libs\chatchat-server\chatchat\data\knowledge_base\ChunkKnowledge\content\航旅钱包客服问题.docx
2025-01-13 10:14:28.385 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\UME_Chat\libs\chatchat-server\chatchat\data\knowledge_base\ChunkKnowledge\content\航旅纵横实习生活指南1.docx
2025-01-13 10:14:45.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:14:46.062 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:14:50.428 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:14:54.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:15:15.287 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:15:22.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:15:27.612 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-01-13 10:34:02.849 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-13 10:34:02.947 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-13 10:34:02.951 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-13 10:34:02.952 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-14 11:08:48.554 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-14 11:08:48.555 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:08:48.579 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:08:48.582 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:08:48.585 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:08:56.398 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-14 11:08:56.398 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:08:56.409 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:09:00.088 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.092 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.198 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.198 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.199 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.199 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.203 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.668 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:00.990 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:09:01.118 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:01.132 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:01.134 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:01.137 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:01.463 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 11:09:07.735 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-14 11:09:07.735 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:09:07.743 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 11:09:15.434 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:15.437 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:15.439 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:20.563 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:20.567 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 11:09:20.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:09:20.901 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:09:20.902 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:09:20.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 11:09:20.903 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 13:16:22.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:16:22.667 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:16:22.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:16:22.668 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:16:22.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:16:31.005 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-01-14 13:16:44.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:16:44.749 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:16:55.202 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:22:17.671 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:22:32.741 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-14 13:22:44.373 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:22:51.508 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:23:00.028 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-14 13:23:12.497 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:23:12.544 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:23:12.545 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:23:12.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:23:12.548 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:37:52.685 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:37:53.347 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:37:53.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:37:53.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:37:53.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:38:28.099 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:38:28.149 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:38:28.150 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-14 13:38:28.151 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-14 13:44:46.689 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:44:52.620 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:44:52.632 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:47:35.535 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:47:35.546 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:47:41.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:47:47.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:47:47.392 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:48:44.212 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:48:44.212 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:48:49.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:48:54.998 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:48:55.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:51:19.672 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:51:19.672 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:51:24.690 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:51:30.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:51:30.509 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 13:52:51.181 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 13:52:51.181 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 14:05:29.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:35.555 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:35.571 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:38.675 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:39.050 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 14:05:42.907 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:42.923 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 14:05:54.169 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:54.246 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:54.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:54.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:05:54.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:07:42.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:07:42.147 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:08:21.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:08:22.070 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:08:36.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:08:45.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:08:45.945 | WARNING  | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:219 - streaming progress has been interrupted by user.
2025-01-14 14:08:57.388 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:09:18.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:09:32.336 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:09:36.884 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:09:56.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:09:56.446 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:10:04.903 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-14 14:10:28.151 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-14 14:10:44.026 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-14 14:11:00.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:11:00.999 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:11:04.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:11:11.989 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:11:27.832 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:11:28.983 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:11:36.350 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:12:40.775 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\UME_Chat\libs\chatchat-server\chatchat\data\knowledge_base\mytest\content\问题.docx
2025-01-14 14:16:13.947 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 14:16:14.004 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 14:16:14.004 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-14 14:16:14.004 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-14 14:16:23.085 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-14 14:16:23.085 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:23.108 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:23.108 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:23.108 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:30.731 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-14 14:16:30.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:30.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:32.446 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.446 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.462 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.462 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.478 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.478 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.478 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.647 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:32.885 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.900 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.900 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:32.900 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:33.197 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 14:16:38.895 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-14 14:16:38.895 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:38.895 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 14:16:42.493 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:42.504 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:42.507 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:43.943 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:43.943 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-14 14:16:44.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.245 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.261 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:44.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:45.389 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:45.440 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:45.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:45.443 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:16:45.444 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-14 14:19:36.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:19:36.552 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:19:39.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:19:40.479 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:19:46.761 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:19:54.203 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:22:45.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:23:06.430 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-14 14:23:41.343 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\deepLearningCode\UME_Chat\libs\chatchat-server\chatchat\data\knowledge_base\MyTest04\content\问题.docx
2025-01-14 14:24:01.600 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:01.968 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:02.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:02.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:02.126 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:02.127 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:06.020 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:06.198 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:06.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:06.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:06.199 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:13.804 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-01-14 14:24:34.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:34.976 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:40.695 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:24:49.419 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-01-14 14:25:02.448 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 14:25:02.915 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-14 14:25:02.949 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-14 14:25:02.969 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-14 14:25:12.713 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:18.484 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:18.495 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:20.677 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:21.057 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 14:25:25.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:25.149 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-14 14:25:30.585 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.585 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.585 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.671 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.671 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.694 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.696 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.698 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.698 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.821 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.932 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.933 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:30.935 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:32.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:32.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:32.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:32.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:32.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:36.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:36.518 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:41.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:25:59.689 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-14 14:26:09.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2-instruct instead
2025-01-15 12:09:09.538 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-15 12:09:09.558 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 12:09:10.594 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-15 12:09:10.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 12:09:10.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 12:09:10.617 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 12:09:10.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 14:04:42.417 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-15 14:04:42.430 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 14:04:53.008 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-15 14:04:53.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 14:04:53.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 14:04:53.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 14:04:53.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 19:12:27.306 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-15 19:12:27.322 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 19:12:37.817 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-15 19:12:37.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 19:12:37.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 19:12:37.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-15 19:12:37.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 08:51:18.045 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-16 08:51:18.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 08:51:28.747 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-16 08:51:28.788 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 08:51:28.789 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 08:51:28.789 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 08:51:28.791 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 11:25:23.535 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-16 11:25:23.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 11:25:34.066 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-16 13:44:36.079 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-16 13:44:36.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:44:46.444 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-16 13:44:46.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:44:46.496 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:44:46.498 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:44:46.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:45:30.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:45:30.200 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:45:40.318 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-16 13:45:40.403 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-16 13:45:43.118 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-16 13:45:43.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:45:43.205 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-16 13:45:53.199 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-16 13:46:03.225 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-17 13:41:32.314 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-17 13:41:32.392 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-17 13:41:32.392 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-17 13:41:32.392 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-17 13:41:48.429 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-17 13:41:48.429 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:41:48.445 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:41:48.460 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:41:48.460 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:43:49.486 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-17 13:43:49.486 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:43:49.501 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:43:49.501 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:43:49.501 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:43:59.244 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-17 13:43:59.244 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:43:59.260 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:04.608 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:04.608 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:04.670 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:04.670 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:04.670 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:04.670 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:04.686 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:05.029 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:05.330 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:05.470 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:05.486 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:05.486 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:05.486 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:06.009 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-17 13:44:21.026 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-17 13:44:21.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:21.026 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-01-17 13:44:28.725 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:28.727 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:28.729 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:32.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:32.832 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-01-17 13:44:33.200 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:33.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:33.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:33.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:33.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:39.320 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:39.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:47.685 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:48.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.084 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.163 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.164 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.165 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.165 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.392 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-17 13:44:49.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-20 09:33:15.466 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-20 09:33:15.470 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-20 09:33:16.321 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-01-20 09:33:16.345 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-20 09:33:16.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-20 09:33:16.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-20 09:33:16.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 13:24:58.375 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-21 13:24:58.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 13:24:59.080 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 13:24:59.082 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 13:24:59.082 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 13:24:59.082 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 14:08:48.343 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-01-21 14:08:48.374 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 14:08:48.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 14:08:48.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 14:08:48.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 14:08:48.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 14:09:01.206 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 14:09:01.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-01-21 15:41:26.505 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-21 15:41:26.568 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-01-21 15:41:26.568 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-01-21 15:41:26.568 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-02-07 09:47:45.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:47:45.626 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:47:45.628 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:47:45.630 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:53:52.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:53:52.551 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:53:52.555 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:53:52.557 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:53:59.045 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:53:59.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:01.459 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.462 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.495 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.498 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.755 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:01.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:02.095 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:02.100 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:02.103 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:02.106 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:02.395 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-02-07 09:54:07.288 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:07.316 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-02-07 09:54:15.316 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:15.320 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:15.323 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:19.007 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:19.011 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-07 09:54:19.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:19.380 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:19.380 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:19.381 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:19.381 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:24.501 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:24.890 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:28.644 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:28.692 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:28.692 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:28.693 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:28.694 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:28.930 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:28.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:29.000 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:29.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:54:29.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:22.834 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:23.152 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:23.190 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:23.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:23.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:23.191 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:28.192 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 09:55:28.500 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:04.713 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-07 12:24:04.733 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-07 12:24:04.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:04.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:05.606 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-07 12:24:05.606 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:05.744 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-07 12:24:05.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:06.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:06.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:06.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:06.729 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:08.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 12:24:08.988 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 16:04:32.332 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 16:04:32.841 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 16:04:32.843 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 16:04:32.844 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-07 16:04:32.845 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 09:16:48.044 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-08 09:16:48.060 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 09:16:49.106 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-02-08 09:16:49.152 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 09:16:49.153 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 09:16:49.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 09:16:49.155 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:06:54.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:06:55.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:06:55.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:06:55.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:06:55.174 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:04.857 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-02-08 10:07:20.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:20.128 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:20.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:20.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:20.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:26.803 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:26.857 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:26.858 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:26.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:26.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-08 10:07:34.321 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-02-08 13:28:26.725 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:28:27.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:28:27.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:28:27.118 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:28:27.120 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:28:34.849 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-02-08 13:32:31.569 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:31.634 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:31.635 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:31.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:31.636 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:39.886 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:40.266 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:45.100 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:32:59.019 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.513 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.812 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.911 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.912 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:06.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:11.914 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:12.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:12.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:12.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:12.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:21.271 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:21.395 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:21.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:21.397 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-08 13:33:21.399 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using deepseek-r1-distill-qwen instead
2025-02-13 15:41:39.145 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-13 15:41:39.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-13 15:41:39.207 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:41:39.214 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:41:39.222 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:42:37.791 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-13 15:42:37.793 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-13 15:42:37.852 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:42:37.860 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:42:37.867 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:43:16.317 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-13 15:43:16.318 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-13 15:43:16.366 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:43:16.369 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-13 15:43:16.371 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:11:25.036 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-17 10:11:25.037 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:11:25.089 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:11:25.093 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:11:25.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:32.615 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-17 10:16:32.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:32.667 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:32.669 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:32.671 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:40.171 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-17 10:16:40.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:40.193 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:42.814 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:42.816 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:42.847 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:42.847 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:42.847 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:42.848 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:42.851 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:43.098 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:43.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:43.647 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:43.651 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:43.654 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:43.656 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:43.982 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-02-17 10:16:50.559 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-17 10:16:50.560 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:50.567 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-02-17 10:16:55.272 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:55.274 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:55.276 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:58.901 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:58.905 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-02-17 10:16:59.207 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:59.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:59.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:59.299 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:16:59.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:03.711 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-17 10:19:03.718 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:03.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:03.850 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:03.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:03.851 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:04.048 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:04.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:04.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:04.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:04.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.163 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.214 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.214 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.215 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.413 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:10.414 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:15.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.027 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.029 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.256 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.295 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.298 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:16.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:17.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:17.095 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:17.096 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:17.096 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:17.097 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.044 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.096 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.096 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.097 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.316 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.316 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:18.317 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:32.363 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:32.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:38.878 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:39.081 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:48.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.488 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.489 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.489 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.489 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.741 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:54.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:58.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:19:58.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:20:18.430 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-17 10:20:18.432 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:20:18.662 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-02-17 10:20:18.662 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:20:18.728 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:20:18.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:20:18.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-02-17 10:20:18.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 17:04:57.800 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 17:04:57.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 17:04:57.841 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 17:04:57.843 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 17:04:57.845 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:24.246 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 18:15:24.247 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:15:24.269 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:24.271 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:24.273 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:30.258 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 18:15:30.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:15:30.268 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:15:38.386 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:38.388 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:38.574 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:38.574 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:38.575 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:38.575 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:38.578 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:39.313 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:39.848 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:15:39.943 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:39.949 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:39.951 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:15:39.953 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:02.832 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-06-28 18:18:02.886 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-06-28 18:18:10.336 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 18:18:10.337 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:10.356 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:10.358 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:10.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:16.053 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 18:18:16.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:16.064 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:17.752 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:17.754 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:17.774 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:17.774 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:17.775 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:17.775 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:17.779 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:17.934 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:18.094 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:18.163 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:18.166 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:18.168 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:18.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:18.434 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-06-28 18:18:22.258 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 18:18:22.258 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:22.264 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-06-28 18:18:31.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:31.487 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:31.489 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:41.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:41.101 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-06-28 18:18:41.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:41.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:41.385 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:41.386 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:18:41.387 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:19:37.071 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 18:19:37.072 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 18:19:37.383 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:08.857 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-28 20:16:09.239 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:10.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:10.213 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:10.214 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:10.214 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.239 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.311 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.312 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.313 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.474 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.517 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.518 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.519 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:19.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.574 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.757 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.796 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:31.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:33.997 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.032 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.033 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.034 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.197 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:34.236 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.108 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.283 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.360 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.361 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-28 20:16:35.361 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-30 19:33:48.251 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-06-30 19:33:48.252 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-30 19:33:51.926 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-06-30 19:33:51.953 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-30 19:33:51.954 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-30 19:33:51.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-06-30 19:33:51.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:11.253 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-07-01 21:52:11.262 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:11.696 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:11.696 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:11.697 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:11.697 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:21.321 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:21.374 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:21.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:21.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-01 21:52:21.376 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:48:55.537 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-07-05 09:48:55.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:48:56.130 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-07-05 09:48:56.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:48:56.157 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:48:56.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:48:56.158 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:49:02.706 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:49:02.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:49:02.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:49:02.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-05 09:49:02.868 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-07-06 09:49:06.846 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-07-06 09:49:06.913 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-07-06 09:49:06.915 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-07-06 09:49:06.916 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-09-07 14:33:28.144 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:33:28.145 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:33:28.176 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:33:28.179 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:33:28.179 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:34:14.035 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:34:14.035 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:34:14.050 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:34:14.050 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:34:14.055 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:28.277 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:38:28.277 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:28.295 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:28.297 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:28.298 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:33.297 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:38:33.297 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:33.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:35.220 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.222 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.254 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.254 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.254 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.254 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.257 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.458 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.720 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:35.802 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.805 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.807 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:35.809 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:36.059 | INFO     | __mp_main__:run_api_server:56 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 14:38:39.274 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:38:39.275 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:39.284 | INFO     | __mp_main__:run_webui:81 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 14:38:44.588 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:44.591 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:44.593 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:47.605 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:47.608 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:38:47.897 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:47.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:47.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:47.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:47.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:56.453 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:56.806 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.333 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.367 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.367 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.368 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.369 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.561 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.593 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.594 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:38:59.595 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:31.915 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:47:31.915 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:31.970 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:31.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:31.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:31.975 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:54.572 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:54.624 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:54.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:54.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:47:54.625 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:05.187 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:05.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.558 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.588 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.588 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.823 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.858 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:08.859 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:10.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:11.040 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.247 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.485 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.516 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.517 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.518 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:12.518 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:13.659 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:13.922 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:57.714 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:48:57.714 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:57.899 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.534 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.817 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.865 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:48:58.867 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:00.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:00.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:00.838 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:00.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:00.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:02.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:02.100 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:02.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:02.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:02.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:03.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:03.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:03.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:03.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:03.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:04.700 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:04.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:04.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:04.736 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:04.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:07.819 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:08.061 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:10.853 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:49:11.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:54:34.963 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 14:54:34.992 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 14:54:34.993 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 14:54:34.993 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 14:54:50.141 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:54:50.141 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:54:50.160 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:50.162 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:50.163 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:54.887 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:54:54.887 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:54:54.897 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:54:56.298 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.300 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.316 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.317 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.317 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.317 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.320 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.440 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.579 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:54:56.643 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.647 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.649 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.651 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:54:56.902 | INFO     | __mp_main__:run_api_server:56 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 14:55:00.007 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:55:00.008 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:00.015 | INFO     | __mp_main__:run_webui:81 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 14:55:03.251 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:55:03.254 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:55:03.256 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:55:04.600 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:55:04.606 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 14:55:04.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:04.820 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:04.918 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:04.922 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:04.924 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:04.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:05.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:05.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:05.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:55:05.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:49.012 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 14:56:49.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:49.334 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:50.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:50.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:50.762 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:50.763 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:50.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:50.969 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:51.004 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:51.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:51.006 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 14:56:51.006 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:31.503 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:10:31.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:31.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:31.731 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:31.732 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:31.733 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:37.499 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:37.552 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:37.553 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:37.553 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:10:37.553 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:25.955 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:16:25.956 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:26.010 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:26.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:26.011 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:26.012 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:29.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:29.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:29.076 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:29.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:29.077 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:35.426 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 15:16:35.474 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 15:16:35.478 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 15:16:35.479 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 15:16:49.533 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:16:49.533 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:49.551 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:49.553 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:49.554 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:54.254 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:16:54.254 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:54.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:55.920 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:55.922 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:55.949 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:55.950 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:55.950 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:55.950 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:55.953 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:56.091 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:56.210 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:56.276 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:56.279 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:56.281 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:56.283 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:16:56.532 | INFO     | __mp_main__:run_api_server:56 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 15:16:59.659 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:16:59.660 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:16:59.666 | INFO     | __mp_main__:run_webui:81 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 15:17:03.028 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:17:03.032 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:17:03.035 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:17:04.209 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:17:04.212 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:17:04.440 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:04.504 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:04.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:04.505 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:04.506 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:05.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:05.589 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:05.590 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:05.591 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:05.591 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:09.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:09.338 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:09.338 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:09.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:09.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:16.806 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:17:45.737 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:46.042 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:17:51.177 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:11.724 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:18:11.725 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:11.836 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:11.836 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:11.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:11.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:11.988 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:12.098 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:12.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:12.100 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:12.100 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:14.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:14.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:14.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:14.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:14.360 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:19.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:19.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:19.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:19.546 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:19.548 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:18:19.585 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:18:26.981 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:19:24.486 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:19:24.487 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:19:24.487 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:23:23.378 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:23:23.378 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.440 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.592 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:23.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:35.707 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:35.741 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:35.742 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:35.743 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:35.744 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:23:35.787 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:23:43.455 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:24:35.028 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 15:24:35.041 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 15:24:35.041 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 15:24:35.042 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 15:24:45.027 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:24:45.027 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:45.044 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:45.046 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:45.048 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:49.925 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:24:49.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:49.934 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:51.358 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.360 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.374 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.374 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.375 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.375 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.378 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.490 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.647 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:51.715 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.718 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.720 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.722 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:51.981 | INFO     | __mp_main__:run_api_server:56 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 15:24:55.122 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:24:55.122 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:55.128 | INFO     | __mp_main__:run_webui:81 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 15:24:58.154 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:58.156 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:58.158 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:59.265 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:59.268 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:24:59.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:59.522 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:59.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:59.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:24:59.524 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:03.164 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:03.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:03.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:03.208 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:03.209 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:07.200 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:07.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:07.234 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:07.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:07.235 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:14.841 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:25:44.600 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:44.638 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:44.639 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:44.640 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:44.641 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:25:52.069 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:26:07.600 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:26:07.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:26:07.898 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:26:10.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:26:14.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:27:07.088 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:07.089 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:18.331 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:27:18.331 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:18.331 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:35.021 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:35.023 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:37.226 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in 'test/vector_store/quentinz/bge-large-zh-v1.5' from disk.
2025-09-07 15:27:37.279 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:27:39.324 | ERROR    | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:140 - Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002481ABFDAE0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-09-07 15:27:39.325 | ERROR    | chatchat.server.knowledge_base.kb_api:create_kb:44 - RuntimeError: 创建知识库出错： 向量库 test 加载失败。
2025-09-07 15:27:39.923 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:39.923 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:39.924 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:39.924 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:27:53.644 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\UME_Assistant\libs\chatchat-server\chatchat\data\knowledge_base\test\content\新建 Microsoft Word 文档.docx
2025-09-07 15:28:50.419 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:28:52.455 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002481EB79330>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-09-07 15:29:13.484 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:29:13.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:29:13.485 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:29:22.745 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:29:22.945 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:29:31.396 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:29:33.590 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002481EBD9CC0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-09-07 15:29:33.591 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:225 - error in knowledge chat: failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002481EBD9CC0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-09-07 15:30:08.872 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:08.979 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:08.979 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:08.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:08.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:09.236 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:09.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:09.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:09.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:09.349 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:12.422 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:12.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:12.525 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:12.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:12.526 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:30:12.569 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:30:19.935 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:32:53.613 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-09-07 15:32:53.714 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\UME_Assistant\libs\chatchat-server\chatchat\data\knowledge_base\test\content\新建 Microsoft Word 文档.docx
2025-09-07 15:33:36.653 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:33:38.696 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002481EBAD720>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-09-07 15:34:33.965 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:34:33.965 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:34:34.030 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:34:34.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:34:34.031 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:34:34.032 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:37:53.662 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-09-07 15:37:53.710 | INFO     | chatchat.server.knowledge_base.utils:file2docs:336 - RapidOCRDocLoader used for C:\UME_Assistant\libs\chatchat-server\chatchat\data\knowledge_base\test\content\新建 Microsoft Word 文档.docx
2025-09-07 15:38:35.663 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:38:37.702 | ERROR    | chatchat.server.utils:check_embed_model:374 - failed to access embed model 'quentinz/bge-large-zh-v1.5': Error raised by inference endpoint: HTTPConnectionPool(host='127.0.0.1', port=11434): Max retries exceeded with url: /api/embeddings (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000002481EBD32B0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))
2025-09-07 15:41:58.440 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:41:58.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:41:58.519 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:41:58.520 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:41:58.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:41:58.522 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:41:58.648 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:42:06.246 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:42:20.300 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:42:20.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:42:20.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:42:20.339 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:42:20.340 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:42:27.923 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 15:42:53.675 | ERROR    | chatchat.webui_pages.utils:post:87 - ReadTimeout: error when post /knowledge_base/upload_docs: timed out
2025-09-07 15:42:53.678 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-09-07 15:48:37.303 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 15:48:37.304 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:48:37.616 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 15:48:44.089 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 15:48:44.090 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:18:27.013 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 16:18:27.024 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:18:27.607 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:18:27.608 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:18:27.609 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:18:27.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:18:35.744 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 16:18:35.805 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 16:18:35.806 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 16:18:35.808 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 16:59:15.929 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 16:59:15.930 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:15.950 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:15.952 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:15.953 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:21.249 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 16:59:21.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:21.262 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:23.084 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.086 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.112 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.112 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.113 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.113 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.118 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.268 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.417 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:23.489 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.494 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.496 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:23.798 | INFO     | __mp_main__:run_api_server:56 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 16:59:27.669 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 16:59:27.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:27.676 | INFO     | __mp_main__:run_webui:81 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 16:59:31.543 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:31.545 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:31.547 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:33.843 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:33.852 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 16:59:34.178 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:34.248 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:34.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:34.251 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:34.254 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:59.321 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:59.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:59.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:59.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 16:59:59.359 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:00:07.002 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 17:00:17.348 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:00:17.349 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:00:51.946 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:00:51.947 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:00:51.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:00:51.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:00:51.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:00:51.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:00:58.990 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:00:58.991 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:04:29.739 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:04:29.741 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:04:29.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:04:29.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:04:29.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:04:29.800 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:04:34.815 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:04:34.815 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:04:55.194 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 17:04:55.237 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 17:04:55.238 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 17:04:55.238 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 17:05:13.634 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:05:13.635 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:13.676 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:13.683 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:13.690 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:24.355 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:05:24.355 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:24.379 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:27.829 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:27.836 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:27.872 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:27.873 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:27.873 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:27.874 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:27.883 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:28.299 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:28.532 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:28.702 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:28.711 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:28.718 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:28.725 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:29.462 | INFO     | __mp_main__:run_api_server:56 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:05:36.992 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:05:36.992 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:37.011 | INFO     | __mp_main__:run_webui:81 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:05:40.578 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:40.585 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:40.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:42.399 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:42.408 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:43.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:43.085 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:43.161 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:43.163 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:43.165 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:43.167 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:51.176 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:51.179 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:05:58.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.114 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.115 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.282 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.360 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.362 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.363 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:05:58.364 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:01.751 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:01.825 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:01.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:01.826 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:01.827 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:07.218 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:06:07.219 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:06:09.466 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 17:06:13.005 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.071 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.072 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.074 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.240 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.306 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:06:13.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:11:41.355 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 17:11:41.452 | WARNING  | __main__:start_main_server:317 - Sending SIGKILL to %s
2025-09-07 17:11:41.454 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 17:11:41.455 | INFO     | __main__:start_main_server:328 - Process status: %s
2025-09-07 17:11:53.760 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:11:53.761 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:11:53.786 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:11:53.788 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:11:53.789 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:14:46.808 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:14:46.809 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:14:46.833 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:14:46.835 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:14:46.837 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:19:54.784 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:19:54.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:19:54.808 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:19:54.810 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:19:54.812 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:21:58.695 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:21:58.695 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:21:58.712 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:21:58.714 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:21:58.716 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:05.132 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:22:05.132 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:05.142 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:07.747 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:07.749 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:07.773 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:07.774 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:07.774 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:07.774 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:07.777 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:07.919 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:08.058 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:08.123 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:08.126 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:08.128 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:08.130 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:08.383 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:22:11.922 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:22:11.922 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:11.929 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:22:15.855 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:15.857 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:15.859 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:17.266 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:17.270 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:22:17.474 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:17.535 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:17.536 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:17.537 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:17.537 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:24.069 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:24.101 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:24.102 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:24.103 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:24.103 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:33.240 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:33.278 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:33.279 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:33.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:22:33.280 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:03.238 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-09-07 17:23:03.255 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-09-07 17:23:03.256 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-09-07 17:23:03.256 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-09-07 17:23:18.737 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:23:18.738 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:18.755 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:18.757 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:18.758 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:23.897 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:23:23.897 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:23.907 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:25.883 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:25.885 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:25.900 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:25.901 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:25.901 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:25.901 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:25.904 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:26.023 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:26.134 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:26.200 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:26.203 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:26.205 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:26.207 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:26.453 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:23:29.739 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:23:29.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:29.747 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:23:33.755 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:33.763 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:33.772 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:36.125 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:36.130 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:23:36.366 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:36.436 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:36.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:36.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:36.438 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:40.746 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:40.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:40.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:40.779 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:40.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:23:48.423 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 17:26:25.813 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:26:25.814 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:25.955 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:25.956 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:25.956 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:25.957 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:28.185 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:28.220 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:28.221 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:28.222 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:28.222 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:28.270 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:26:35.772 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 17:26:36.161 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:26:36.162 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:26:57.430 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:57.475 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:57.476 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:57.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:26:57.477 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:03.474 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:27:03.475 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:27:06.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.601 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.603 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.730 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.774 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.775 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.776 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:06.777 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:16.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:16.835 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:16.836 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:16.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:16.837 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:36.269 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:27:36.269 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:36.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:36.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:36.327 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:36.328 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:36.363 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:27:43.950 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 17:27:54.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:55.013 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:55.014 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:55.015 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:55.016 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:59.673 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:27:59.930 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:02.515 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 17:28:06.113 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:28:06.114 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:28:22.899 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:28:22.900 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:28:27.925 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:27.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:27.972 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:27.973 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:27.975 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:28.117 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:28.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:28.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:28.171 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:28:28.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:29:59.935 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:29:59.935 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:29:59.936 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:46:51.784 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:46:51.787 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:47:58.923 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:47:58.926 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:48:00.287 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:48:00.987 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:48:54.683 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:48:54.684 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:48:55.445 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:49:02.629 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:49:03.974 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:50:51.862 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-09-07 17:50:51.866 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-09-07 17:53:00.283 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:53:00.288 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:53:01.713 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:02.530 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:03.195 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:46.465 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:53:46.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:53:46.642 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:53:51.183 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:52.458 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:53.715 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:54.684 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:55.573 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:56.329 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:53:57.008 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:54:03.171 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:54:04.310 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:54:05.299 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:54:06.092 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:54:06.291 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:54:06.906 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:54:15.853 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:57:02.628 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:57:02.630 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:04.048 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-07 17:57:45.921 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:57:46.776 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:57:48.323 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:57:54.469 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:57:54.962 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:57:56.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:56.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:56.637 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:57.464 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:57.465 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:57.959 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:57.961 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:59.622 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:57:59.623 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:00.348 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:00.352 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:22.717 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:58:22.718 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:22.778 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:22.779 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:22.779 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:22.780 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 17:58:22.946 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 17:58:30.928 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 17:58:34.167 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 17:58:34.169 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-07 21:13:49.081 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 21:13:49.092 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:49.739 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:49.829 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:50.805 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-09-07 21:13:50.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:50.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:50.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:50.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:51.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:51.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:51.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:13:51.755 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:15:51.990 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 21:15:51.990 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:15:52.129 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:15:52.131 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:15:52.132 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:15:52.132 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:48.974 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.057 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.061 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.259 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.341 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.342 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.344 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:49.344 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:54.726 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 21:16:54.727 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:54.783 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:54.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:54.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:54.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-07 21:16:54.949 | WARNING  | chatchat.server.utils:detect_xf_models:106 - auto_detect_model needs xinference-client installed. Please try "pip install xinference-client". 
2025-09-07 21:17:02.654 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-07 21:26:49.511 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-09-07 21:26:49.581 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-09-07 21:26:49.584 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-09-07 21:26:49.584 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-09-08 09:23:07.781 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 09:23:07.784 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:07.816 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:07.821 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:07.821 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:14.705 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 09:23:14.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:14.715 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:16.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:16.571 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:16.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:16.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:16.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:16.592 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:16.596 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:16.812 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:17.004 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:17.091 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:17.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:17.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:17.096 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:17.351 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-08 09:23:22.732 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 09:23:22.732 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:22.736 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-08 09:23:26.903 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:26.908 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:26.910 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:29.692 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:29.697 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:23:29.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:30.027 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:30.027 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:30.032 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:23:30.032 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:34:07.524 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:07.905 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:07.906 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:07.907 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:07.907 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:18.450 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:18.522 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:18.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:18.523 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:18.524 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:34:47.750 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-09-08 09:35:19.854 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\UME_Assistant\libs\chatchat-server\chatchat\data\knowledge_base\20250908\content\航旅钱包客服问题.docx
2025-09-08 09:35:25.939 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('20250908', 'bge-m3') 保存到磁盘
2025-09-08 09:35:34.984 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('20250908', 'bge-m3') 保存到磁盘
2025-09-08 09:36:07.759 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:07.873 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:07.913 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:07.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:07.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:07.917 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:09.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:09.931 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:36:19.565 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:37:01.401 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:37:37.419 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:38:20.357 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:39:01.516 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('20250908', 'bge-m3') 保存到磁盘
2025-09-08 09:39:29.191 | INFO     | chatchat.server.knowledge_base.utils:file2docs:335 - RapidOCRDocLoader used for C:\UME_Assistant\libs\chatchat-server\chatchat\data\knowledge_base\20250908\content\航旅钱包客服问题.docx
2025-09-08 09:39:31.181 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:save:40 - 已将向量库 ('20250908', 'bge-m3') 保存到磁盘
2025-09-08 09:40:11.567 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:11.697 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:16.240 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:16.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:16.463 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:16.464 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:16.465 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:18.390 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:18.727 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:40:34.257 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:22.614 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:41.066 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:41.122 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:41.122 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:41.127 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:41.127 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:42.388 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:42.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:41:47.263 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:14.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:21.079 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:36.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:36.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:36.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:36.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:36.539 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:37.764 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:38.047 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:45.305 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:49.797 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:49.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:49.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:49.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:49.839 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:55.452 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:55.491 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:55.492 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:55.493 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:42:55.494 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:43:01.286 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:43:01.553 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:43:26.437 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-08 09:48:42.169 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 09:48:42.169 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:48:42.215 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:42.215 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:42.219 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:48.971 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 09:48:48.971 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:48:48.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:48:50.610 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:50.616 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:50.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:50.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:50.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:50.646 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:50.651 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:50.905 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:51.109 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:48:51.200 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:51.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:51.204 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:51.209 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:48:51.459 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-08 09:48:57.175 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 09:48:57.175 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:48:57.190 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-08 09:49:01.190 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:49:01.195 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:49:01.195 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:49:04.503 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:49:04.503 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-09-08 09:49:04.766 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:04.995 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:05.001 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:05.002 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:05.003 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:08.870 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:08.901 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:08.902 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:08.902 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:08.902 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:09.410 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:09.441 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:09.442 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:09.442 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:09.443 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:12.769 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 09:49:13.088 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 16:27:14.028 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 16:27:14.039 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 16:27:16.140 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-09-08 16:27:16.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 16:27:16.170 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 16:27:16.172 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 16:27:16.173 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:24:52.293 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 17:24:52.314 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:24:52.899 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:24:52.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:24:52.901 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:24:52.901 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:24:55.727 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 17:25:03.718 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-09-08 17:25:44.149 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:25:44.217 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:25:44.217 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:25:44.217 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 17:25:44.217 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 20:56:29.227 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-09-08 20:56:29.302 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 20:56:30.638 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-09-08 20:56:30.683 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 20:56:30.685 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 20:56:30.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-08 20:56:30.686 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-09-25 08:44:46.401 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:54:32.487 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:54:41.547 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:54:41.558 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:54:45.584 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:54:46.093 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-25 08:54:50.869 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:54:50.876 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-09-25 08:55:21.656 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:26.980 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:26.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:26.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:26.983 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:41.055 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:41.122 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:41.124 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:41.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:41.125 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:53.843 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:53.918 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:53.919 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:53.921 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-09-25 08:55:53.922 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-10-28 19:34:14.338 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-28 19:34:14.343 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:14.366 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:14.368 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:14.371 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:24.098 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-28 19:34:24.099 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:24.138 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:27.364 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:27.368 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:27.422 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:27.423 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:27.423 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:27.424 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:27.428 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:27.724 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:28.150 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:28.285 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:28.293 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:28.298 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:28.301 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:28.611 | INFO     | __mp_main__:run_api_server:54 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-10-28 19:34:35.232 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-28 19:34:35.233 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:35.249 | INFO     | __mp_main__:run_webui:79 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-10-28 19:34:40.985 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:40.988 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:40.991 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:48.058 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:48.080 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-28 19:34:48.618 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:48.716 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:48.716 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:48.717 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 19:34:48.718 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-28 20:06:11.256 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-10-28 20:06:11.383 | WARNING  | __main__:start_main_server:315 - Sending SIGKILL to %s
2025-10-28 20:06:11.385 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-10-28 20:06:11.388 | INFO     | __main__:start_main_server:326 - Process status: %s
2025-10-29 09:32:41.067 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 09:32:41.068 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:32:41.088 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:41.090 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:41.092 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:49.361 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 09:32:49.362 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:32:49.393 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:32:51.497 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:51.499 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:51.524 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:51.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:51.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:51.525 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:51.529 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:51.779 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:52.010 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:32:52.121 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:52.126 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:52.129 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:52.131 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:32:52.503 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-10-29 09:32:58.926 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 09:32:58.926 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:32:58.936 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-10-29 09:33:04.551 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:33:04.558 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:33:04.566 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:33:09.118 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:33:09.126 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 09:33:09.469 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:09.575 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:09.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:09.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:09.576 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:45.669 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:45.703 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:45.704 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:45.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:45.705 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 09:33:53.572 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-10-29 16:47:35.566 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 16:47:35.566 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:47:35.580 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:47:35.580 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:47:35.580 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:14.424 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 16:50:14.426 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:14.454 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:14.457 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:14.459 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:21.598 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 16:50:21.599 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:21.610 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:23.771 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:23.773 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:23.799 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:23.800 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:23.800 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:23.800 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:23.803 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:24.024 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:24.249 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:24.343 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:24.347 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:24.348 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:24.350 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:24.619 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-10-29 16:50:30.152 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 16:50:30.152 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:30.159 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-10-29 16:50:34.332 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:34.334 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:34.335 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:38.541 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:38.546 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 16:50:38.866 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:38.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:38.944 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:38.945 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 16:50:38.946 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:25:24.556 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 21:25:24.573 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:25:26.095 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-10-29 21:25:26.143 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:25:26.148 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:25:26.151 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:25:26.154 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:26:22.521 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:26:22.751 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:26:22.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:26:22.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:26:22.756 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:30:44.834 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 21:30:44.834 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:30:45.137 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-29 21:30:45.137 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:30:49.245 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 21:30:49.245 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 21:30:49.245 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 21:30:49.252 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-10-29 21:30:58.734 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-29 21:30:59.081 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-30 09:06:26.461 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-30 09:06:26.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-30 09:06:26.781 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-30 09:06:26.781 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-30 09:06:27.015 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-10-30 09:06:27.026 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-30 09:06:27.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-30 09:06:27.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-30 09:06:27.028 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-10-30 17:54:45.321 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-10-30 17:54:45.331 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:25:49.490 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 09:25:49.490 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:25:49.520 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:25:49.535 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:25:49.535 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:28:58.854 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 09:28:58.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:28:58.898 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:28:58.906 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:28:58.913 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:11.983 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 09:29:11.986 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:29:12.039 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:29:16.465 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:16.472 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:16.506 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:16.506 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:16.507 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:16.507 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:16.517 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:16.966 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:17.307 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:29:17.568 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:17.577 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:17.584 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:17.591 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:29:30.373 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-11-06 09:29:50.236 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 09:29:50.236 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:30:07.351 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-11-06 09:30:30.492 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:30:30.499 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:30:30.509 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:30:35.170 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:30:35.179 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 09:30:35.785 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:30:35.898 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:30:35.899 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:30:35.899 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 09:30:35.900 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 10:06:58.841 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 10:06:58.871 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 10:06:59.263 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 10:06:59.265 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 10:06:59.266 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 10:06:59.267 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 10:07:01.641 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 10:07:09.440 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-11-06 10:37:32.165 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:37:32.458 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:37:32.459 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:37:32.460 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:37:32.461 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:37:40.287 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-11-06 10:38:05.807 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:05.981 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:05.982 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:05.983 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:05.984 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:15.268 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:15.350 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:15.351 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:15.352 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:15.352 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:23.831 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:23.938 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:23.939 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:23.943 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:38:23.947 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-11-06 10:49:32.202 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-11-06 10:49:32.261 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-11-06 10:49:32.264 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-11-06 10:49:32.265 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-11-06 14:42:40.077 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 14:42:40.078 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:42:40.122 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:40.129 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:40.136 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:52.787 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 14:42:52.790 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:42:52.840 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:42:57.066 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.073 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.110 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.111 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.111 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.113 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.123 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.602 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:57.939 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:42:58.152 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:58.167 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:58.181 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:42:58.189 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:43:05.630 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-11-06 14:43:14.796 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 14:43:14.799 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:19.743 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-11-06 14:43:23.996 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:43:24.003 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:43:24.009 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:43:27.727 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:43:27.737 | WARNING  | chatchat.server.utils:get_default_embedding:218 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-06 14:43:28.371 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:28.448 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:28.448 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:28.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:28.449 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:44.798 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:44.852 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:44.854 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:44.855 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:44.856 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:43:52.455 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-11-06 14:44:13.966 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:44:14.053 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:44:14.054 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:44:14.056 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:44:14.059 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 14:44:16.292 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 14:44:23.699 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-11-06 17:05:06.754 | WARNING  | chatchat.server.utils:detect_xf_models:109 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-06 17:05:06.777 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 17:05:08.143 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-11-06 17:05:08.205 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 17:05:08.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 17:05:08.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-06 17:05:08.218 | WARNING  | chatchat.server.utils:get_default_llm:208 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:31.362 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-30 13:23:31.363 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:31.413 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:31.415 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:31.417 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:38.342 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-30 13:23:38.344 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:38.384 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:40.319 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.320 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.343 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.343 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.343 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.343 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.346 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.556 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.791 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:40.883 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.887 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.889 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:40.890 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:41.147 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-11-30 13:23:46.583 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-11-30 13:23:46.584 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:46.602 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-11-30 13:23:52.339 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:52.341 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:52.343 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:55.378 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:55.384 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-11-30 13:23:55.673 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:55.757 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:55.757 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:55.758 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-11-30 13:23:55.758 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:51:27.552 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-04 08:51:27.662 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-04 08:51:27.664 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-04 08:51:27.664 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-04 08:51:42.820 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-04 08:51:42.820 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:51:42.851 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:42.854 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:42.856 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:49.935 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-04 08:51:49.936 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:51:49.981 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:51:52.035 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.038 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.063 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.063 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.064 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.064 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.067 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.277 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.525 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:51:52.612 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.615 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.617 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.619 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:51:52.872 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-04 08:51:58.231 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-04 08:51:58.231 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:51:58.238 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-04 08:52:01.614 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:52:01.616 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:52:01.617 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:52:04.613 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:52:04.617 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-04 08:52:04.930 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:04.998 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:04.999 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:05.000 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:05.000 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:05.642 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:05.696 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:05.696 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:05.697 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:52:05.698 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 08:58:19.723 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:58:20.036 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:58:20.038 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:58:20.038 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:58:20.038 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:58:28.074 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-04 08:59:09.431 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:59:09.544 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:59:09.546 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:59:09.548 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:59:09.549 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 08:59:17.251 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-04 09:04:20.559 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:21.131 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:21.135 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:21.136 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:21.136 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:34.362 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:34.416 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:34.417 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:34.418 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 09:04:34.419 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:39:17.988 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-04 20:39:18.007 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:19.973 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:20.749 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-12-04 20:39:20.772 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:20.772 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:20.772 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:20.772 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:24.808 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:25.026 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:25.028 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:25.028 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:25.028 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:27.178 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-04 20:39:34.882 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-04 20:39:37.925 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:37.979 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:37.980 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:37.981 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:37.982 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:45.373 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-04 20:39:59.044 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:59.087 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:59.088 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:59.089 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:39:59.089 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:40:06.562 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-04 20:40:14.225 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:40:14.270 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:40:14.270 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:40:14.272 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:40:14.272 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-04 20:40:21.704 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-04 20:44:43.817 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:43.992 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:43.994 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:43.994 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:43.996 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:51.580 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:51.653 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:51.654 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:51.654 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:51.669 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:51.799 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-04 20:44:55.504 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:55.553 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:55.553 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:55.555 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:44:55.555 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:45:51.027 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:45:51.082 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:45:51.083 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:45:51.085 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:45:51.086 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:16.990 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:17.044 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:17.044 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:17.045 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:17.046 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:31.064 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:31.555 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:35.069 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:38.712 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:46:41.392 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-04 20:47:20.429 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:20.539 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:20.736 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:20.738 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:20.738 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:20.738 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:23.278 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:23.333 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:23.336 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:23.336 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:23.336 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:32.317 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:32.350 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:32.350 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:32.351 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:47:32.351 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:05.789 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:05.831 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:05.831 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:05.832 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:05.834 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:13.488 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:13.773 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:16.833 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:16.878 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:16.883 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:16.884 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:16.886 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:33.023 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:33.136 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:36.177 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:36.212 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:36.213 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:36.213 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:36.214 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:37.475 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:37.730 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:48:46.878 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:52:41.752 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-04 20:55:35.382 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 11:07:11.672 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:07:11.686 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:07:12.749 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:07:12.749 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:07:13.257 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-12-06 11:07:13.285 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:07:13.286 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:07:13.287 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:07:13.291 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:17:52.842 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:17:52.924 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:17:53.879 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:17:53.882 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:17:53.884 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:17:53.885 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:17:56.245 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:18:03.878 | ERROR    | chatchat.server.api_server.openai_routes:generator:111 - openai request error: Connection error.
2025-12-06 11:20:44.896 | ERROR    | chatchat.server.chat.feedback:chat_feedback:20 - AttributeError: 反馈聊天记录出错： 'NoneType' object has no attribute 'id'
2025-12-06 11:21:59.996 | ERROR    | chatchat.server.chat.feedback:chat_feedback:20 - AttributeError: 反馈聊天记录出错： 'NoneType' object has no attribute 'id'
2025-12-06 11:22:02.637 | ERROR    | chatchat.server.chat.feedback:chat_feedback:20 - AttributeError: 反馈聊天记录出错： 'NoneType' object has no attribute 'id'
2025-12-06 11:22:03.580 | ERROR    | chatchat.server.chat.feedback:chat_feedback:20 - AttributeError: 反馈聊天记录出错： 'NoneType' object has no attribute 'id'
2025-12-06 11:23:06.317 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:23:06.317 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:23:06.360 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:06.367 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:06.374 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:20.107 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:23:20.109 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:23:20.138 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:23:25.598 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:25.612 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:25.655 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:25.655 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:25.655 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:25.655 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:25.666 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:26.329 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:26.753 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:23:27.036 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:27.052 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:27.064 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:27.083 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:28.192 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 11:23:39.911 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:23:39.913 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:23:39.950 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 11:23:46.205 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:46.219 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:46.228 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:51.407 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:23:51.418 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:24:15.887 | ERROR    | chatchat.server.chat.feedback:chat_feedback:20 - AttributeError: 反馈聊天记录出错： 'NoneType' object has no attribute 'id'
2025-12-06 11:24:30.071 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:30.563 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:30.567 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:30.569 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:30.571 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:32.402 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:32.562 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:32.565 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:32.569 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:24:32.574 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:25:13.310 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:25:13.312 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:25:13.528 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:25:13.530 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:26:20.362 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:30:18.554 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-06 11:30:18.608 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-06 11:30:18.612 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-06 11:30:18.614 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-06 11:30:30.736 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:30:30.736 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:30:30.806 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:30.812 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:30.812 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:43.216 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:30:43.216 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:30:43.267 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:30:46.971 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:46.978 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.013 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.013 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.014 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.014 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.024 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.459 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.731 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:30:47.899 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.914 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.914 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:47.929 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:30:48.656 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 11:30:57.691 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 11:30:57.691 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:30:57.737 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 11:31:05.123 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:31:05.138 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:31:05.145 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:31:07.072 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:31:07.089 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 11:31:07.605 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:07.662 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:07.662 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:07.662 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:07.662 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:09.854 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:09.926 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:09.926 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:09.927 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:09.928 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:11.117 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:11.195 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:11.197 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:11.198 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:11.199 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:33.117 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:33.191 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:33.193 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:33.193 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 11:31:33.194 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 13:07:45.895 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 13:12:53.283 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 13:12:53.308 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 13:16:08.168 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:19.934 | WARNING  | chatchat.server.api_server.chat_routes:chat_completions:209 - failed to add message to db: (pymysql.err.OperationalError) (2006, "MySQL server has gone away (ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None))")
[SQL: INSERT INTO message (id, conversation_id, chat_type, query, response, meta_data, feedback_score, feedback_reason, create_time) VALUES (%(id)s, %(conversation_id)s, %(chat_type)s, %(query)s, %(response)s, %(meta_data)s, %(feedback_score)s, %(feedback_reason)s, now())]
[parameters: {'id': 'a8b162d7fde54428a186a5e24201949e', 'conversation_id': '3b4e681a94004a85ad3b222283dc076d', 'chat_type': 'llm_chat', 'query': '你好', 'response': '', 'meta_data': '{}', 'feedback_score': -1, 'feedback_reason': ''}]
(Background on this error at: https://sqlalche.me/e/20/e3q8)
2025-12-06 13:16:20.512 | WARNING  | chatchat.server.api_server.openai_routes:generator:110 - streaming progress has been interrupted by user.
2025-12-06 13:16:21.120 | WARNING  | chatchat.server.api_server.openai_routes:generator:110 - streaming progress has been interrupted by user.
2025-12-06 13:16:21.121 | WARNING  | chatchat.server.api_server.openai_routes:generator:110 - streaming progress has been interrupted by user.
2025-12-06 13:16:21.169 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:21.177 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:21.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:21.181 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:21.183 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:21.183 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:21.192 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:21.199 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:32.700 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:33.143 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:44.940 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:47.613 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-06 13:16:56.462 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.592 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.592 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.595 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.596 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.766 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.907 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.908 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.909 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:16:56.909 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:00.384 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:00.494 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:00.496 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:00.497 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:00.497 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:07.284 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-06 13:17:07.325 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-06 13:17:07.327 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-06 13:17:07.327 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-06 13:17:18.372 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:29.051 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:29.086 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:33.583 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:34.504 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 13:17:43.031 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:43.058 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 13:17:49.365 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:49.461 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:49.463 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:49.465 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:49.466 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:51.421 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:17:51.682 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:00.515 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:02.352 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-06 13:18:13.894 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.039 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.039 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.040 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.041 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.201 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.327 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.332 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.332 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:14.332 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:17.494 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:17.633 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:17.634 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:17.636 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:18:17.637 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:08.508 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:08.644 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:13.191 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:13.310 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:13.311 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:13.312 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:13.314 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:14.538 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:14.810 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:24:20.049 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:32:02.418 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:32:35.447 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 13:37:35.498 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2025-12-06 13:42:35.523 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2025-12-06 13:47:35.525 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2025-12-06 13:47:35.525 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-06 13:47:37.617 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 13:47:37.617 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 13:47:37.618 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 13:47:37.618 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 13:47:37.618 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:41:20.754 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 17:41:20.756 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:41:20.829 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:41:20.836 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:41:20.840 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:41:34.246 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 17:41:34.247 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:41:34.297 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:43:26.388 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-06 17:43:26.388 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-06 17:43:38.649 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 17:43:38.649 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:43:38.716 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:38.723 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:38.728 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:51.147 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 17:43:51.151 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:43:51.201 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:43:55.166 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.171 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.208 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.208 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.210 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.210 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.216 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.659 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:55.990 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:43:56.177 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:56.187 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:56.194 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:56.200 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:43:56.930 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 17:44:06.015 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 17:44:06.016 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:06.030 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-06 17:44:10.090 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:44:10.098 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:44:10.102 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:44:14.250 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:44:14.260 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-06 17:44:14.969 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.045 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.137 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.147 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.149 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.149 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.149 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.154 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.154 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:44:15.159 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-06 17:47:00.068 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:00.318 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.093 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.189 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.190 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.191 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.193 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.360 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.435 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.435 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.439 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.441 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:04.864 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:05.059 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:47:08.948 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:51:13.956 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-06 17:51:23.405 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-06 17:51:35.954 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:53:02.864 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-06 17:53:03.294 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:53:27.321 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-06 17:53:41.344 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:54:33.828 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 17:55:54.076 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-06 17:55:58.447 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-06 17:56:02.596 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-06 18:05:20.493 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-06 18:05:20.500 | ERROR    | chatchat.server.utils:get_Embeddings:342 - failed to create Embeddings for model: bge-m3.
2025-12-06 18:05:20.511 | ERROR    | chatchat.server.utils:check_embed_model:356 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-06 18:05:20.511 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:244 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-07 10:56:55.568 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 10:56:55.570 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 10:56:55.666 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 10:56:55.666 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 10:56:55.683 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 10:59:44.927 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 10:59:44.930 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 10:59:45.004 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 10:59:45.011 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 10:59:45.018 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 10:59:58.094 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 10:59:58.097 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 10:59:58.161 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:01:23.230 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 11:01:23.231 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 11:01:35.125 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 11:01:35.128 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:01:35.197 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:35.204 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:35.212 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:47.707 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 11:01:47.707 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:01:47.733 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:01:51.052 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.059 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.091 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.091 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.092 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.093 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.107 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.543 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:51.847 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:01:52.049 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:52.061 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:52.069 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:52.076 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:01:52.791 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 11:02:01.847 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 11:02:01.850 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:02:01.899 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 11:02:06.341 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:02:06.348 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:02:06.356 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:02:12.673 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:02:12.684 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 11:08:32.619 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 11:08:32.622 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:08:38.660 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:08:39.332 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:08:39.336 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:08:39.338 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 11:08:39.340 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:31:40.294 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:31:40.296 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:31:40.375 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:40.380 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:40.384 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:53.079 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:31:53.079 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:31:53.127 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:31:57.058 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.058 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.100 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.100 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.100 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.100 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.108 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.511 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:31:57.825 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:40:33.037 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 14:40:33.039 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 14:40:48.263 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:40:48.267 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:40:48.345 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:40:48.345 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:40:48.345 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:00.603 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:41:00.604 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:00.621 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:03.886 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:03.886 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:03.921 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:03.921 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:03.921 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:03.921 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:03.921 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:04.287 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:04.519 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:04.725 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:04.736 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:04.739 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:04.739 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:05.468 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 14:41:14.548 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:41:14.548 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:14.597 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 14:41:18.262 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:18.262 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:18.278 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:22.359 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:22.359 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 14:41:22.997 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:23.113 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:23.113 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:23.113 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:41:23.113 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:42:55.353 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:42:55.354 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:42:55.405 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:42:55.407 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:42:55.408 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:42:55.408 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:07.719 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:43:26.391 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:26.628 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:29.377 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadError: error when get /knowledge_base/list_knowledge_bases: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-12-07 14:43:29.379 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadError: error when get /knowledge_base/list_knowledge_bases: [WinError 10054] 远程主机强迫关闭了一个现有的连接。
2025-12-07 14:43:31.401 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:31.403 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:33.422 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:33.425 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:33.443 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:33.534 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:34.395 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:34.533 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:35.290 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:35.402 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:36.450 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:36.585 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:37.351 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:37.468 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:38.498 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:38.631 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:39.398 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:39.499 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:40.533 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:40.536 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:40.698 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:40.698 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:41.430 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:41.431 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:41.571 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:41.574 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:48.273 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:48.411 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:50.310 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:50.460 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:52.343 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:52.507 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:54.374 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:54.376 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:54.556 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:43:54.557 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:43:54.596 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:54.597 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:54.597 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:43:54.597 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:44:04.934 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:44:04.935 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:44:05.067 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 14:44:05.069 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:44:07.149 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:09.184 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:11.230 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:11.230 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:44:21.189 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:44:21.326 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:44:23.225 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:23.391 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:24.383 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:44:24.491 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 14:44:25.290 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:25.424 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:26.440 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:26.556 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:27.323 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:27.324 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:44:27.468 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:27.469 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:44:28.473 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:28.592 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:30.488 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:30.488 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:44:30.615 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /knowledge_base/list_knowledge_bases: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:44:30.615 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:46:47.164 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:46:47.386 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:46:49.442 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:46:51.510 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:46:53.556 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:46:53.556 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:46:53.590 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:46:53.592 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:46:53.595 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:46:53.598 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:46:59.136 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:01.179 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:47:03.224 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:47:05.255 | ERROR    | chatchat.webui_pages.utils:get:64 - ConnectError: error when get /tools: [WinError 10061] 由于目标计算机积极拒绝，无法连接。
2025-12-07 14:47:05.257 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-07 14:47:05.321 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:05.321 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:05.321 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:05.321 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:21.775 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 14:47:21.775 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 14:47:21.775 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 14:47:21.775 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 14:47:37.192 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:47.793 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:47.828 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:51.918 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:47:52.841 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 14:48:00.031 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:00.070 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 14:48:09.002 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:09.086 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:09.088 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:09.091 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:09.092 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:14.279 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:14.334 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:14.336 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:14.338 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:14.339 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:19.068 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:19.396 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:48:50.147 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 14:50:00.419 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 15:01:52.034 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:02:41.031 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:07:30.449 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 15:07:30.491 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 15:07:30.493 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 15:07:30.493 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 15:07:40.578 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:07:51.374 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:07:51.404 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:07:55.647 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:07:56.551 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 15:08:03.691 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:03.747 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 15:08:15.529 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:15.607 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:15.607 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:15.607 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:15.607 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:16.609 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:16.667 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:16.669 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:16.670 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:16.672 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:19.645 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:19.866 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:22.042 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 15:08:24.169 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 15:08:44.793 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:19:28.555 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 16:19:28.558 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:19:28.622 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:28.628 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:28.639 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:42.512 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 16:19:42.512 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:19:42.561 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:19:46.675 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:46.675 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:46.721 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:46.721 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:46.723 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:46.723 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:46.725 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:47.406 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:47.774 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:19:48.008 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:48.032 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:48.043 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:48.053 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:19:48.907 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:19:58.787 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 16:19:58.787 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:19:58.818 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:20:01.865 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:20:01.867 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:20:01.867 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:20:04.248 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:20:04.265 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:20:04.782 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:20:04.890 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:20:04.892 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:20:04.893 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:20:04.894 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:21:47.758 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:21:47.864 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:21:47.934 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:21:47.936 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:21:47.936 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:21:47.936 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:21:50.721 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:21:50.963 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:22:10.636 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:22:34.736 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 16:22:58.207 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:22:58.324 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:22:58.329 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:22:58.330 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:22:58.331 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:26:17.390 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 16:26:28.601 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:26:48.836 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:26:59.617 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:26:59.685 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:04.011 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:04.963 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:27:12.136 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:12.176 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:27:17.757 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:17.942 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:17.943 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:17.944 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:17.945 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:32.255 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:32.520 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:35.748 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:37.826 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 16:27:43.693 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:43.742 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:43.751 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:43.753 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:43.753 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:44.858 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:44.917 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:44.919 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:44.919 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:44.921 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:59.033 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:59.150 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:59.150 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:59.150 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:27:59.160 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:28:00.668 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:28:00.913 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:28:03.648 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:15.911 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 16:30:15.913 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 16:30:15.913 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 16:30:15.913 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 16:30:26.379 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:37.971 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:38.028 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:42.204 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:43.157 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:30:50.371 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:50.403 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:30:56.084 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:56.247 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:56.251 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:56.251 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:30:56.251 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:06.092 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:06.153 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:06.156 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:06.156 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:06.157 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:07.280 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:07.355 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:07.358 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:07.360 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:07.361 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:09.225 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:09.518 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:11.168 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:11.246 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:11.248 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:11.251 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:11.251 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:12.589 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:12.824 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:29.656 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:31.826 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 16:31:43.983 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:44.246 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:44.247 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:44.248 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:44.249 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:47.673 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:47.924 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:31:59.938 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:32:19.590 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 16:32:19.590 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 16:32:19.590 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 16:32:19.590 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 16:37:27.966 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:38.454 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:38.491 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:42.557 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:43.473 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:37:50.851 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:50.886 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:37:56.453 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.466 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.664 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.672 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.673 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.674 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.675 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.675 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.677 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:56.682 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:58.469 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:58.534 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:58.534 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:58.534 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:37:58.534 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:38:07.358 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:38:07.570 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:38:24.309 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:38:57.381 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 16:39:07.605 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:53:55.907 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 16:53:55.956 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 16:53:55.956 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 16:53:55.958 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 16:54:07.441 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 16:54:07.441 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:07.506 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:07.506 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:07.523 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:21.766 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 16:54:21.766 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:21.848 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:25.516 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:25.530 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:25.558 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:25.558 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:25.561 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:25.561 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:25.563 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:25.956 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:26.263 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:26.429 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:26.442 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:26.446 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:26.446 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:27.229 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:54:37.125 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 16:54:37.125 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:37.190 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 16:54:40.650 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:40.659 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:40.659 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:42.802 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:42.803 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 16:54:43.420 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:43.520 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:43.520 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:43.520 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:43.535 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:44.419 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:44.486 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:44.486 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:44.486 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:54:44.486 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 16:56:16.828 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:56:16.951 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:56:20.789 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:56:22.514 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:56:50.514 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 16:57:14.510 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 16:57:23.561 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-07 16:58:01.790 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:00:57.327 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-07 17:01:08.013 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:23:13.566 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 17:23:13.602 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 17:23:13.602 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 17:23:13.602 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 17:23:26.505 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 17:23:26.506 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 17:23:26.551 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:26.556 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:26.556 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:40.349 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 17:23:40.349 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 17:23:40.382 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 17:23:44.076 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.080 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.117 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.117 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.119 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.119 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.129 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.480 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.729 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 17:23:44.896 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.909 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.913 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:44.913 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:45.696 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 17:23:55.289 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 17:23:55.291 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 17:23:55.324 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 17:23:58.389 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:58.389 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:23:58.409 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:24:00.933 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:24:00.958 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 17:24:01.697 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 17:25:06.468 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:25:06.552 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:25:06.552 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:25:06.552 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:25:06.552 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:25:07.490 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:25:07.779 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:25:16.757 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:28:40.796 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 17:28:40.799 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 17:28:40.799 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 17:28:40.799 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 17:28:51.568 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:02.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:02.202 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:06.266 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:07.180 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 17:29:14.354 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:14.362 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 17:29:20.037 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:20.137 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:20.138 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:20.140 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:20.141 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:23.646 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:23.711 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:23.712 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:23.713 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:23.714 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:24.648 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:24.704 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:24.706 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:24.708 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:24.709 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:25.876 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:25.950 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:25.952 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:25.954 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:25.956 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:28.492 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:28.701 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:30.925 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:29:52.109 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 17:30:10.359 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:06.754 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 17:32:06.764 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-07 17:32:06.766 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 17:32:06.766 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-07 17:32:16.494 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:27.197 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:27.237 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:31.313 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:32.218 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 17:32:39.445 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:39.481 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 17:32:44.513 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:44.702 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:44.704 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:44.705 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:44.705 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:56.106 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:56.163 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:56.164 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:56.164 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:56.165 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:57.231 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:57.293 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:57.294 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:57.296 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:32:57.297 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:33:02.958 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:33:03.204 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:33:20.234 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 17:34:15.643 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 17:36:17.203 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:42:26.838 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 19:42:26.838 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:42:26.871 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:26.886 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:26.894 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:39.248 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 19:42:39.248 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:42:39.281 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:42:43.271 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:43.279 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:43.311 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:43.312 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:43.312 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:43.313 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:43.325 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:43.786 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:44.088 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:42:44.268 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:44.276 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:44.283 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:44.289 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:45.046 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 19:42:55.072 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-07 19:42:55.073 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:42:55.094 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 19:42:59.708 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:59.708 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:42:59.720 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:43:02.092 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:43:02.100 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-07 19:43:02.667 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:02.763 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:02.765 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:02.767 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:02.767 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:03.709 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:03.770 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:03.771 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:03.772 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:43:03.773 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-07 19:44:45.462 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:45.592 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.413 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.464 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.465 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.467 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.467 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.627 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.705 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.706 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.707 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:47.707 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:48.069 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:44:48.362 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:45:09.380 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:47:06.081 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-07 19:47:15.232 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-07 19:48:42.253 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:18.832 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-07 19:54:35.320 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:35.502 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:35.718 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:35.718 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:35.722 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:35.722 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:37.334 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:37.559 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:40.403 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:40.478 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:40.483 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:40.484 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:40.484 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:41.974 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:42.232 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:54:45.473 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 19:55:58.698 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:01:40.593 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:01:51.137 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:01:51.170 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:01:55.380 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:01:56.317 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 20:02:03.480 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:03.530 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-07 20:02:09.297 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:09.460 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:09.460 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:09.460 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:09.460 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:15.331 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:15.426 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:15.426 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:15.426 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:15.426 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:16.442 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:16.514 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:16.517 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:16.518 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:16.519 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:18.584 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:18.774 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:02:27.363 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:15.635 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:15.698 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:15.698 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:15.698 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:15.698 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:16.803 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:17.130 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-07 20:04:26.933 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 11:23:33.729 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 11:23:33.803 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 11:23:33.806 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 11:23:33.806 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 11:23:46.016 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 11:23:46.018 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:23:46.080 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:23:46.086 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:23:46.089 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:23:58.858 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 11:23:58.858 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:23:58.921 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:02.517 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:02.526 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:02.562 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:02.562 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:02.562 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:02.562 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:02.572 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:02.964 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:03.295 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:03.469 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:03.479 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:03.485 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:03.488 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:04.225 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 11:24:13.481 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 11:24:13.481 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:13.499 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 11:24:17.205 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:17.215 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:17.221 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:19.229 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:19.236 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:24:19.779 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:19.867 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:19.869 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:19.871 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:19.873 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:21.254 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:21.340 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:21.341 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:21.350 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:21.351 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:22.304 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:22.364 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:22.367 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:22.368 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:22.369 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:51.873 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:51.929 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:51.931 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:51.933 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:24:51.933 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:25:33.564 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 11:25:33.567 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:25:33.569 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:25:33.572 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:25:33.574 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 11:25:34.419 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 11:25:34.429 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:25:34.561 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 11:25:34.562 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:26:41.783 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 11:26:41.783 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:26:41.858 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:26:41.859 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:26:41.862 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:26:41.863 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:26:43.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 11:26:43.441 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 13:23:25.891 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 13:23:25.909 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 13:23:26.933 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 13:23:26.935 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 13:23:27.769 | ERROR    | chatchat.webui_pages.utils:to_json:233 - JSONDecodeError: API未能返回正确的JSON。Expecting value: line 1 column 1 (char 0)
2025-12-11 13:23:27.787 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 13:23:27.787 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 13:23:27.787 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 13:23:27.787 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:06:55.530 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 14:06:55.530 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:06:55.591 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:06:55.596 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:06:55.596 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:08.324 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 14:07:08.324 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:08.355 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:12.237 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:12.237 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:12.292 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:12.292 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:12.298 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:12.298 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:12.303 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:12.736 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:13.019 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:13.210 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:13.219 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:13.219 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:13.234 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:13.936 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 14:07:24.880 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 14:07:24.880 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:24.896 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 14:07:28.527 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:28.527 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:28.545 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:32.545 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:32.559 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 14:07:33.377 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:33.476 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:33.476 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:33.476 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 14:07:33.476 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:40:37.749 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 15:40:37.749 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:40:37.818 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:40:37.834 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:40:37.839 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:15.269 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 15:44:15.271 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:15.331 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:15.337 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:15.344 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:28.002 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 15:44:28.003 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:28.057 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:31.831 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:31.838 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:31.868 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:31.868 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:31.869 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:31.869 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:31.879 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:32.319 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:32.629 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:32.849 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:32.859 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:32.866 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:32.873 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:33.602 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 15:44:42.742 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 15:44:42.743 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:42.761 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 15:44:46.575 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:46.583 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:46.590 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:51.294 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:51.310 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 15:44:52.198 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:52.538 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:52.540 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:52.542 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:44:52.543 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 15:47:18.100 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:18.240 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.230 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.232 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.233 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.234 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.427 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.510 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.511 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.513 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.514 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:20.896 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:21.078 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:24.052 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:47:52.735 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-11 15:48:02.172 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-11 15:50:27.995 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:51:14.163 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:52:16.700 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 15:55:46.959 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 15:55:46.961 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 16:22:28.827 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 16:22:28.846 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 16:32:11.381 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 16:32:11.382 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 16:37:11.423 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2025-12-11 16:42:11.440 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2025-12-11 16:47:11.453 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /tools: timed out
2025-12-11 16:47:11.455 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-11 20:37:57.224 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 20:37:57.230 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:37:57.297 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:37:57.302 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:37:57.308 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:12.460 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 20:38:12.460 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:12.502 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:17.157 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:17.163 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:17.267 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:17.267 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:17.267 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:17.269 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:17.277 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:17.759 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:18.073 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:18.268 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:18.277 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:18.289 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:18.301 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:19.120 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 20:38:30.740 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 20:38:30.740 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:30.767 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 20:38:34.645 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:34.655 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:34.658 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:36.890 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:36.912 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 20:38:37.412 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:37.649 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:37.650 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:37.652 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:37.653 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:40.169 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:40.462 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:38:56.050 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:39:33.154 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A17B17BA60>
2025-12-11 20:41:44.374 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 20:41:44.376 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 20:43:13.217 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A17B17AB30>
2025-12-11 20:45:56.300 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:47:11.497 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A17B1F2B00>
2025-12-11 20:47:12.628 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-11 20:47:21.810 | ERROR    | chatchat.server.utils:wrap_done:46 - APIConnectionError: Caught exception: Connection error.
2025-12-11 20:47:56.519 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:47:56.590 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:47:56.594 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:47:56.594 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:47:56.594 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:48:13.211 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:48:13.272 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:48:13.273 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:48:13.274 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:48:13.275 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:04.855 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 20:55:04.876 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 20:55:04.876 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 20:55:04.876 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 20:55:14.831 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:25.556 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:25.592 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:29.669 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:30.583 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 20:55:37.712 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:37.757 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 20:55:43.423 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.439 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.541 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.542 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.548 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.549 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.549 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.551 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.555 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:43.559 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:47.983 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:48.245 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:52.024 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:55:54.653 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000002604CFBFD30>
2025-12-11 20:56:06.482 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:56:09.920 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000002604CFFDC60>
2025-12-11 20:56:27.503 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 20:56:29.806 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000002604CFFE8C0>
2025-12-11 21:02:41.690 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:02:41.706 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:02:41.706 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:02:41.706 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:02:54.572 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:02:54.572 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:02:54.629 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:02:54.636 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:02:54.644 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:08.059 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:03:08.061 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:08.088 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:11.937 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:11.952 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:11.989 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:11.992 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:11.992 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:11.994 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:12.006 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:12.419 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:12.668 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:12.860 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:12.862 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:12.878 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:12.888 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:13.700 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:03:24.337 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:03:24.337 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:24.384 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:03:27.702 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:27.704 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:27.704 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:29.303 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:29.303 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:03:29.755 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:29.933 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:29.934 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:29.937 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:03:29.938 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:05:05.140 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:05.140 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:05.316 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:05.317 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:05.319 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:05.319 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:08.387 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:08.606 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:13.877 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:36.337 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000002B7F7492C20>
2025-12-11 21:05:53.816 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:05:57.791 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000002B7F74E1AB0>
2025-12-11 21:06:16.360 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:06:18.419 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000002B7F74E2770>
2025-12-11 21:06:29.930 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen2.5-instruct instead
2025-12-11 21:06:35.839 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000002B7F7492CB0>
2025-12-11 21:06:36.759 | INFO     | chatchat.server.knowledge_base.kb_cache.faiss_cache:load_vector_store:109 - loading vector store in '20250908/vector_store/bge-m3' from disk.
2025-12-11 21:12:30.154 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:12:30.205 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:12:30.207 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:12:30.210 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:12:45.068 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:12:45.069 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:12:45.217 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:12:45.238 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:12:45.257 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:02.498 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:13:02.500 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:02.581 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:07.717 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:07.724 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:07.760 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:07.760 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:07.760 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:07.760 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:07.771 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:08.297 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:08.662 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:08.855 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:08.870 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:08.890 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:08.914 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:10.047 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:13:23.268 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:13:23.268 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:23.333 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:13:29.215 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:29.262 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:29.276 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:34.294 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:34.315 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:13:35.620 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:35.703 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:35.703 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:35.708 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:35.708 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:42.799 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:42.913 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:42.915 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:42.915 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:42.915 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:44.053 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:44.175 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:44.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:44.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:44.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:46.649 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:47.563 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:47.655 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:47.657 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:47.657 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:47.661 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:53.402 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:53.744 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:56.714 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:13:58.140 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA10BE50>
2025-12-11 21:14:19.508 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:14:24.860 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA1D5900>
2025-12-11 21:14:35.507 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:14:35.509 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:14:37.349 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA1D6680>
2025-12-11 21:15:18.828 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:16:08.906 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA1D6050>
2025-12-11 21:16:32.333 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:16:32.333 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:16:36.791 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234F87BFE20>
2025-12-11 21:18:03.816 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:18:03.816 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:04.014 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:18:04.014 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:04.083 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:04.090 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:04.090 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:04.091 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:08.094 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:08.319 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:11.711 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:16.730 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA077C40>
2025-12-11 21:18:20.368 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:23.559 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA1D48B0>
2025-12-11 21:18:31.350 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:18:36.081 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA0769B0>
2025-12-11 21:18:38.156 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:18:38.156 | ERROR    | chatchat.server.utils:get_Embeddings:342 - failed to create Embeddings for model: bge-m3.
2025-12-11 21:18:38.159 | ERROR    | chatchat.server.utils:check_embed_model:356 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-11 21:18:38.159 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:295 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-11 21:18:57.293 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:19:00.957 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000234FA70FA90>
2025-12-11 21:28:34.598 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:28:34.618 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:28:34.620 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:28:34.621 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:28:48.376 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:28:48.376 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:28:48.436 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:28:48.443 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:28:48.459 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:04.343 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:29:04.343 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:04.416 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:08.899 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:08.913 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:08.969 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:08.971 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:08.973 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:08.974 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:09.004 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:09.768 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:10.153 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:10.368 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:10.380 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:10.389 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:10.398 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:11.341 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:29:22.150 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:29:22.150 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:22.215 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:29:26.391 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:26.406 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:26.422 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:30.990 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:31.009 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:32.180 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:32.216 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:32.334 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:32.337 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:32.338 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:32.340 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:37.268 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:38.019 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:41.005 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:41.285 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:41.662 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:41.925 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:42.858 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:43.178 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:44.944 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:47.459 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:47.464 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:47.466 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:47.469 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:29:49.282 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:56.909 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:29:59.546 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:30:04.474 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:30:06.721 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A547D57D90>
2025-12-11 21:30:42.980 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:30:42.980 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:30:48.004 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A547EC3040>
2025-12-11 21:31:07.147 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:07.230 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:07.232 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:07.234 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:07.235 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:09.915 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:10.407 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:21.964 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:26.488 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A547EC3F40>
2025-12-11 21:31:48.039 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:31:48.041 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:31:54.582 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A547E7B550>
2025-12-11 21:31:56.742 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:31:56.742 | ERROR    | chatchat.server.utils:get_Embeddings:342 - failed to create Embeddings for model: bge-m3.
2025-12-11 21:31:56.756 | ERROR    | chatchat.server.utils:check_embed_model:356 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-11 21:31:56.758 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:295 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-11 21:32:03.998 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:32:05.598 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001A54845C310>
2025-12-11 21:35:14.689 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:35:14.689 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:39:33.021 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:39:33.024 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-11 21:39:33.024 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:39:33.026 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-11 21:39:48.575 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:39:48.576 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:39:48.621 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:39:48.629 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:39:48.634 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:02.473 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:40:02.473 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:02.515 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:05.845 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:05.850 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:05.876 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:05.876 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:05.876 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:05.876 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:05.887 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:06.218 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:06.462 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:06.627 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:06.635 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:06.643 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:06.648 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:07.363 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:40:16.433 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:40:16.433 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:16.475 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-11 21:40:19.559 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:19.565 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:19.574 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:21.464 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:21.481 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-11 21:40:22.039 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.046 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.174 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.181 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.182 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.182 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.182 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.182 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.182 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:22.186 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:24.026 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:24.031 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:24.179 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:24.182 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:24.183 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:24.184 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:24.740 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:41.061 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:40:49.298 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000138FA2B3CA0>
2025-12-11 21:41:05.592 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:43:14.166 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-11 21:43:14.167 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-11 21:48:14.221 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /knowledge_base/list_knowledge_bases: timed out
2025-12-13 11:12:53.592 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /knowledge_base/list_knowledge_bases: timed out
2025-12-13 11:17:53.673 | ERROR    | chatchat.webui_pages.utils:get:64 - ReadTimeout: error when get /knowledge_base/list_knowledge_bases: timed out
2025-12-13 11:17:53.695 | ERROR    | chatchat.webui_pages.utils:to_json:233 - AttributeError: API未能返回正确的JSON。'NoneType' object has no attribute 'json'
2025-12-13 11:23:30.320 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-13 11:23:30.416 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-13 11:23:30.416 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-13 11:23:30.416 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-13 13:25:19.796 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:25:19.798 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:25:19.928 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:25:19.958 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:25:20.022 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:25:59.710 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:25:59.712 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:25:59.818 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:26:09.987 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:10.000 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:10.063 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:10.064 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:10.065 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:10.067 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:10.092 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:11.149 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:11.822 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:26:12.316 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:12.336 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:12.357 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:12.383 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:14.270 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 13:26:34.405 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:26:34.406 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:26:34.469 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 13:26:44.394 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:44.413 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:44.434 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:51.243 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:51.272 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:26:52.485 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:26:52.626 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:26:52.628 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:26:52.629 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:26:52.631 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:30:40.889 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:30:40.890 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:30:40.968 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:30:40.970 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:30:40.971 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:30:40.972 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:30:43.166 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:30:50.756 | ERROR    | chatchat.server.api_server.openai_routes:generator:113 - openai request error: Connection error.
2025-12-13 13:31:11.328 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:11.440 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:11.442 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:11.444 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:11.446 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:19.084 | ERROR    | chatchat.server.api_server.openai_routes:generator:113 - openai request error: Connection error.
2025-12-13 13:31:31.046 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:31.366 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:34.715 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:31:52.414 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x00000157F80C40A0>
2025-12-13 13:32:20.213 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:32:20.214 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:32:22.096 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001578F9AA350>
2025-12-13 13:32:46.568 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:32:48.435 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001578F9AAC20>
2025-12-13 13:51:37.574 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:51:37.576 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:51:37.640 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:37.647 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:37.653 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:50.227 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:51:50.229 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:51:50.285 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:51:53.949 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:53.956 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:53.988 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:53.988 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:53.989 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:53.989 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:53.999 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:54.430 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:54.738 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:51:54.921 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:54.931 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:54.938 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:54.945 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:51:55.680 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 13:52:04.910 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:52:04.912 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:04.946 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 13:52:08.405 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:52:08.412 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:52:08.419 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:52:11.820 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:52:11.831 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:52:12.393 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:12.556 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:12.557 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:12.558 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:12.559 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:17.597 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:17.845 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:21.249 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:52:43.588 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001D489667220>
2025-12-13 13:52:46.530 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:53:53.244 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x000001D48953EBC0>
2025-12-13 13:53:55.340 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:53:55.343 | ERROR    | chatchat.server.utils:get_Embeddings:342 - failed to create Embeddings for model: bge-m3.
2025-12-13 13:53:55.355 | ERROR    | chatchat.server.utils:check_embed_model:356 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-13 13:53:55.358 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:295 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-13 13:54:12.833 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:54:12.836 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:54:53.957 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:54:55.498 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:54:55.690 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:00.974 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-13 13:55:00.975 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-13 13:55:00.975 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-13 13:55:00.976 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-13 13:55:12.678 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:55:12.680 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:12.719 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:12.726 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:12.732 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:25.030 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:55:25.033 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:25.082 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:28.346 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.353 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.378 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.379 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.379 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.379 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.389 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.718 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:28.971 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:29.137 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:29.146 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:29.152 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:29.159 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:29.895 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 13:55:38.844 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:55:38.846 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:38.884 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 13:55:41.343 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:41.356 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:41.370 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:42.930 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:42.940 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 13:55:43.370 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:43.545 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:43.546 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:43.548 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:43.549 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:57.496 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:55:57.799 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:56:00.426 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:56:22.430 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x0000020F5B7D3700>
2025-12-13 13:56:27.136 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:57:34.799 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x0000020F5B6BAAD0>
2025-12-13 13:57:36.878 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:57:36.881 | ERROR    | chatchat.server.utils:get_Embeddings:342 - failed to create Embeddings for model: bge-m3.
2025-12-13 13:57:36.894 | ERROR    | chatchat.server.utils:check_embed_model:356 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-13 13:57:36.895 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:295 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-13 13:58:12.190 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:58:12.192 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 13:59:45.839 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x0000020F5BDB6AA0>
2025-12-13 13:59:47.900 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 13:59:47.900 | ERROR    | chatchat.server.utils:get_Embeddings:342 - failed to create Embeddings for model: bge-m3.
2025-12-13 13:59:47.904 | ERROR    | chatchat.server.utils:check_embed_model:356 - failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-13 13:59:47.904 | ERROR    | chatchat.server.chat.kb_chat:knowledge_base_chat_iterator:295 - error in knowledge chat: failed to access embed model 'bge-m3': 'NoneType' object has no attribute 'embed_query'
2025-12-13 14:02:35.298 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-13 14:02:35.298 | WARNING  | __main__:start_main_server:316 - Sending SIGKILL to %s
2025-12-13 14:02:35.299 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-13 14:02:35.299 | INFO     | __main__:start_main_server:327 - Process status: %s
2025-12-13 14:02:46.354 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 14:02:46.355 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:02:46.407 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:02:46.414 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:02:46.422 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:02:59.444 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 14:02:59.446 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:02:59.497 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:02.727 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:02.734 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:02.757 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:02.758 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:02.758 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:02.758 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:02.767 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:03.096 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:03.325 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:03.491 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:03.501 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:03.508 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:03.515 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:04.231 | INFO     | __mp_main__:run_api_server:55 - Api MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 14:03:13.205 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 14:03:13.207 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:13.234 | INFO     | __mp_main__:run_webui:80 - Webui MODEL_PLATFORMS: [PlatformConfig(platform_name='xinference', platform_type='xinference', api_base_url='http://127.0.0.1:9997/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=True, llm_models=[], embed_models=[], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='ollama', platform_type='ollama', api_base_url='http://127.0.0.1:11434/v1', api_key='EMPTY', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['qwen:7b', 'qwen2:7b'], embed_models=['quentinz/bge-large-zh-v1.5'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='oneapi', platform_type='oneapi', api_base_url='http://127.0.0.1:3000/v1', api_key='sk-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['chatglm_pro', 'chatglm_turbo', 'chatglm_std', 'chatglm_lite', 'qwen-turbo', 'qwen-plus', 'qwen-max', 'qwen-max-longcontext', 'ERNIE-Bot', 'ERNIE-Bot-turbo', 'ERNIE-Bot-4', 'SparkDesk'], embed_models=['text-embedding-v1', 'Embedding-V1'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[]), PlatformConfig(platform_name='openai', platform_type='openai', api_base_url='https://api.openai.com/v1', api_key='sk-proj-', api_proxy='', api_concurrencies=5, auto_detect_model=False, llm_models=['gpt-4o', 'gpt-3.5-turbo'], embed_models=['text-embedding-3-small', 'text-embedding-3-large'], text2image_models=[], image2text_models=[], rerank_models=[], speech2text_models=[], text2speech_models=[])]
2025-12-13 14:03:15.715 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:15.723 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:15.731 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:17.243 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:17.254 | WARNING  | chatchat.server.utils:get_default_embedding:200 - default embedding model bge-m3 is not found in available embeddings, using quentinz/bge-large-zh-v1.5 instead
2025-12-13 14:03:17.679 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:17.829 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:17.831 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:17.832 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:17.833 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:20.296 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:20.359 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:20.359 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:20.360 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:20.360 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:21.358 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:21.439 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:21.441 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:21.443 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:21.444 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:22.191 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:22.430 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:03:25.810 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:04:01.081 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x0000021E0EE540A0>
2025-12-13 14:04:20.451 | WARNING  | chatchat.server.utils:detect_xf_models:103 - cannot connect to xinference host: http://127.0.0.1:9997, please check your configuration.
2025-12-13 14:04:20.454 | WARNING  | chatchat.server.utils:get_default_llm:190 - default llm model glm4-chat is not found in available llms, using qwen:7b instead
2025-12-13 14:05:04.954 | INFO     | chatchat.server.api_server.kb_routes:kb_chat_endpoint:67 - kb_chat_endpoint returning type: <class 'sse_starlette.sse.EventSourceResponse'>, content: <sse_starlette.sse.EventSourceResponse object at 0x0000021E26429960>
